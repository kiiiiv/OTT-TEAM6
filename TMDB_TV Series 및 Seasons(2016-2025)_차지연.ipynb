{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMDB APIë¡œ ë°ì´í„° ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TMDB TV Series ë°ì´í„° ì „ìˆ˜ ìˆ˜ì§‘\n",
    "\n",
    "1. ìˆ˜ì§‘ ëŒ€ìƒ: TMDB TV Series & Seasons ë°ì´í„°\n",
    "2. ìˆ˜ì§‘ ê¸°ê°„: 2016ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 11ì›” 30ì¼ (ì•½ 11ë…„ì¹˜)\n",
    "3. ìˆ˜ì§‘ ë²”ìœ„: TV Seriesê³¼ Seasons ì „ì²´ í˜ì´ì§€ (ì—í”¼ì†Œë“œ ì œì™¸)\n",
    "4. ìˆ˜ì§‘ ë°©ì‹: 100% ë¹„ë™ê¸°(Pure Async) ë°©ì‹\n",
    "5. ìˆ˜ì§‘ ì»¬ëŸ¼: ì‹œë¦¬ì¦ˆ 50ê°œ, ì‹œì¦Œ 14ê°œ\n",
    "6. ìˆ˜ì§‘ ì¡°ê±´: í˜ì´ì§€ ëˆ„ë½ 0%, TMDB API ì´ˆë‹¹ ìš”ì²­ ìˆ˜(40 req/s) ì œí•œ ëŒ€ì‘, 500í˜ì´ì§€ ì œí•œ ëŒ€ì‘\n",
    "7. ìˆ˜ì§‘ ê²°ê³¼: ì´ ê°œ ë°ì´í„°, ì•½ ë¶„ ì†Œìš” (ì‹œë¦¬ì¦ˆ ê°œ ì»¬ëŸ¼ / ì‹œì¦Œ ê°œ ì»¬ëŸ¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… nest_asyncio ì ìš© ì™„ë£Œ\n",
      "==========================================================================================\n",
      "ğŸš€ TMDB FULL SERIES + SEASONS COLLECTOR (FINAL + poster_path)\n",
      "==========================================================================================\n",
      "\n",
      "ğŸ“Œ 1ë‹¨ê³„: ID ìˆ˜ì§‘\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 556\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# RUN\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 556\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:562\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# TMDB TV SERIES + SEASONS FULL ASYNC COLLECTOR\n",
    "# Series 51 cols / Seasons 15 cols (FINAL + poster_path)\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import async_timeout\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==========================================================\n",
    "# Jupyter async ì§€ì›\n",
    "# ==========================================================\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"âœ… nest_asyncio ì ìš© ì™„ë£Œ\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ==========================================================\n",
    "# ê¸°ë³¸ ì„¤ì •\n",
    "# ==========================================================\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"TMDB_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"TMDB_API_KEY missing\")\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "MAX_CALLS_PER_SECOND = 38\n",
    "TIMEOUT = 10\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "START_DATE = \"2016-01-01\"\n",
    "END_DATE = \"2025-11-30\"\n",
    "\n",
    "# temp\n",
    "SERIES_TEMP = Path(\"tv_series_temp.csv\")\n",
    "SEASONS_TEMP = Path(\"tv_seasons_temp.csv\")\n",
    "\n",
    "# final\n",
    "SERIES_CSV = \"tv_series_2016_2025_FULL.csv\"\n",
    "SEASONS_CSV = \"tv_seasons_2016_2025_FULL.csv\"\n",
    "SERIES_PARQ = \"tv_series_2016_2025_FULL.parquet\"\n",
    "SEASONS_PARQ = \"tv_seasons_2016_2025_FULL.parquet\"\n",
    "\n",
    "# ==========================================================\n",
    "# Rate limiter\n",
    "# ==========================================================\n",
    "class AsyncRateLimiter:\n",
    "    def __init__(self, max_calls, period=1.0):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = deque()\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def acquire(self):\n",
    "        async with self.lock:\n",
    "            now = asyncio.get_event_loop().time()\n",
    "\n",
    "            # ê¸°ê°„ ë°– í˜¸ì¶œ ì œê±°\n",
    "            while self.calls and now - self.calls[0] > self.period:\n",
    "                self.calls.popleft()\n",
    "\n",
    "            # ì´ˆë‹¹ í˜¸ì¶œ ìˆ˜ ì´ˆê³¼ ì‹œ sleep\n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                sleep_for = self.period - (now - self.calls[0]) + 0.01\n",
    "                await asyncio.sleep(max(0, sleep_for))\n",
    "\n",
    "                now = asyncio.get_event_loop().time()\n",
    "                while self.calls and now - self.calls[0] > self.period:\n",
    "                    self.calls.popleft()\n",
    "\n",
    "            self.calls.append(now)\n",
    "\n",
    "rate_limiter = AsyncRateLimiter(MAX_CALLS_PER_SECOND)\n",
    "\n",
    "# ==========================================================\n",
    "# TMDB GET\n",
    "# ==========================================================\n",
    "async def tmdb_get(session, path, params=None, retry=0):\n",
    "    if retry >= MAX_RETRIES:\n",
    "        return None\n",
    "\n",
    "    params = params or {}\n",
    "    params.setdefault(\"api_key\", API_KEY)\n",
    "    params.setdefault(\"language\", \"en-US\")\n",
    "\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "\n",
    "    await rate_limiter.acquire()\n",
    "\n",
    "    try:\n",
    "        async with async_timeout.timeout(TIMEOUT):\n",
    "            async with session.get(url, params=params, headers=HEADERS) as resp:\n",
    "                # ë ˆì´íŠ¸ ë¦¬ë°‹\n",
    "                if resp.status == 429:\n",
    "                    wait = float(resp.headers.get(\"Retry-After\", 2)) * (2 ** retry)\n",
    "                    await asyncio.sleep(wait)\n",
    "                    return await tmdb_get(session, path, params, retry + 1)\n",
    "\n",
    "                # Not found\n",
    "                if resp.status == 404:\n",
    "                    return None\n",
    "\n",
    "                # ì„œë²„ ì—ëŸ¬\n",
    "                if 500 <= resp.status < 600:\n",
    "                    await asyncio.sleep(2 ** retry)\n",
    "                    return await tmdb_get(session, path, params, retry + 1)\n",
    "\n",
    "                resp.raise_for_status()\n",
    "                return await resp.json()\n",
    "\n",
    "    except:\n",
    "        await asyncio.sleep(2 ** retry)\n",
    "        return await tmdb_get(session, path, params, retry + 1)\n",
    "\n",
    "# ==========================================================\n",
    "# discover page\n",
    "# ==========================================================\n",
    "async def fetch_discover_page(session, page, gte, lte):\n",
    "    params = {\n",
    "        \"sort_by\": \"popularity.desc\",\n",
    "        \"first_air_date.gte\": gte,\n",
    "        \"first_air_date.lte\": lte,\n",
    "        \"page\": page,\n",
    "        \"include_adult\": \"true\",   # ğŸ”¥ FIX: ë¬¸ìì—´ true (TMDB discover ê·œê²©)\n",
    "    }\n",
    "    data = await tmdb_get(session, \"/discover/tv\", params)\n",
    "    if not data:\n",
    "        return [], 1, 0\n",
    "    return data.get(\"results\", []), data.get(\"total_pages\", 1), data.get(\"total_results\", 0)\n",
    "\n",
    "# ==========================================================\n",
    "# ë¶„í•  ìˆ˜ì§‘ (500 page limit íšŒí”¼)\n",
    "# ==========================================================\n",
    "async def collect_ids_in_range(session, start, end, depth=0):\n",
    "    indent = \"  \" * depth\n",
    "\n",
    "    res1, total_pages, total_results = await fetch_discover_page(session, 1, start, end)\n",
    "\n",
    "    if total_results == 0:\n",
    "        return set()\n",
    "\n",
    "    # 500í˜ì´ì§€ ì´í•˜ë©´ ê·¸ëŒ€ë¡œ ì „ìˆ˜ ìˆ˜ì§‘\n",
    "    if total_pages <= 500:\n",
    "        ids = {r[\"id\"] for r in res1}\n",
    "        if total_pages > 1:\n",
    "            tasks = [\n",
    "                fetch_discover_page(session, p, start, end)\n",
    "                for p in range(2, total_pages + 1)\n",
    "            ]\n",
    "            for coro in asyncio.as_completed(tasks):\n",
    "                r, _, _ = await coro\n",
    "                for x in r:\n",
    "                    ids.add(x[\"id\"])\n",
    "        return ids\n",
    "\n",
    "    # 500 í˜ì´ì§€ ì´ˆê³¼ â†’ ë‚ ì§œ ë°˜ìœ¼ë¡œ ë¶„í• \n",
    "    start_dt = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    mid_dt = start_dt + (end_dt - start_dt) / 2\n",
    "    mid = mid_dt.strftime(\"%Y-%m-%d\")\n",
    "    right_start = (mid_dt + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    left = await collect_ids_in_range(session, start, mid, depth + 1)\n",
    "    right = await collect_ids_in_range(session, right_start, end, depth + 1)\n",
    "    return left | right\n",
    "\n",
    "# ==========================================================\n",
    "# helpers\n",
    "# ==========================================================\n",
    "def list_to_str(lst, key=\"name\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key, \"\")) for i in lst if i.get(key))\n",
    "\n",
    "def list_ids_to_str(lst, key=\"id\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key)) for i in lst if i.get(key))\n",
    "\n",
    "# ==========================================================\n",
    "# providers\n",
    "# ==========================================================\n",
    "async def fetch_providers(session, sid):\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/watch/providers\")\n",
    "    if not data:\n",
    "        return \"\"\n",
    "    out = set()\n",
    "    for cc in [\"US\", \"GB\", \"KR\"]:\n",
    "        if cc in data.get(\"results\", {}):\n",
    "            p = data[\"results\"][cc].get(\"flatrate\", [])\n",
    "            for x in p:\n",
    "                if x.get(\"provider_name\"):\n",
    "                    out.add(x[\"provider_name\"])\n",
    "    return \", \".join(sorted(out))\n",
    "\n",
    "# ==========================================================\n",
    "# single season (ì‹œì¦Œ 1ê°œ ìˆ˜ì§‘)\n",
    "# ==========================================================\n",
    "async def fetch_single_season(session, sid, sname, net_names, net_ids, meta):\n",
    "    sn = meta.get(\"season_number\")\n",
    "    if sn is None:\n",
    "        return None\n",
    "\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/season/{sn}\")\n",
    "\n",
    "    # ì‹œì¦Œ ìƒì„¸ ëª» ê°€ì ¸ì˜¤ë©´ meta ê¸°ë°˜ìœ¼ë¡œ fallback\n",
    "    if not data:\n",
    "        return {\n",
    "            \"_id\": f\"{sid}_{sn}\",\n",
    "            \"season_id\": meta.get(\"id\"),\n",
    "            \"series_id\": sid,\n",
    "            \"series_name\": sname,\n",
    "            \"season_number\": sn,\n",
    "            \"name\": meta.get(\"name\"),\n",
    "            \"air_date\": meta.get(\"air_date\"),\n",
    "            \"overview\": meta.get(\"overview\"),\n",
    "            \"vote_average\": meta.get(\"vote_average\"),\n",
    "            \"vote_count\": meta.get(\"vote_count\"),\n",
    "            \"network_names\": net_names,\n",
    "            \"network_ids\": net_ids,\n",
    "            \"total_episodes\": meta.get(\"episode_count\"),\n",
    "            \"avg_episode_runtime\": None,\n",
    "            \"poster_path\": meta.get(\"poster_path\"),   # ğŸ”¥ ì‹œì¦Œ í¬ìŠ¤í„° (meta ê¸°ì¤€)\n",
    "        }\n",
    "\n",
    "    eps = data.get(\"episodes\") or []\n",
    "    runtimes = [ep.get(\"runtime\") for ep in eps if isinstance(ep.get(\"runtime\"), (int, float))]\n",
    "    avg_rt = sum(runtimes) / len(runtimes) if runtimes else None\n",
    "\n",
    "    return {\n",
    "        \"_id\": data.get(\"_id\") or f\"{sid}_{sn}\",\n",
    "        \"season_id\": data.get(\"id\"),\n",
    "        \"series_id\": sid,\n",
    "        \"series_name\": sname,\n",
    "        \"season_number\": data.get(\"season_number\"),\n",
    "        \"name\": data.get(\"name\"),\n",
    "        \"air_date\": data.get(\"air_date\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "        \"network_names\": net_names,\n",
    "        \"network_ids\": net_ids,\n",
    "        \"total_episodes\": len(eps),\n",
    "        \"avg_episode_runtime\": avg_rt,\n",
    "        \"poster_path\": data.get(\"poster_path\"),   # ğŸ”¥ ì‹œì¦Œ í¬ìŠ¤í„° (ìƒì„¸ ê¸°ì¤€)\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# TV ìƒì„¸ + ì‹œì¦Œ (Series 51 cols / Seasons 15 cols)\n",
    "# ==========================================================\n",
    "async def fetch_tv_details_and_seasons(session, sid):\n",
    "    params = {\"append_to_response\": \"reviews,keywords,aggregate_credits\"}\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}\", params)\n",
    "    if not data:\n",
    "        return None, []\n",
    "\n",
    "    genres = data.get(\"genres\") or []\n",
    "    networks = data.get(\"networks\") or []\n",
    "    last_ep = data.get(\"last_episode_to_air\") or {}\n",
    "\n",
    "    net_names = list_to_str(networks)\n",
    "    net_ids = list_ids_to_str(networks)\n",
    "\n",
    "    # ========== SERIES 51ê°œ ==========\n",
    "    series = {\n",
    "        \"id\": data.get(\"id\"),\n",
    "        \"title\": data.get(\"name\"),            # ğŸ”¥ name í•„ë“œëŠ” ì—¬ê¸°ë§Œ ì‚¬ìš© (ì»¬ëŸ¼ì—” 'title')\n",
    "        \"type\": \"tv_series\",\n",
    "        \"adult\": data.get(\"adult\"),\n",
    "        \"backdrop_path\": data.get(\"backdrop_path\"),\n",
    "        \"created_by\": list_to_str(data.get(\"created_by\")),\n",
    "        \"episode_run_time\": (\n",
    "            \", \".join(map(str, data.get(\"episode_run_time\", [])))\n",
    "            if data.get(\"episode_run_time\") else \"\"\n",
    "        ),\n",
    "        \"first_air_date\": data.get(\"first_air_date\"),\n",
    "        \"genres\": list_to_str(genres),\n",
    "        \"genre_ids\": \", \".join(str(g.get(\"id\")) for g in genres if g.get(\"id\")),\n",
    "        \"homepage\": data.get(\"homepage\"),\n",
    "        \"in_production\": data.get(\"in_production\"),\n",
    "        \"languages\": \", \".join(data.get(\"languages\", [])) if data.get(\"languages\") else \"\",\n",
    "        \"last_air_date\": data.get(\"last_air_date\"),\n",
    "\n",
    "        \"last_episode_to_air_id\": last_ep.get(\"id\"),\n",
    "        \"last_episode_to_air_name\": last_ep.get(\"name\"),\n",
    "        \"last_episode_to_air_overview\": last_ep.get(\"overview\"),\n",
    "        \"last_episode_to_air_vote_average\": last_ep.get(\"vote_average\"),\n",
    "        \"last_episode_to_air_vote_count\": last_ep.get(\"vote_count\"),\n",
    "        \"last_episode_to_air_air_date\": last_ep.get(\"air_date\"),\n",
    "        \"last_episode_to_air_episode_number\": last_ep.get(\"episode_number\"),\n",
    "        \"last_episode_to_air_production_code\": last_ep.get(\"production_code\"),\n",
    "        \"last_episode_to_air_runtime\": last_ep.get(\"runtime\"),\n",
    "        \"last_episode_to_air_season_number\": last_ep.get(\"season_number\"),\n",
    "        \"last_episode_to_air_show_id\": last_ep.get(\"show_id\"),\n",
    "        \"last_episode_to_air_still_path\": last_ep.get(\"still_path\"),\n",
    "\n",
    "        \"next_episode_to_air\": \"\" if not data.get(\"next_episode_to_air\") else str(data.get(\"next_episode_to_air\")),\n",
    "        \"networks\": net_names,\n",
    "        \"number_of_episodes\": data.get(\"number_of_episodes\"),\n",
    "        \"number_of_seasons\": data.get(\"number_of_seasons\"),\n",
    "\n",
    "        \"origin_country\": \", \".join(data.get(\"origin_country\", [])) if data.get(\"origin_country\") else \"\",\n",
    "        \"original_language\": data.get(\"original_language\"),\n",
    "        \"original_name\": data.get(\"original_name\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"popularity\": data.get(\"popularity\"),\n",
    "        \"poster_path\": data.get(\"poster_path\"),   # ğŸ”¥ ì‹œë¦¬ì¦ˆ í¬ìŠ¤í„°\n",
    "\n",
    "        \"production_companies\": list_to_str(data.get(\"production_companies\")),\n",
    "        \"production_countries\": list_to_str(data.get(\"production_countries\")),\n",
    "\n",
    "        \"seasons\": \"; \".join(\n",
    "            f\"S{s.get('season_number')}: {s.get('name')} ({s.get('episode_count')} eps)\"\n",
    "            for s in (data.get(\"seasons\") or [])\n",
    "            if s.get(\"season_number\") is not None\n",
    "        ),\n",
    "\n",
    "        \"spoken_languages\": list_to_str(data.get(\"spoken_languages\")),\n",
    "        \"status\": data.get(\"status\"),\n",
    "        \"tagline\": data.get(\"tagline\"),\n",
    "        \"type_detail\": data.get(\"type\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "    }\n",
    "\n",
    "    # ë¦¬ë·°\n",
    "    rv_block = data.get(\"reviews\", {})\n",
    "    rv_items = rv_block.get(\"results\", []) if isinstance(rv_block, dict) else []\n",
    "    rev = []\n",
    "    for r in rv_items[:5]:\n",
    "        author = r.get(\"author\", \"\")\n",
    "        rating = r.get(\"author_details\", {}).get(\"rating\")\n",
    "        rt = f\"({rating})\" if rating is not None else \"\"\n",
    "        cont = (r.get(\"content\") or \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \")[:200]\n",
    "        rev.append(f\"{author}{rt}: {cont}\")\n",
    "    series[\"review\"] = \" || \".join(rev)\n",
    "\n",
    "    # í‚¤ì›Œë“œ\n",
    "    kw_block = data.get(\"keywords\", {})\n",
    "    kw_items = kw_block.get(\"results\", []) if isinstance(kw_block, dict) else []\n",
    "    series[\"keyword\"] = \", \".join([k.get(\"name\") for k in kw_items[:20] if k.get(\"name\")])\n",
    "\n",
    "    # top cast & crew\n",
    "    credits = data.get(\"aggregate_credits\", {})\n",
    "    cast = credits.get(\"cast\") or []\n",
    "    crew = credits.get(\"crew\") or []\n",
    "\n",
    "    series[\"top_cast\"] = \", \".join([c.get(\"name\") for c in cast[:4] if c.get(\"name\")])\n",
    "\n",
    "    dirs = set()\n",
    "    wrs = set()\n",
    "    for c in crew:\n",
    "        nm = c.get(\"name\")\n",
    "        if not nm:\n",
    "            continue\n",
    "        for job in (c.get(\"jobs\") or []):\n",
    "            jn = job.get(\"job\", \"\")\n",
    "            if \"Director\" in jn:\n",
    "                dirs.add(nm)\n",
    "            if jn in [\"Writer\", \"Screenplay\", \"Story\"]:\n",
    "                wrs.add(nm)\n",
    "\n",
    "    series[\"directors\"] = \", \".join(sorted(dirs)[:10])\n",
    "    series[\"writers\"] = \", \".join(sorted(wrs)[:10])\n",
    "\n",
    "    # providers\n",
    "    series[\"providers\"] = await fetch_providers(session, sid)\n",
    "\n",
    "    # ì‹œì¦Œ ë¦¬ìŠ¤íŠ¸\n",
    "    seasons_meta = data.get(\"seasons\") or []\n",
    "    tasks = [\n",
    "        fetch_single_season(session, sid, data.get(\"name\") or \"\", net_names, net_ids, s)\n",
    "        for s in seasons_meta\n",
    "        if s.get(\"season_number\") is not None\n",
    "    ]\n",
    "\n",
    "    season_records = []\n",
    "    for coro in asyncio.as_completed(tasks):\n",
    "        x = await coro\n",
    "        if x:\n",
    "            season_records.append(x)\n",
    "\n",
    "    return series, season_records\n",
    "\n",
    "# ==========================================================\n",
    "# ì‹œì¦Œ ê¸°ë°˜ ê²°ì¸¡ì¹˜\n",
    "# ==========================================================\n",
    "def fill_series_gaps(series_df, seasons_df):\n",
    "    print(\"\\nğŸ“Š ì‹œì¦Œ ê¸°ë°˜ ê²°ì¸¡ì¹˜ ë³´ì™„ ì‹œì‘\")\n",
    "\n",
    "    # episode_run_time\n",
    "    rtmap = seasons_df.groupby(\"series_id\")[\"avg_episode_runtime\"].mean().round()\n",
    "    mask = series_df[\"episode_run_time\"].astype(str).str.strip().isin([\"\", \"nan\"])\n",
    "    series_df.loc[mask, \"episode_run_time\"] = (\n",
    "        series_df.loc[mask, \"id\"].map(rtmap).fillna(0).astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    # number_of_episodes\n",
    "    epmap = seasons_df.groupby(\"series_id\")[\"total_episodes\"].sum()\n",
    "    mask2 = series_df[\"number_of_episodes\"].isna()\n",
    "    series_df.loc[mask2, \"number_of_episodes\"] = (\n",
    "        series_df.loc[mask2, \"id\"].map(epmap)\n",
    "    )\n",
    "\n",
    "    # number_of_seasons\n",
    "    seasmap = seasons_df.groupby(\"series_id\")[\"season_number\"].nunique()\n",
    "    mask3 = series_df[\"number_of_seasons\"].isna()\n",
    "    series_df.loc[mask3, \"number_of_seasons\"] = (\n",
    "        series_df.loc[mask3, \"id\"].map(seasmap)\n",
    "    )\n",
    "\n",
    "    # last_air_date\n",
    "    lastair = seasons_df.sort_values(\"air_date\").groupby(\"series_id\")[\"air_date\"].last()\n",
    "    ladmask = series_df[\"last_air_date\"].astype(str).str.strip().isin([\"\", \"nan\", \"NaT\"])\n",
    "    series_df.loc[ladmask, \"last_air_date\"] = series_df.loc[ladmask, \"id\"].map(lastair)\n",
    "\n",
    "    print(\"âœ… ë³´ì™„ ì™„ë£Œ\\n\")\n",
    "    return series_df\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN\n",
    "# ==========================================================\n",
    "async def main():\n",
    "    print(\"=\" * 90)\n",
    "    print(\"ğŸš€ TMDB FULL SERIES + SEASONS COLLECTOR (FINAL + poster_path)\")\n",
    "    print(\"=\" * 90)\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    # ê¸°ì¡´ temp ì‚­ì œ\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # 1) ID ìˆ˜ì§‘\n",
    "        print(\"\\nğŸ“Œ 1ë‹¨ê³„: ID ìˆ˜ì§‘\")\n",
    "        ids = sorted(list(await collect_ids_in_range(session, START_DATE, END_DATE)))\n",
    "        print(f\"âœ¨ ì´ ID: {len(ids):,}ê°œ\")\n",
    "\n",
    "        # 2) ìƒì„¸ + ì‹œì¦Œ ìˆ˜ì§‘\n",
    "        print(\"\\nğŸ“Œ 2ë‹¨ê³„: ìƒì„¸ + ì‹œì¦Œ ìˆ˜ì§‘\")\n",
    "        batch_size = 500\n",
    "        sem = asyncio.Semaphore(25)\n",
    "\n",
    "        async def fetch_one(i):\n",
    "            async with sem:\n",
    "                return await fetch_tv_details_and_seasons(session, i)\n",
    "\n",
    "        processed = 0\n",
    "        for start in range(0, len(ids), batch_size):\n",
    "            part = ids[start:start+batch_size]\n",
    "            tasks = [fetch_one(x) for x in part]\n",
    "\n",
    "            bs = []\n",
    "            bse = []\n",
    "\n",
    "            for coro in asyncio.as_completed(tasks):\n",
    "                s, ss = await coro\n",
    "                if s:\n",
    "                    bs.append(s)\n",
    "                if ss:\n",
    "                    bse.extend(ss)\n",
    "\n",
    "                processed += 1\n",
    "                if processed % 100 == 0:\n",
    "                    print(f\"â± {processed:,}/{len(ids):,}\")\n",
    "\n",
    "            # flush\n",
    "            if bs:\n",
    "                pd.DataFrame(bs).to_csv(\n",
    "                    SERIES_TEMP, mode=\"a\", header=not SERIES_TEMP.exists(),\n",
    "                    index=False, encoding=\"utf-8-sig\"\n",
    "                )\n",
    "            if bse:\n",
    "                pd.DataFrame(bse).to_csv(\n",
    "                    SEASONS_TEMP, mode=\"a\", header=not SEASONS_TEMP.exists(),\n",
    "                    index=False, encoding=\"utf-8-sig\"\n",
    "                )\n",
    "\n",
    "    # ==========================================================\n",
    "    # 3) temp â†’ final DF\n",
    "    # ==========================================================\n",
    "    df_series = pd.read_csv(SERIES_TEMP)\n",
    "    df_seasons = pd.read_csv(SEASONS_TEMP) if SEASONS_TEMP.exists() else pd.DataFrame()\n",
    "\n",
    "    SERIES_COLS = [\n",
    "        \"id\",\"title\",\"type\",\"adult\",\"backdrop_path\",\"created_by\",\"episode_run_time\",\n",
    "        \"first_air_date\",\"genres\",\"genre_ids\",\"homepage\",\"in_production\",\"languages\",\n",
    "        \"last_air_date\",\"last_episode_to_air_id\",\"last_episode_to_air_name\",\n",
    "        \"last_episode_to_air_overview\",\"last_episode_to_air_vote_average\",\n",
    "        \"last_episode_to_air_vote_count\",\"last_episode_to_air_air_date\",\n",
    "        \"last_episode_to_air_episode_number\",\"last_episode_to_air_production_code\",\n",
    "        \"last_episode_to_air_runtime\",\"last_episode_to_air_season_number\",\n",
    "        \"last_episode_to_air_show_id\",\"last_episode_to_air_still_path\",\n",
    "        \"next_episode_to_air\",\"networks\",\"number_of_episodes\",\"number_of_seasons\",\n",
    "        \"origin_country\",\"original_language\",\"original_name\",\"overview\",\"popularity\",\n",
    "        \"poster_path\",                              # ğŸ”¥ Series poster_path\n",
    "        \"production_companies\",\"production_countries\",\"seasons\",\"spoken_languages\",\n",
    "        \"status\",\"tagline\",\"type_detail\",\"vote_average\",\"vote_count\",\"review\",\n",
    "        \"keyword\",\"top_cast\",\"directors\",\"writers\",\"providers\"\n",
    "    ]\n",
    "\n",
    "    SEASON_COLS = [\n",
    "        \"_id\",\"season_id\",\"series_id\",\"series_name\",\"season_number\",\"name\",\"air_date\",\n",
    "        \"overview\",\"vote_average\",\"vote_count\",\"network_names\",\"network_ids\",\n",
    "        \"total_episodes\",\"avg_episode_runtime\",\"poster_path\"   # ğŸ”¥ Season poster_path\n",
    "    ]\n",
    "\n",
    "    df_series = df_series[SERIES_COLS].drop_duplicates(subset=[\"id\"])\n",
    "    if not df_seasons.empty:\n",
    "        df_seasons = df_seasons[SEASON_COLS].drop_duplicates(subset=[\"series_id\",\"season_number\"])\n",
    "\n",
    "    # 4) ì‹œì¦Œ ê¸°ë°˜ ê²°ì¸¡ì¹˜ ë³´ì™„\n",
    "    if not df_seasons.empty:\n",
    "        df_series = fill_series_gaps(df_series, df_seasons)\n",
    "\n",
    "    # 5) ì €ì¥\n",
    "    df_series.to_csv(SERIES_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    df_seasons.to_csv(SEASONS_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    try:\n",
    "        df_series.to_parquet(SERIES_PARQ, index=False)\n",
    "        df_seasons.to_parquet(SEASONS_PARQ, index=False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    elapsed = (datetime.now() - t0).total_seconds()/60\n",
    "\n",
    "    print(\"=\"*90)\n",
    "    print(\"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"ğŸ“Œ Series ìˆ˜: {len(df_series):,}ê°œ\")\n",
    "    print(f\"ğŸ“Œ Season ìˆ˜: {len(df_seasons):,}ê°œ\")\n",
    "    print(f\"ğŸ“Œ Series ì»¬ëŸ¼ ìˆ˜: {len(SERIES_COLS)}\")   # 51\n",
    "    print(f\"ğŸ“Œ Season  ì»¬ëŸ¼ ìˆ˜: {len(SEASON_COLS)}\")  # 15\n",
    "    print(f\"â± ì´ ì†Œìš”ì‹œê°„: {elapsed:.1f}ë¶„\")\n",
    "    print(\"=\"*90)\n",
    "\n",
    "# ==========================================================\n",
    "# RUN\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìˆ˜ì§‘ ê²°ê³¼ ì ê²€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
