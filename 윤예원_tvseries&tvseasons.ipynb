{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55b07d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lizzy\\Ïä§ÌååÎ•¥ÌÉÄ_ÌååÏù¥Ïç¨\\tmdbapiÏã§Ïäµ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\lizzy\\Ïä§ÌååÎ•¥ÌÉÄ_ÌååÏù¥Ïç¨\\tmdbapiÏã§Ïäµ\")  # ÏõêÌïòÎäî Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤Ω\n",
    "print(os.getcwd())  # Ïûò Î∞îÎÄåÏóàÎäîÏßÄ ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a26ab010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # ÌòÑÏû¨ ÎîîÎ†âÌÑ∞Î¶¨Ïùò .env ÌååÏùº ÏùΩÏñ¥ÏÑú ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú Îì±Î°ù\n",
    "\n",
    "API_KEY = os.getenv(\"API_KEY\")  # .envÏóêÏÑú API ÌÇ§ ÏùΩÏñ¥Ïò§Í∏∞\n",
    "\n",
    "if API_KEY is None:\n",
    "    raise ValueError(\"API_KEYÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. .env ÌååÏùºÏóê API_KEYÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c680f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in c:\\users\\lizzy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.13.2)\n",
      "Requirement already satisfied: async-timeout in c:\\users\\lizzy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\lizzy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\lizzy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lizzy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lizzy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lizzy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\lizzy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\lizzy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\lizzy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\lizzy\\appdata\\roaming\\python\\python311\\site-packages (from aiosignal>=1.4.0->aiohttp) (4.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install aiohttp async-timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df37e8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ nest_asyncio Ï†ÅÏö© ÏôÑÎ£å\n",
      "==========================================================================================\n",
      "üöÄ TMDB FULL SERIES + SEASONS COLLECTOR (2005-2015)\n",
      "==========================================================================================\n",
      "\n",
      "üìå 1Îã®Í≥Ñ: ID ÏàòÏßë\n",
      "‚ú® Ï¥ù ID: 42,844Í∞ú\n",
      "\n",
      "üìå 2Îã®Í≥Ñ: ÏÉÅÏÑ∏ + ÏãúÏ¶å ÏàòÏßë\n",
      "‚è± 100/42,844\n",
      "‚è± 200/42,844\n",
      "‚è± 300/42,844\n",
      "‚è± 400/42,844\n",
      "‚è± 500/42,844\n",
      "‚è± 600/42,844\n",
      "‚è± 700/42,844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 557\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[32m    554\u001b[39m \u001b[38;5;66;03m# RUN\u001b[39;00m\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py:323\u001b[39m, in \u001b[36mSelectSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    321\u001b[39m ready = []\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     r, w, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py:314\u001b[39m, in \u001b[36mSelectSelector._select\u001b[39m\u001b[34m(self, r, w, _, timeout)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     r, w, x = select.select(r, w, w, timeout)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w + x, []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# TMDB TV SERIES + SEASONS FULL ASYNC COLLECTOR\n",
    "# Series 51 cols / Seasons 15 cols (FINAL + poster_path)\n",
    "# 2005-01-01 ~ 2015-12-31\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import async_timeout\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==========================================================\n",
    "# Jupyter async ÏßÄÏõê\n",
    "# ==========================================================\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"‚úÖ nest_asyncio Ï†ÅÏö© ÏôÑÎ£å\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ==========================================================\n",
    "# Í∏∞Î≥∏ ÏÑ§Ï†ï\n",
    "# ==========================================================\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"TMDB_API_KEY missing\")\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "MAX_CALLS_PER_SECOND = 38\n",
    "TIMEOUT = 10\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "START_DATE = \"2005-01-01\"\n",
    "END_DATE = \"2015-12-31\"\n",
    "\n",
    "# temp\n",
    "SERIES_TEMP = Path(\"tv_series_temp.csv\")\n",
    "SEASONS_TEMP = Path(\"tv_seasons_temp.csv\")\n",
    "\n",
    "# final\n",
    "SERIES_CSV = \"tv_series_2005_2015_FULL.csv\"\n",
    "SEASONS_CSV = \"tv_seasons_2005_2015_FULL.csv\"\n",
    "SERIES_PARQ = \"tv_series_2005_2015_FULL.parquet\"\n",
    "SEASONS_PARQ = \"tv_seasons_2005_2015_FULL.parquet\"\n",
    "\n",
    "# ==========================================================\n",
    "# Rate limiter\n",
    "# ==========================================================\n",
    "class AsyncRateLimiter:\n",
    "    def __init__(self, max_calls, period=1.0):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = deque()\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def acquire(self):\n",
    "        async with self.lock:\n",
    "            now = asyncio.get_event_loop().time()\n",
    "\n",
    "            # Í∏∞Í∞Ñ Î∞ñ Ìò∏Ï∂ú Ï†úÍ±∞\n",
    "            while self.calls and now - self.calls[0] > self.period:\n",
    "                self.calls.popleft()\n",
    "\n",
    "            # Ï¥àÎãπ Ìò∏Ï∂ú Ïàò Ï¥àÍ≥º Ïãú sleep\n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                sleep_for = self.period - (now - self.calls[0]) + 0.01\n",
    "                await asyncio.sleep(max(0, sleep_for))\n",
    "\n",
    "                now = asyncio.get_event_loop().time()\n",
    "                while self.calls and now - self.calls[0] > self.period:\n",
    "                    self.calls.popleft()\n",
    "\n",
    "            self.calls.append(now)\n",
    "\n",
    "rate_limiter = AsyncRateLimiter(MAX_CALLS_PER_SECOND)\n",
    "\n",
    "# ==========================================================\n",
    "# TMDB GET\n",
    "# ==========================================================\n",
    "async def tmdb_get(session, path, params=None, retry=0):\n",
    "    if retry >= MAX_RETRIES:\n",
    "        return None\n",
    "\n",
    "    params = params or {}\n",
    "    params.setdefault(\"api_key\", API_KEY)\n",
    "    params.setdefault(\"language\", \"en-US\")\n",
    "\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "\n",
    "    await rate_limiter.acquire()\n",
    "\n",
    "    try:\n",
    "        async with async_timeout.timeout(TIMEOUT):\n",
    "            async with session.get(url, params=params, headers=HEADERS) as resp:\n",
    "                # Î†àÏù¥Ìä∏ Î¶¨Î∞ã\n",
    "                if resp.status == 429:\n",
    "                    wait = float(resp.headers.get(\"Retry-After\", 2)) * (2 ** retry)\n",
    "                    await asyncio.sleep(wait)\n",
    "                    return await tmdb_get(session, path, params, retry + 1)\n",
    "\n",
    "                # Not found\n",
    "                if resp.status == 404:\n",
    "                    return None\n",
    "\n",
    "                # ÏÑúÎ≤Ñ ÏóêÎü¨\n",
    "                if 500 <= resp.status < 600:\n",
    "                    await asyncio.sleep(2 ** retry)\n",
    "                    return await tmdb_get(session, path, params, retry + 1)\n",
    "\n",
    "                resp.raise_for_status()\n",
    "                return await resp.json()\n",
    "\n",
    "    except:\n",
    "        await asyncio.sleep(2 ** retry)\n",
    "        return await tmdb_get(session, path, params, retry + 1)\n",
    "\n",
    "# ==========================================================\n",
    "# discover page\n",
    "# ==========================================================\n",
    "async def fetch_discover_page(session, page, gte, lte):\n",
    "    params = {\n",
    "        \"sort_by\": \"popularity.desc\",\n",
    "        \"first_air_date.gte\": gte,\n",
    "        \"first_air_date.lte\": lte,\n",
    "        \"page\": page,\n",
    "        \"include_adult\": \"true\",\n",
    "    }\n",
    "    data = await tmdb_get(session, \"/discover/tv\", params)\n",
    "    if not data:\n",
    "        return [], 1, 0\n",
    "    return data.get(\"results\", []), data.get(\"total_pages\", 1), data.get(\"total_results\", 0)\n",
    "\n",
    "# ==========================================================\n",
    "# Î∂ÑÌï† ÏàòÏßë (500 page limit ÌöåÌîº)\n",
    "# ==========================================================\n",
    "async def collect_ids_in_range(session, start, end, depth=0):\n",
    "    indent = \"  \" * depth\n",
    "\n",
    "    res1, total_pages, total_results = await fetch_discover_page(session, 1, start, end)\n",
    "\n",
    "    if total_results == 0:\n",
    "        return set()\n",
    "\n",
    "    # 500ÌéòÏù¥ÏßÄ Ïù¥ÌïòÎ©¥ Í∑∏ÎåÄÎ°ú Ï†ÑÏàò ÏàòÏßë\n",
    "    if total_pages <= 500:\n",
    "        ids = {r[\"id\"] for r in res1}\n",
    "        if total_pages > 1:\n",
    "            tasks = [\n",
    "                fetch_discover_page(session, p, start, end)\n",
    "                for p in range(2, total_pages + 1)\n",
    "            ]\n",
    "            for coro in asyncio.as_completed(tasks):\n",
    "                r, _, _ = await coro\n",
    "                for x in r:\n",
    "                    ids.add(x[\"id\"])\n",
    "        return ids\n",
    "\n",
    "    # 500 ÌéòÏù¥ÏßÄ Ï¥àÍ≥º ‚Üí ÎÇ†Ïßú Î∞òÏúºÎ°ú Î∂ÑÌï†\n",
    "    start_dt = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    mid_dt = start_dt + (end_dt - start_dt) / 2\n",
    "    mid = mid_dt.strftime(\"%Y-%m-%d\")\n",
    "    right_start = (mid_dt + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    left = await collect_ids_in_range(session, start, mid, depth + 1)\n",
    "    right = await collect_ids_in_range(session, right_start, end, depth + 1)\n",
    "    return left | right\n",
    "\n",
    "# ==========================================================\n",
    "# helpers\n",
    "# ==========================================================\n",
    "def list_to_str(lst, key=\"name\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key, \"\")) for i in lst if i.get(key))\n",
    "\n",
    "def list_ids_to_str(lst, key=\"id\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key)) for i in lst if i.get(key))\n",
    "\n",
    "# ==========================================================\n",
    "# providers\n",
    "# ==========================================================\n",
    "async def fetch_providers(session, sid):\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/watch/providers\")\n",
    "    if not data:\n",
    "        return \"\"\n",
    "    out = set()\n",
    "    for cc in [\"US\", \"GB\", \"KR\"]:\n",
    "        if cc in data.get(\"results\", {}):\n",
    "            p = data[\"results\"][cc].get(\"flatrate\", [])\n",
    "            for x in p:\n",
    "                if x.get(\"provider_name\"):\n",
    "                    out.add(x[\"provider_name\"])\n",
    "    return \", \".join(sorted(out))\n",
    "\n",
    "# ==========================================================\n",
    "# single season (ÏãúÏ¶å 1Í∞ú ÏàòÏßë)\n",
    "# ==========================================================\n",
    "async def fetch_single_season(session, sid, sname, net_names, net_ids, meta):\n",
    "    sn = meta.get(\"season_number\")\n",
    "    if sn is None:\n",
    "        return None\n",
    "\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/season/{sn}\")\n",
    "\n",
    "    # ÏãúÏ¶å ÏÉÅÏÑ∏ Î™ª Í∞ÄÏ†∏Ïò§Î©¥ meta Í∏∞Î∞òÏúºÎ°ú fallback\n",
    "    if not data:\n",
    "        return {\n",
    "            \"_id\": f\"{sid}_{sn}\",\n",
    "            \"season_id\": meta.get(\"id\"),\n",
    "            \"series_id\": sid,\n",
    "            \"series_name\": sname,\n",
    "            \"season_number\": sn,\n",
    "            \"name\": meta.get(\"name\"),\n",
    "            \"air_date\": meta.get(\"air_date\"),\n",
    "            \"overview\": meta.get(\"overview\"),\n",
    "            \"vote_average\": meta.get(\"vote_average\"),\n",
    "            \"vote_count\": meta.get(\"vote_count\"),\n",
    "            \"network_names\": net_names,\n",
    "            \"network_ids\": net_ids,\n",
    "            \"total_episodes\": meta.get(\"episode_count\"),\n",
    "            \"avg_episode_runtime\": None,\n",
    "            \"poster_path\": meta.get(\"poster_path\"),\n",
    "        }\n",
    "\n",
    "    eps = data.get(\"episodes\") or []\n",
    "    runtimes = [ep.get(\"runtime\") for ep in eps if isinstance(ep.get(\"runtime\"), (int, float))]\n",
    "    avg_rt = sum(runtimes) / len(runtimes) if runtimes else None\n",
    "\n",
    "    return {\n",
    "        \"_id\": data.get(\"_id\") or f\"{sid}_{sn}\",\n",
    "        \"season_id\": data.get(\"id\"),\n",
    "        \"series_id\": sid,\n",
    "        \"series_name\": sname,\n",
    "        \"season_number\": data.get(\"season_number\"),\n",
    "        \"name\": data.get(\"name\"),\n",
    "        \"air_date\": data.get(\"air_date\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "        \"network_names\": net_names,\n",
    "        \"network_ids\": net_ids,\n",
    "        \"total_episodes\": len(eps),\n",
    "        \"avg_episode_runtime\": avg_rt,\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# TV ÏÉÅÏÑ∏ + ÏãúÏ¶å (Series 51 cols / Seasons 15 cols)\n",
    "# ==========================================================\n",
    "async def fetch_tv_details_and_seasons(session, sid):\n",
    "    params = {\"append_to_response\": \"reviews,keywords,aggregate_credits\"}\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}\", params)\n",
    "    if not data:\n",
    "        return None, []\n",
    "\n",
    "    genres = data.get(\"genres\") or []\n",
    "    networks = data.get(\"networks\") or []\n",
    "    last_ep = data.get(\"last_episode_to_air\") or {}\n",
    "\n",
    "    net_names = list_to_str(networks)\n",
    "    net_ids = list_ids_to_str(networks)\n",
    "\n",
    "    # ========== SERIES 51Í∞ú ==========\n",
    "    series = {\n",
    "        \"id\": data.get(\"id\"),\n",
    "        \"title\": data.get(\"name\"),\n",
    "        \"type\": \"tv_series\",\n",
    "        \"adult\": data.get(\"adult\"),\n",
    "        \"backdrop_path\": data.get(\"backdrop_path\"),\n",
    "        \"created_by\": list_to_str(data.get(\"created_by\")),\n",
    "        \"episode_run_time\": (\n",
    "            \", \".join(map(str, data.get(\"episode_run_time\", [])))\n",
    "            if data.get(\"episode_run_time\") else \"\"\n",
    "        ),\n",
    "        \"first_air_date\": data.get(\"first_air_date\"),\n",
    "        \"genres\": list_to_str(genres),\n",
    "        \"genre_ids\": \", \".join(str(g.get(\"id\")) for g in genres if g.get(\"id\")),\n",
    "        \"homepage\": data.get(\"homepage\"),\n",
    "        \"in_production\": data.get(\"in_production\"),\n",
    "        \"languages\": \", \".join(data.get(\"languages\", [])) if data.get(\"languages\") else \"\",\n",
    "        \"last_air_date\": data.get(\"last_air_date\"),\n",
    "\n",
    "        \"last_episode_to_air_id\": last_ep.get(\"id\"),\n",
    "        \"last_episode_to_air_name\": last_ep.get(\"name\"),\n",
    "        \"last_episode_to_air_overview\": last_ep.get(\"overview\"),\n",
    "        \"last_episode_to_air_vote_average\": last_ep.get(\"vote_average\"),\n",
    "        \"last_episode_to_air_vote_count\": last_ep.get(\"vote_count\"),\n",
    "        \"last_episode_to_air_air_date\": last_ep.get(\"air_date\"),\n",
    "        \"last_episode_to_air_episode_number\": last_ep.get(\"episode_number\"),\n",
    "        \"last_episode_to_air_production_code\": last_ep.get(\"production_code\"),\n",
    "        \"last_episode_to_air_runtime\": last_ep.get(\"runtime\"),\n",
    "        \"last_episode_to_air_season_number\": last_ep.get(\"season_number\"),\n",
    "        \"last_episode_to_air_show_id\": last_ep.get(\"show_id\"),\n",
    "        \"last_episode_to_air_still_path\": last_ep.get(\"still_path\"),\n",
    "\n",
    "        \"next_episode_to_air\": \"\" if not data.get(\"next_episode_to_air\") else str(data.get(\"next_episode_to_air\")),\n",
    "        \"networks\": net_names,\n",
    "        \"number_of_episodes\": data.get(\"number_of_episodes\"),\n",
    "        \"number_of_seasons\": data.get(\"number_of_seasons\"),\n",
    "\n",
    "        \"origin_country\": \", \".join(data.get(\"origin_country\", [])) if data.get(\"origin_country\") else \"\",\n",
    "        \"original_language\": data.get(\"original_language\"),\n",
    "        \"original_name\": data.get(\"original_name\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"popularity\": data.get(\"popularity\"),\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "\n",
    "        \"production_companies\": list_to_str(data.get(\"production_companies\")),\n",
    "        \"production_countries\": list_to_str(data.get(\"production_countries\")),\n",
    "\n",
    "        \"seasons\": \"; \".join(\n",
    "            f\"S{s.get('season_number')}: {s.get('name')} ({s.get('episode_count')} eps)\"\n",
    "            for s in (data.get(\"seasons\") or [])\n",
    "            if s.get(\"season_number\") is not None\n",
    "        ),\n",
    "\n",
    "        \"spoken_languages\": list_to_str(data.get(\"spoken_languages\")),\n",
    "        \"status\": data.get(\"status\"),\n",
    "        \"tagline\": data.get(\"tagline\"),\n",
    "        \"type_detail\": data.get(\"type\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "    }\n",
    "\n",
    "    # Î¶¨Î∑∞\n",
    "    rv_block = data.get(\"reviews\", {})\n",
    "    rv_items = rv_block.get(\"results\", []) if isinstance(rv_block, dict) else []\n",
    "    rev = []\n",
    "    for r in rv_items[:5]:\n",
    "        author = r.get(\"author\", \"\")\n",
    "        rating = r.get(\"author_details\", {}).get(\"rating\")\n",
    "        rt = f\"({rating})\" if rating is not None else \"\"\n",
    "        cont = (r.get(\"content\") or \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \")[:200]\n",
    "        rev.append(f\"{author}{rt}: {cont}\")\n",
    "    series[\"review\"] = \" || \".join(rev)\n",
    "\n",
    "    # ÌÇ§ÏõåÎìú\n",
    "    kw_block = data.get(\"keywords\", {})\n",
    "    kw_items = kw_block.get(\"results\", []) if isinstance(kw_block, dict) else []\n",
    "    series[\"keyword\"] = \", \".join([k.get(\"name\") for k in kw_items[:20] if k.get(\"name\")])\n",
    "\n",
    "    # top cast & crew\n",
    "    credits = data.get(\"aggregate_credits\", {})\n",
    "    cast = credits.get(\"cast\") or []\n",
    "    crew = credits.get(\"crew\") or []\n",
    "\n",
    "    series[\"top_cast\"] = \", \".join([c.get(\"name\") for c in cast[:4] if c.get(\"name\")])\n",
    "\n",
    "    dirs = set()\n",
    "    wrs = set()\n",
    "    for c in crew:\n",
    "        nm = c.get(\"name\")\n",
    "        if not nm:\n",
    "            continue\n",
    "        for job in (c.get(\"jobs\") or []):\n",
    "            jn = job.get(\"job\", \"\")\n",
    "            if \"Director\" in jn:\n",
    "                dirs.add(nm)\n",
    "            if jn in [\"Writer\", \"Screenplay\", \"Story\"]:\n",
    "                wrs.add(nm)\n",
    "\n",
    "    series[\"directors\"] = \", \".join(sorted(dirs)[:10])\n",
    "    series[\"writers\"] = \", \".join(sorted(wrs)[:10])\n",
    "\n",
    "    # providers\n",
    "    series[\"providers\"] = await fetch_providers(session, sid)\n",
    "\n",
    "    # ÏãúÏ¶å Î¶¨Ïä§Ìä∏\n",
    "    seasons_meta = data.get(\"seasons\") or []\n",
    "    tasks = [\n",
    "        fetch_single_season(session, sid, data.get(\"name\") or \"\", net_names, net_ids, s)\n",
    "        for s in seasons_meta\n",
    "        if s.get(\"season_number\") is not None\n",
    "    ]\n",
    "\n",
    "    season_records = []\n",
    "    for coro in asyncio.as_completed(tasks):\n",
    "        x = await coro\n",
    "        if x:\n",
    "            season_records.append(x)\n",
    "\n",
    "    return series, season_records\n",
    "\n",
    "# ==========================================================\n",
    "# ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò\n",
    "# ==========================================================\n",
    "def fill_series_gaps(series_df, seasons_df):\n",
    "    print(\"\\nüìä ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ ÏãúÏûë\")\n",
    "\n",
    "    # episode_run_time\n",
    "    rtmap = seasons_df.groupby(\"series_id\")[\"avg_episode_runtime\"].mean().round()\n",
    "    mask = series_df[\"episode_run_time\"].astype(str).str.strip().isin([\"\", \"nan\"])\n",
    "    series_df.loc[mask, \"episode_run_time\"] = (\n",
    "        series_df.loc[mask, \"id\"].map(rtmap).fillna(0).astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    # number_of_episodes\n",
    "    epmap = seasons_df.groupby(\"series_id\")[\"total_episodes\"].sum()\n",
    "    mask2 = series_df[\"number_of_episodes\"].isna()\n",
    "    series_df.loc[mask2, \"number_of_episodes\"] = (\n",
    "        series_df.loc[mask2, \"id\"].map(epmap)\n",
    "    )\n",
    "\n",
    "    # number_of_seasons\n",
    "    seasmap = seasons_df.groupby(\"series_id\")[\"season_number\"].nunique()\n",
    "    mask3 = series_df[\"number_of_seasons\"].isna()\n",
    "    series_df.loc[mask3, \"number_of_seasons\"] = (\n",
    "        series_df.loc[mask3, \"id\"].map(seasmap)\n",
    "    )\n",
    "\n",
    "    # last_air_date\n",
    "    lastair = seasons_df.sort_values(\"air_date\").groupby(\"series_id\")[\"air_date\"].last()\n",
    "    ladmask = series_df[\"last_air_date\"].astype(str).str.strip().isin([\"\", \"nan\", \"NaT\"])\n",
    "    series_df.loc[ladmask, \"last_air_date\"] = series_df.loc[ladmask, \"id\"].map(lastair)\n",
    "\n",
    "    print(\"‚úÖ Î≥¥ÏôÑ ÏôÑÎ£å\\n\")\n",
    "    return series_df\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN\n",
    "# ==========================================================\n",
    "async def main():\n",
    "    print(\"=\" * 90)\n",
    "    print(\"üöÄ TMDB FULL SERIES + SEASONS COLLECTOR (2005-2015)\")\n",
    "    print(\"=\" * 90)\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    # Í∏∞Ï°¥ temp ÏÇ≠Ï†ú\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # 1) ID ÏàòÏßë\n",
    "        print(\"\\nüìå 1Îã®Í≥Ñ: ID ÏàòÏßë\")\n",
    "        ids = sorted(list(await collect_ids_in_range(session, START_DATE, END_DATE)))\n",
    "        print(f\"‚ú® Ï¥ù ID: {len(ids):,}Í∞ú\")\n",
    "\n",
    "        # 2) ÏÉÅÏÑ∏ + ÏãúÏ¶å ÏàòÏßë\n",
    "        print(\"\\nüìå 2Îã®Í≥Ñ: ÏÉÅÏÑ∏ + ÏãúÏ¶å ÏàòÏßë\")\n",
    "        batch_size = 500\n",
    "        sem = asyncio.Semaphore(25)\n",
    "\n",
    "        async def fetch_one(i):\n",
    "            async with sem:\n",
    "                return await fetch_tv_details_and_seasons(session, i)\n",
    "\n",
    "        processed = 0\n",
    "        for start in range(0, len(ids), batch_size):\n",
    "            part = ids[start:start+batch_size]\n",
    "            tasks = [fetch_one(x) for x in part]\n",
    "\n",
    "            bs = []\n",
    "            bse = []\n",
    "\n",
    "            for coro in asyncio.as_completed(tasks):\n",
    "                s, ss = await coro\n",
    "                if s:\n",
    "                    bs.append(s)\n",
    "                if ss:\n",
    "                    bse.extend(ss)\n",
    "\n",
    "                processed += 1\n",
    "                if processed % 100 == 0:\n",
    "                    print(f\"‚è± {processed:,}/{len(ids):,}\")\n",
    "\n",
    "            # flush\n",
    "            if bs:\n",
    "                pd.DataFrame(bs).to_csv(\n",
    "                    SERIES_TEMP, mode=\"a\", header=not SERIES_TEMP.exists(),\n",
    "                    index=False, encoding=\"utf-8-sig\"\n",
    "                )\n",
    "            if bse:\n",
    "                pd.DataFrame(bse).to_csv(\n",
    "                    SEASONS_TEMP, mode=\"a\", header=not SEASONS_TEMP.exists(),\n",
    "                    index=False, encoding=\"utf-8-sig\"\n",
    "                )\n",
    "\n",
    "    # ==========================================================\n",
    "    # 3) temp ‚Üí final DF\n",
    "    # ==========================================================\n",
    "    df_series = pd.read_csv(SERIES_TEMP)\n",
    "    df_seasons = pd.read_csv(SEASONS_TEMP) if SEASONS_TEMP.exists() else pd.DataFrame()\n",
    "\n",
    "    SERIES_COLS = [\n",
    "        \"id\",\"title\",\"type\",\"adult\",\"backdrop_path\",\"created_by\",\"episode_run_time\",\n",
    "        \"first_air_date\",\"genres\",\"genre_ids\",\"homepage\",\"in_production\",\"languages\",\n",
    "        \"last_air_date\",\"last_episode_to_air_id\",\"last_episode_to_air_name\",\n",
    "        \"last_episode_to_air_overview\",\"last_episode_to_air_vote_average\",\n",
    "        \"last_episode_to_air_vote_count\",\"last_episode_to_air_air_date\",\n",
    "        \"last_episode_to_air_episode_number\",\"last_episode_to_air_production_code\",\n",
    "        \"last_episode_to_air_runtime\",\"last_episode_to_air_season_number\",\n",
    "        \"last_episode_to_air_show_id\",\"last_episode_to_air_still_path\",\n",
    "        \"next_episode_to_air\",\"networks\",\"number_of_episodes\",\"number_of_seasons\",\n",
    "        \"origin_country\",\"original_language\",\"original_name\",\"overview\",\"popularity\",\n",
    "        \"poster_path\",\n",
    "        \"production_companies\",\"production_countries\",\"seasons\",\"spoken_languages\",\n",
    "        \"status\",\"tagline\",\"type_detail\",\"vote_average\",\"vote_count\",\"review\",\n",
    "        \"keyword\",\"top_cast\",\"directors\",\"writers\",\"providers\"\n",
    "    ]\n",
    "\n",
    "    SEASON_COLS = [\n",
    "        \"_id\",\"season_id\",\"series_id\",\"series_name\",\"season_number\",\"name\",\"air_date\",\n",
    "        \"overview\",\"vote_average\",\"vote_count\",\"network_names\",\"network_ids\",\n",
    "        \"total_episodes\",\"avg_episode_runtime\",\"poster_path\"\n",
    "    ]\n",
    "\n",
    "    df_series = df_series[SERIES_COLS].drop_duplicates(subset=[\"id\"])\n",
    "    if not df_seasons.empty:\n",
    "        df_seasons = df_seasons[SEASON_COLS].drop_duplicates(subset=[\"series_id\",\"season_number\"])\n",
    "\n",
    "    # 4) ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ\n",
    "    if not df_seasons.empty:\n",
    "        df_series = fill_series_gaps(df_series, df_seasons)\n",
    "\n",
    "    # 5) Ï†ÄÏû•\n",
    "    df_series.to_csv(SERIES_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    df_seasons.to_csv(SEASONS_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    try:\n",
    "        df_series.to_parquet(SERIES_PARQ, index=False)\n",
    "        df_seasons.to_parquet(SEASONS_PARQ, index=False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    elapsed = (datetime.now() - t0).total_seconds()/60\n",
    "\n",
    "    print(\"=\"*90)\n",
    "    print(\"üéâ ÏàòÏßë ÏôÑÎ£å\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"üìå Series Ïàò: {len(df_series):,}Í∞ú\")\n",
    "    print(f\"üìå Season Ïàò: {len(df_seasons):,}Í∞ú\")\n",
    "    print(f\"üìå Series Ïª¨Îüº Ïàò: {len(SERIES_COLS)}\")\n",
    "    print(f\"üìå Season  Ïª¨Îüº Ïàò: {len(SEASON_COLS)}\")\n",
    "    print(f\"‚è± Ï¥ù ÏÜåÏöîÏãúÍ∞Ñ: {elapsed:.1f}Î∂Ñ\")\n",
    "    print(\"=\"*90)\n",
    "\n",
    "# ==========================================================\n",
    "# RUN\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa347b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ nest_asyncio Ï†ÅÏö© ÏôÑÎ£å\n",
      "==========================================================================================\n",
      "üöÄ TMDB FULL SERIES + SEASONS COLLECTOR (OPTIMIZED)\n",
      "==========================================================================================\n",
      "\n",
      "üìå 1Îã®Í≥Ñ: ID ÏàòÏßë\n",
      "‚ú® Ï¥ù ID: 43,869Í∞ú\n",
      "\n",
      "üìå 2Îã®Í≥Ñ: ÏÉÅÏÑ∏ + ÏãúÏ¶å ÏàòÏßë\n",
      "‚è± 100/43,869\n",
      "‚è± 200/43,869\n",
      "‚è± 300/43,869\n",
      "‚è± 400/43,869\n",
      "‚è± 500/43,869\n",
      "‚è± 600/43,869\n",
      "‚è± 700/43,869\n",
      "‚è± 800/43,869\n",
      "‚è± 900/43,869\n",
      "‚è± 1,000/43,869\n",
      "‚è± 1,100/43,869\n",
      "‚è± 1,200/43,869\n",
      "‚è± 1,300/43,869\n",
      "‚è± 1,400/43,869\n",
      "‚è± 1,500/43,869\n",
      "‚è± 1,600/43,869\n",
      "‚è± 1,700/43,869\n",
      "‚è± 1,800/43,869\n",
      "‚è± 1,900/43,869\n",
      "‚è± 2,000/43,869\n",
      "‚è± 2,100/43,869\n",
      "‚è± 2,200/43,869\n",
      "‚è± 2,300/43,869\n",
      "‚è± 2,400/43,869\n",
      "‚è± 2,500/43,869\n",
      "‚è± 2,600/43,869\n",
      "‚è± 2,700/43,869\n",
      "‚è± 2,800/43,869\n",
      "‚è± 2,900/43,869\n",
      "‚è± 3,000/43,869\n",
      "‚è± 3,100/43,869\n",
      "‚è± 3,200/43,869\n",
      "‚è± 3,300/43,869\n",
      "‚è± 3,400/43,869\n",
      "‚è± 3,500/43,869\n",
      "‚è± 3,600/43,869\n",
      "‚è± 3,700/43,869\n",
      "‚è± 3,800/43,869\n",
      "‚è± 3,900/43,869\n",
      "‚è± 4,000/43,869\n",
      "‚è± 4,100/43,869\n",
      "‚è± 4,200/43,869\n",
      "‚è± 4,300/43,869\n",
      "‚è± 4,400/43,869\n",
      "‚è± 4,500/43,869\n",
      "‚è± 4,600/43,869\n",
      "‚è± 4,700/43,869\n",
      "‚è± 4,800/43,869\n",
      "‚è± 4,900/43,869\n",
      "‚è± 5,000/43,869\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 579\u001b[39m\n\u001b[32m    575\u001b[39m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[32m    576\u001b[39m \u001b[38;5;66;03m# RUN\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py:323\u001b[39m, in \u001b[36mSelectSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    321\u001b[39m ready = []\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     r, w, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py:314\u001b[39m, in \u001b[36mSelectSelector._select\u001b[39m\u001b[34m(self, r, w, _, timeout)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     r, w, x = select.select(r, w, w, timeout)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w + x, []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# TMDB TV SERIES + SEASONS FULL ASYNC COLLECTOR (OPTIMIZED)\n",
    "# Series 51 cols / Seasons 15 cols\n",
    "# 2005-01-01 ~ 2015-12-31\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==========================================================\n",
    "# Jupyter async ÏßÄÏõê\n",
    "# ==========================================================\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"‚úÖ nest_asyncio Ï†ÅÏö© ÏôÑÎ£å\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ==========================================================\n",
    "# Í∏∞Î≥∏ ÏÑ§Ï†ï (ÏµúÏ†ÅÌôî)\n",
    "# ==========================================================\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"TMDB_API_KEY missing\")\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "MAX_CALLS_PER_SECOND = 45  # 38 ‚Üí 45Î°ú Ï¶ùÍ∞Ä\n",
    "TIMEOUT = 15  # 10 ‚Üí 15Î°ú Ï¶ùÍ∞Ä (ÌÉÄÏûÑÏïÑÏõÉ Ïó¨Ïú†)\n",
    "MAX_RETRIES = 3  # 5 ‚Üí 3ÏúºÎ°ú Í∞êÏÜå (Îπ†Î•∏ Ïã§Ìå®)\n",
    "\n",
    "START_DATE = \"2005-01-01\"\n",
    "END_DATE = \"2015-12-31\"\n",
    "\n",
    "# temp\n",
    "SERIES_TEMP = Path(\"tv_series_temp.csv\")\n",
    "SEASONS_TEMP = Path(\"tv_seasons_temp.csv\")\n",
    "\n",
    "# final\n",
    "SERIES_CSV = \"tv_series_2005_2015_FULL.csv\"\n",
    "SEASONS_CSV = \"tv_seasons_2005_2015_FULL.csv\"\n",
    "SERIES_PARQ = \"tv_series_2005_2015_FULL.parquet\"\n",
    "SEASONS_PARQ = \"tv_seasons_2005_2015_FULL.parquet\"\n",
    "\n",
    "# ==========================================================\n",
    "# Rate limiter (Í∞úÏÑ†)\n",
    "# ==========================================================\n",
    "class AsyncRateLimiter:\n",
    "    def __init__(self, max_calls, period=1.0):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = deque()\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def acquire(self):\n",
    "        async with self.lock:\n",
    "            now = asyncio.get_event_loop().time()\n",
    "            \n",
    "            # Í∏∞Í∞Ñ Î∞ñ Ìò∏Ï∂ú Ï†úÍ±∞\n",
    "            while self.calls and now - self.calls[0] > self.period:\n",
    "                self.calls.popleft()\n",
    "\n",
    "            # Ï¥àÍ≥º Ïãú ÏµúÏÜåÌïúÎßå ÎåÄÍ∏∞\n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                sleep_for = self.period - (now - self.calls[0])\n",
    "                if sleep_for > 0:\n",
    "                    await asyncio.sleep(sleep_for)\n",
    "                    now = asyncio.get_event_loop().time()\n",
    "                    while self.calls and now - self.calls[0] > self.period:\n",
    "                        self.calls.popleft()\n",
    "\n",
    "            self.calls.append(now)\n",
    "\n",
    "rate_limiter = AsyncRateLimiter(MAX_CALLS_PER_SECOND)\n",
    "\n",
    "# ==========================================================\n",
    "# TMDB GET (ÏµúÏ†ÅÌôî)\n",
    "# ==========================================================\n",
    "async def tmdb_get(session, path, params=None, retry=0):\n",
    "    if retry >= MAX_RETRIES:\n",
    "        return None\n",
    "\n",
    "    params = params or {}\n",
    "    params.setdefault(\"api_key\", API_KEY)\n",
    "    params.setdefault(\"language\", \"en-US\")\n",
    "\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "\n",
    "    await rate_limiter.acquire()\n",
    "\n",
    "    try:\n",
    "        async with session.get(url, params=params, headers=HEADERS, timeout=TIMEOUT) as resp:\n",
    "            # Î†àÏù¥Ìä∏ Î¶¨Î∞ã\n",
    "            if resp.status == 429:\n",
    "                wait = float(resp.headers.get(\"Retry-After\", 1))\n",
    "                await asyncio.sleep(wait)\n",
    "                return await tmdb_get(session, path, params, retry + 1)\n",
    "\n",
    "            # Not found\n",
    "            if resp.status == 404:\n",
    "                return None\n",
    "\n",
    "            # ÏÑúÎ≤Ñ ÏóêÎü¨ - Îπ†Î•∏ Ïû¨ÏãúÎèÑ\n",
    "            if 500 <= resp.status < 600:\n",
    "                await asyncio.sleep(0.5 * (retry + 1))\n",
    "                return await tmdb_get(session, path, params, retry + 1)\n",
    "\n",
    "            resp.raise_for_status()\n",
    "            return await resp.json()\n",
    "    except asyncio.TimeoutError:\n",
    "        stats[\"errors\"] += 1\n",
    "        if retry < MAX_RETRIES - 1:\n",
    "            await asyncio.sleep(1.0 + retry * 0.5) # ÎåÄÍ∏∞ ÏãúÍ∞ÑÏùÑ Ï¢Ä Îçî ÌôïÎ≥¥\n",
    "            return await tmdb_get(session, path, params, retry + 1)\n",
    "        # ÏµúÏ¢Ö Ïã§Ìå® Ïãú\n",
    "        print(f\"‚ùå [Timeout Fail] {url} after {MAX_RETRIES} retries\") # Î°úÍ∑∏ Ï∂îÍ∞Ä\n",
    "        return None\n",
    "\n",
    "# ==========================================================\n",
    "# discover page\n",
    "# ==========================================================\n",
    "async def fetch_discover_page(session, page, gte, lte):\n",
    "    params = {\n",
    "        \"sort_by\": \"popularity.desc\",\n",
    "        \"first_air_date.gte\": gte,\n",
    "        \"first_air_date.lte\": lte,\n",
    "        \"page\": page,\n",
    "        \"include_adult\": \"true\",\n",
    "    }\n",
    "    data = await tmdb_get(session, \"/discover/tv\", params)\n",
    "    if not data:\n",
    "        return [], 1, 0\n",
    "    return data.get(\"results\", []), data.get(\"total_pages\", 1), data.get(\"total_results\", 0)\n",
    "\n",
    "# ==========================================================\n",
    "# Î∂ÑÌï† ÏàòÏßë (Î≥ëÎ†¨ Ï≤òÎ¶¨ ÏµúÏ†ÅÌôî)\n",
    "# ==========================================================\n",
    "async def collect_ids_in_range(session, start, end, depth=0):\n",
    "    res1, total_pages, total_results = await fetch_discover_page(session, 1, start, end)\n",
    "\n",
    "    if total_results == 0:\n",
    "        return set()\n",
    "\n",
    "    # 500ÌéòÏù¥ÏßÄ Ïù¥ÌïòÎ©¥ Î≥ëÎ†¨Î°ú Ï†ÑÏàò ÏàòÏßë\n",
    "    if total_pages <= 500:\n",
    "        ids = {r[\"id\"] for r in res1}\n",
    "        if total_pages > 1:\n",
    "            # ÎèôÏãúÏóê Ïó¨Îü¨ ÌéòÏù¥ÏßÄ ÏàòÏßë\n",
    "            tasks = [\n",
    "                fetch_discover_page(session, p, start, end)\n",
    "                for p in range(2, total_pages + 1)\n",
    "            ]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            for r in results:\n",
    "                if isinstance(r, tuple):\n",
    "                    for x in r[0]:\n",
    "                        ids.add(x[\"id\"])\n",
    "        return ids\n",
    "\n",
    "    # 500 ÌéòÏù¥ÏßÄ Ï¥àÍ≥º ‚Üí ÎÇ†Ïßú Î∂ÑÌï† (Î≥ëÎ†¨ Ï≤òÎ¶¨)\n",
    "    start_dt = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    mid_dt = start_dt + (end_dt - start_dt) / 2\n",
    "    mid = mid_dt.strftime(\"%Y-%m-%d\")\n",
    "    right_start = (mid_dt + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Îëê Íµ¨Í∞ÑÏùÑ Î≥ëÎ†¨Î°ú ÏàòÏßë\n",
    "    left, right = await asyncio.gather(\n",
    "        collect_ids_in_range(session, start, mid, depth + 1),\n",
    "        collect_ids_in_range(session, right_start, end, depth + 1)\n",
    "    )\n",
    "    return left | right\n",
    "\n",
    "# ==========================================================\n",
    "# helpers\n",
    "# ==========================================================\n",
    "def list_to_str(lst, key=\"name\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key, \"\")) for i in lst if i.get(key))\n",
    "\n",
    "def list_ids_to_str(lst, key=\"id\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key)) for i in lst if i.get(key))\n",
    "\n",
    "# ==========================================================\n",
    "# providers (Í∞ÑÏÜåÌôî)\n",
    "# ==========================================================\n",
    "async def fetch_providers(session, sid):\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/watch/providers\")\n",
    "    if not data:\n",
    "        return \"\"\n",
    "    out = set()\n",
    "    results = data.get(\"results\", {})\n",
    "    for cc in [\"US\", \"GB\", \"KR\"]:\n",
    "        if cc in results:\n",
    "            for x in results[cc].get(\"flatrate\", []):\n",
    "                if x.get(\"provider_name\"):\n",
    "                    out.add(x[\"provider_name\"])\n",
    "    return \", \".join(sorted(out))\n",
    "\n",
    "# ==========================================================\n",
    "# single season (ÏµúÏ†ÅÌôî)\n",
    "# ==========================================================\n",
    "async def fetch_single_season(session, sid, sname, net_names, net_ids, meta):\n",
    "    sn = meta.get(\"season_number\")\n",
    "    if sn is None:\n",
    "        return None\n",
    "\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/season/{sn}\")\n",
    "\n",
    "    # fallback\n",
    "    if not data:\n",
    "        return {\n",
    "            \"_id\": f\"{sid}_{sn}\",\n",
    "            \"season_id\": meta.get(\"id\"),\n",
    "            \"series_id\": sid,\n",
    "            \"series_name\": sname,\n",
    "            \"season_number\": sn,\n",
    "            \"name\": meta.get(\"name\"),\n",
    "            \"air_date\": meta.get(\"air_date\"),\n",
    "            \"overview\": meta.get(\"overview\"),\n",
    "            \"vote_average\": meta.get(\"vote_average\"),\n",
    "            \"vote_count\": meta.get(\"vote_count\"),\n",
    "            \"network_names\": net_names,\n",
    "            \"network_ids\": net_ids,\n",
    "            \"total_episodes\": meta.get(\"episode_count\"),\n",
    "            \"avg_episode_runtime\": None,\n",
    "            \"poster_path\": meta.get(\"poster_path\"),\n",
    "        }\n",
    "\n",
    "    eps = data.get(\"episodes\") or []\n",
    "    runtimes = [ep.get(\"runtime\") for ep in eps if isinstance(ep.get(\"runtime\"), (int, float))]\n",
    "    avg_rt = sum(runtimes) / len(runtimes) if runtimes else None\n",
    "\n",
    "    return {\n",
    "        \"_id\": data.get(\"_id\") or f\"{sid}_{sn}\",\n",
    "        \"season_id\": data.get(\"id\"),\n",
    "        \"series_id\": sid,\n",
    "        \"series_name\": sname,\n",
    "        \"season_number\": data.get(\"season_number\"),\n",
    "        \"name\": data.get(\"name\"),\n",
    "        \"air_date\": data.get(\"air_date\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "        \"network_names\": net_names,\n",
    "        \"network_ids\": net_ids,\n",
    "        \"total_episodes\": len(eps),\n",
    "        \"avg_episode_runtime\": avg_rt,\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# TV ÏÉÅÏÑ∏ + ÏãúÏ¶å (ÏµúÏ†ÅÌôî - gather ÏÇ¨Ïö©)\n",
    "# ==========================================================\n",
    "async def fetch_tv_details_and_seasons(session, sid):\n",
    "    params = {\"append_to_response\": \"reviews,keywords,aggregate_credits\"}\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}\", params)\n",
    "    if not data:\n",
    "        return None, []\n",
    "\n",
    "    genres = data.get(\"genres\") or []\n",
    "    networks = data.get(\"networks\") or []\n",
    "    last_ep = data.get(\"last_episode_to_air\") or {}\n",
    "\n",
    "    net_names = list_to_str(networks)\n",
    "    net_ids = list_ids_to_str(networks)\n",
    "\n",
    "    # ========== SERIES 51Í∞ú ==========\n",
    "    series = {\n",
    "        \"id\": data.get(\"id\"),\n",
    "        \"title\": data.get(\"name\"),\n",
    "        \"type\": \"tv_series\",\n",
    "        \"adult\": data.get(\"adult\"),\n",
    "        \"backdrop_path\": data.get(\"backdrop_path\"),\n",
    "        \"created_by\": list_to_str(data.get(\"created_by\")),\n",
    "        \"episode_run_time\": (\n",
    "            \", \".join(map(str, data.get(\"episode_run_time\", [])))\n",
    "            if data.get(\"episode_run_time\") else \"\"\n",
    "        ),\n",
    "        \"first_air_date\": data.get(\"first_air_date\"),\n",
    "        \"genres\": list_to_str(genres),\n",
    "        \"genre_ids\": \", \".join(str(g.get(\"id\")) for g in genres if g.get(\"id\")),\n",
    "        \"homepage\": data.get(\"homepage\"),\n",
    "        \"in_production\": data.get(\"in_production\"),\n",
    "        \"languages\": \", \".join(data.get(\"languages\", [])) if data.get(\"languages\") else \"\",\n",
    "        \"last_air_date\": data.get(\"last_air_date\"),\n",
    "\n",
    "        \"last_episode_to_air_id\": last_ep.get(\"id\"),\n",
    "        \"last_episode_to_air_name\": last_ep.get(\"name\"),\n",
    "        \"last_episode_to_air_overview\": last_ep.get(\"overview\"),\n",
    "        \"last_episode_to_air_vote_average\": last_ep.get(\"vote_average\"),\n",
    "        \"last_episode_to_air_vote_count\": last_ep.get(\"vote_count\"),\n",
    "        \"last_episode_to_air_air_date\": last_ep.get(\"air_date\"),\n",
    "        \"last_episode_to_air_episode_number\": last_ep.get(\"episode_number\"),\n",
    "        \"last_episode_to_air_production_code\": last_ep.get(\"production_code\"),\n",
    "        \"last_episode_to_air_runtime\": last_ep.get(\"runtime\"),\n",
    "        \"last_episode_to_air_season_number\": last_ep.get(\"season_number\"),\n",
    "        \"last_episode_to_air_show_id\": last_ep.get(\"show_id\"),\n",
    "        \"last_episode_to_air_still_path\": last_ep.get(\"still_path\"),\n",
    "\n",
    "        \"next_episode_to_air\": \"\" if not data.get(\"next_episode_to_air\") else str(data.get(\"next_episode_to_air\")),\n",
    "        \"networks\": net_names,\n",
    "        \"number_of_episodes\": data.get(\"number_of_episodes\"),\n",
    "        \"number_of_seasons\": data.get(\"number_of_seasons\"),\n",
    "\n",
    "        \"origin_country\": \", \".join(data.get(\"origin_country\", [])) if data.get(\"origin_country\") else \"\",\n",
    "        \"original_language\": data.get(\"original_language\"),\n",
    "        \"original_name\": data.get(\"original_name\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"popularity\": data.get(\"popularity\"),\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "\n",
    "        \"production_companies\": list_to_str(data.get(\"production_companies\")),\n",
    "        \"production_countries\": list_to_str(data.get(\"production_countries\")),\n",
    "\n",
    "        \"seasons\": \"; \".join(\n",
    "            f\"S{s.get('season_number')}: {s.get('name')} ({s.get('episode_count')} eps)\"\n",
    "            for s in (data.get(\"seasons\") or [])\n",
    "            if s.get(\"season_number\") is not None\n",
    "        ),\n",
    "\n",
    "        \"spoken_languages\": list_to_str(data.get(\"spoken_languages\")),\n",
    "        \"status\": data.get(\"status\"),\n",
    "        \"tagline\": data.get(\"tagline\"),\n",
    "        \"type_detail\": data.get(\"type\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "    }\n",
    "\n",
    "    # Î¶¨Î∑∞\n",
    "    rv_block = data.get(\"reviews\", {})\n",
    "    rv_items = rv_block.get(\"results\", []) if isinstance(rv_block, dict) else []\n",
    "    rev = []\n",
    "    for r in rv_items[:5]:\n",
    "        author = r.get(\"author\", \"\")\n",
    "        rating = r.get(\"author_details\", {}).get(\"rating\")\n",
    "        rt = f\"({rating})\" if rating is not None else \"\"\n",
    "        cont = (r.get(\"content\") or \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \")[:200]\n",
    "        rev.append(f\"{author}{rt}: {cont}\")\n",
    "    series[\"review\"] = \" || \".join(rev)\n",
    "\n",
    "    # ÌÇ§ÏõåÎìú\n",
    "    kw_block = data.get(\"keywords\", {})\n",
    "    kw_items = kw_block.get(\"results\", []) if isinstance(kw_block, dict) else []\n",
    "    series[\"keyword\"] = \", \".join([k.get(\"name\") for k in kw_items[:20] if k.get(\"name\")])\n",
    "\n",
    "    # top cast & crew\n",
    "    credits = data.get(\"aggregate_credits\", {})\n",
    "    cast = credits.get(\"cast\") or []\n",
    "    crew = credits.get(\"crew\") or []\n",
    "\n",
    "    series[\"top_cast\"] = \", \".join([c.get(\"name\") for c in cast[:4] if c.get(\"name\")])\n",
    "\n",
    "    dirs = set()\n",
    "    wrs = set()\n",
    "    for c in crew:\n",
    "        nm = c.get(\"name\")\n",
    "        if not nm:\n",
    "            continue\n",
    "        for job in (c.get(\"jobs\") or []):\n",
    "            jn = job.get(\"job\", \"\")\n",
    "            if \"Director\" in jn:\n",
    "                dirs.add(nm)\n",
    "            if jn in [\"Writer\", \"Screenplay\", \"Story\"]:\n",
    "                wrs.add(nm)\n",
    "\n",
    "    series[\"directors\"] = \", \".join(sorted(dirs)[:10])\n",
    "    series[\"writers\"] = \", \".join(sorted(wrs)[:10])\n",
    "\n",
    "    # providers & seasonsÎ•º Î≥ëÎ†¨Î°ú ÏàòÏßë\n",
    "    seasons_meta = data.get(\"seasons\") or []\n",
    "    \n",
    "    # providersÏôÄ Î™®Îì† ÏãúÏ¶åÏùÑ ÎèôÏãúÏóê ÏàòÏßë\n",
    "    tasks = [fetch_providers(session, sid)]\n",
    "    tasks.extend([\n",
    "        fetch_single_season(session, sid, data.get(\"name\") or \"\", net_names, net_ids, s)\n",
    "        for s in seasons_meta\n",
    "        if s.get(\"season_number\") is not None\n",
    "    ])\n",
    "    \n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Ï≤´ Î≤àÏß∏Îäî providers\n",
    "    series[\"providers\"] = results[0] if isinstance(results[0], str) else \"\"\n",
    "    \n",
    "    # ÎÇòÎ®∏ÏßÄÎäî ÏãúÏ¶å\n",
    "    season_records = [r for r in results[1:] if r and not isinstance(r, Exception)]\n",
    "\n",
    "    return series, season_records\n",
    "\n",
    "# ==========================================================\n",
    "# ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò\n",
    "# ==========================================================\n",
    "def fill_series_gaps(series_df, seasons_df):\n",
    "    print(\"\\nüìä ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ ÏãúÏûë\")\n",
    "\n",
    "    # episode_run_time\n",
    "    rtmap = seasons_df.groupby(\"series_id\")[\"avg_episode_runtime\"].mean().round()\n",
    "    mask = series_df[\"episode_run_time\"].astype(str).str.strip().isin([\"\", \"nan\"])\n",
    "    series_df.loc[mask, \"episode_run_time\"] = (\n",
    "        series_df.loc[mask, \"id\"].map(rtmap).fillna(0).astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    # number_of_episodes\n",
    "    epmap = seasons_df.groupby(\"series_id\")[\"total_episodes\"].sum()\n",
    "    mask2 = series_df[\"number_of_episodes\"].isna()\n",
    "    series_df.loc[mask2, \"number_of_episodes\"] = (\n",
    "        series_df.loc[mask2, \"id\"].map(epmap)\n",
    "    )\n",
    "\n",
    "    # number_of_seasons\n",
    "    seasmap = seasons_df.groupby(\"series_id\")[\"season_number\"].nunique()\n",
    "    mask3 = series_df[\"number_of_seasons\"].isna()\n",
    "    series_df.loc[mask3, \"number_of_seasons\"] = (\n",
    "        series_df.loc[mask3, \"id\"].map(seasmap)\n",
    "    )\n",
    "\n",
    "    # last_air_date\n",
    "    lastair = seasons_df.sort_values(\"air_date\").groupby(\"series_id\")[\"air_date\"].last()\n",
    "    ladmask = series_df[\"last_air_date\"].astype(str).str.strip().isin([\"\", \"nan\", \"NaT\"])\n",
    "    series_df.loc[ladmask, \"last_air_date\"] = series_df.loc[ladmask, \"id\"].map(lastair)\n",
    "\n",
    "    print(\"‚úÖ Î≥¥ÏôÑ ÏôÑÎ£å\\n\")\n",
    "    return series_df\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN (ÏµúÏ†ÅÌôî)\n",
    "# ==========================================================\n",
    "async def main():\n",
    "    print(\"=\" * 90)\n",
    "    print(\"üöÄ TMDB FULL SERIES + SEASONS COLLECTOR (OPTIMIZED)\")\n",
    "    print(\"=\" * 90)\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    # Í∏∞Ï°¥ temp ÏÇ≠Ï†ú\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    # TCPConnector ÏµúÏ†ÅÌôî ÏÑ§Ï†ï\n",
    "    connector = aiohttp.TCPConnector(\n",
    "        limit=100,  # ÎèôÏãú Ïó∞Í≤∞ Ïàò Ï¶ùÍ∞Ä\n",
    "        limit_per_host=50,  # Ìò∏Ïä§Ìä∏Îãπ Ïó∞Í≤∞ Ïàò Ï¶ùÍ∞Ä\n",
    "        ttl_dns_cache=300  # DNS Ï∫êÏãú\n",
    "    )\n",
    "    \n",
    "    timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_read=15)\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        # 1) ID ÏàòÏßë\n",
    "        print(\"\\nüìå 1Îã®Í≥Ñ: ID ÏàòÏßë\")\n",
    "        ids = sorted(list(await collect_ids_in_range(session, START_DATE, END_DATE)))\n",
    "        print(f\"‚ú® Ï¥ù ID: {len(ids):,}Í∞ú\")\n",
    "\n",
    "        # 2) ÏÉÅÏÑ∏ + ÏãúÏ¶å ÏàòÏßë (ÏµúÏ†ÅÌôî)\n",
    "        print(\"\\nüìå 2Îã®Í≥Ñ: ÏÉÅÏÑ∏ + ÏãúÏ¶å ÏàòÏßë\")\n",
    "        batch_size = 1000  # 500 ‚Üí 1000ÏúºÎ°ú Ï¶ùÍ∞Ä\n",
    "        sem = asyncio.Semaphore(40)  # 25 ‚Üí 40ÏúºÎ°ú Ï¶ùÍ∞Ä\n",
    "\n",
    "        async def fetch_one(i):\n",
    "            async with sem:\n",
    "                return await fetch_tv_details_and_seasons(session, i)\n",
    "\n",
    "        processed = 0\n",
    "        for start in range(0, len(ids), batch_size):\n",
    "            part = ids[start:start+batch_size]\n",
    "            tasks = [fetch_one(x) for x in part]\n",
    "\n",
    "            # gatherÎ°ú Ìïú Î≤àÏóê ÏàòÏßë (Îçî Îπ†Î¶Ñ)\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            bs = []\n",
    "            bse = []\n",
    "            \n",
    "            for r in results:\n",
    "                if isinstance(r, tuple) and r[0]:\n",
    "                    s, ss = r\n",
    "                    bs.append(s)\n",
    "                    if ss:\n",
    "                        bse.extend(ss)\n",
    "                \n",
    "                processed += 1\n",
    "                if processed % 100 == 0:\n",
    "                    print(f\"‚è± {processed:,}/{len(ids):,}\")\n",
    "\n",
    "            # flush\n",
    "            if bs:\n",
    "                pd.DataFrame(bs).to_csv(\n",
    "                    SERIES_TEMP, mode=\"a\", header=not SERIES_TEMP.exists(),\n",
    "                    index=False, encoding=\"utf-8-sig\"\n",
    "                )\n",
    "            if bse:\n",
    "                pd.DataFrame(bse).to_csv(\n",
    "                    SEASONS_TEMP, mode=\"a\", header=not SEASONS_TEMP.exists(),\n",
    "                    index=False, encoding=\"utf-8-sig\"\n",
    "                )\n",
    "\n",
    "    # ==========================================================\n",
    "    # 3) temp ‚Üí final DF\n",
    "    # ==========================================================\n",
    "    df_series = pd.read_csv(SERIES_TEMP)\n",
    "    df_seasons = pd.read_csv(SEASONS_TEMP) if SEASONS_TEMP.exists() else pd.DataFrame()\n",
    "\n",
    "    SERIES_COLS = [\n",
    "        \"id\",\"title\",\"type\",\"adult\",\"backdrop_path\",\"created_by\",\"episode_run_time\",\n",
    "        \"first_air_date\",\"genres\",\"genre_ids\",\"homepage\",\"in_production\",\"languages\",\n",
    "        \"last_air_date\",\"last_episode_to_air_id\",\"last_episode_to_air_name\",\n",
    "        \"last_episode_to_air_overview\",\"last_episode_to_air_vote_average\",\n",
    "        \"last_episode_to_air_vote_count\",\"last_episode_to_air_air_date\",\n",
    "        \"last_episode_to_air_episode_number\",\"last_episode_to_air_production_code\",\n",
    "        \"last_episode_to_air_runtime\",\"last_episode_to_air_season_number\",\n",
    "        \"last_episode_to_air_show_id\",\"last_episode_to_air_still_path\",\n",
    "        \"next_episode_to_air\",\"networks\",\"number_of_episodes\",\"number_of_seasons\",\n",
    "        \"origin_country\",\"original_language\",\"original_name\",\"overview\",\"popularity\",\n",
    "        \"poster_path\",\n",
    "        \"production_companies\",\"production_countries\",\"seasons\",\"spoken_languages\",\n",
    "        \"status\",\"tagline\",\"type_detail\",\"vote_average\",\"vote_count\",\"review\",\n",
    "        \"keyword\",\"top_cast\",\"directors\",\"writers\",\"providers\"\n",
    "    ]\n",
    "\n",
    "    SEASON_COLS = [\n",
    "        \"_id\",\"season_id\",\"series_id\",\"series_name\",\"season_number\",\"name\",\"air_date\",\n",
    "        \"overview\",\"vote_average\",\"vote_count\",\"network_names\",\"network_ids\",\n",
    "        \"total_episodes\",\"avg_episode_runtime\",\"poster_path\"\n",
    "    ]\n",
    "\n",
    "    df_series = df_series[SERIES_COLS].drop_duplicates(subset=[\"id\"])\n",
    "    if not df_seasons.empty:\n",
    "        df_seasons = df_seasons[SEASON_COLS].drop_duplicates(subset=[\"series_id\",\"season_number\"])\n",
    "\n",
    "    # 4) ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ\n",
    "    if not df_seasons.empty:\n",
    "        df_series = fill_series_gaps(df_series, df_seasons)\n",
    "\n",
    "    # 5) Ï†ÄÏû•\n",
    "    df_series.to_csv(SERIES_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    df_seasons.to_csv(SEASONS_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    try:\n",
    "        df_series.to_parquet(SERIES_PARQ, index=False)\n",
    "        df_seasons.to_parquet(SEASONS_PARQ, index=False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    elapsed = (datetime.now() - t0).total_seconds()/60\n",
    "\n",
    "    print(\"=\"*90)\n",
    "    print(\"üéâ ÏàòÏßë ÏôÑÎ£å\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"üìå Series Ïàò: {len(df_series):,}Í∞ú\")\n",
    "    print(f\"üìå Season Ïàò: {len(df_seasons):,}Í∞ú\")\n",
    "    print(f\"üìå Series Ïª¨Îüº Ïàò: {len(SERIES_COLS)}\")\n",
    "    print(f\"üìå Season  Ïª¨Îüº Ïàò: {len(SEASON_COLS)}\")\n",
    "    print(f\"‚è± Ï¥ù ÏÜåÏöîÏãúÍ∞Ñ: {elapsed:.1f}Î∂Ñ\")\n",
    "    print(\"=\"*90)\n",
    "\n",
    "# ==========================================================\n",
    "# RUN\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ nest_asyncio Ï†ÅÏö© ÏôÑÎ£å\n",
      "==========================================================================================\n",
      "üöÄ TMDB ULTRA-FAST SERIES + SEASONS COLLECTOR\n",
      "==========================================================================================\n",
      "\n",
      "üìå 1Îã®Í≥Ñ: ID ÏàòÏßë\n",
      "‚ú® Ï¥ù ID: 43,869Í∞ú\n",
      "\n",
      "üìå 2Îã®Í≥Ñ: ÏÉÅÏÑ∏ + ÏãúÏ¶å ÏàòÏßë\n",
      "‚è± 200/43,869 | 1.9 series/s | ETA: 385.7Î∂Ñ | ÏöîÏ≤≠: 12,147 | ÏóêÎü¨: 0\n",
      "‚è± 400/43,869 | 3.8 series/s | ETA: 192.0Î∂Ñ | ÏöîÏ≤≠: 12,147 | ÏóêÎü¨: 0\n",
      "‚è± 600/43,869 | 5.7 series/s | ETA: 127.4Î∂Ñ | ÏöîÏ≤≠: 12,147 | ÏóêÎü¨: 0\n",
      "‚è± 800/43,869 | 7.5 series/s | ETA: 95.1Î∂Ñ | ÏöîÏ≤≠: 12,147 | ÏóêÎü¨: 0\n",
      "‚è± 1,000/43,869 | 9.4 series/s | ETA: 75.7Î∂Ñ | ÏöîÏ≤≠: 12,147 | ÏóêÎü¨: 0\n",
      "‚è± 1,200/43,869 | 11.3 series/s | ETA: 62.8Î∂Ñ | ÏöîÏ≤≠: 12,147 | ÏóêÎü¨: 0\n",
      "‚è± 1,400/43,869 | 13.2 series/s | ETA: 53.6Î∂Ñ | ÏöîÏ≤≠: 12,147 | ÏóêÎü¨: 0\n",
      "‚è± 1,600/43,869 | 15.1 series/s | ETA: 46.7Î∂Ñ | ÏöîÏ≤≠: 12,147 | ÏóêÎü¨: 0\n",
      "‚è± 1,800/43,869 | 17.0 series/s | ETA: 41.3Î∂Ñ | ÏöîÏ≤≠: 12,147 | ÏóêÎü¨: 0\n",
      "‚è± 2,000/43,869 | 18.9 series/s | ETA: 37.0Î∂Ñ | ÏöîÏ≤≠: 12,147 | ÏóêÎü¨: 0\n",
      "‚è± 2,200/43,869 | 11.6 series/s | ETA: 60.0Î∂Ñ | ÏöîÏ≤≠: 21,560 | ÏóêÎü¨: 0\n",
      "‚è± 2,400/43,869 | 12.6 series/s | ETA: 54.7Î∂Ñ | ÏöîÏ≤≠: 21,560 | ÏóêÎü¨: 0\n",
      "‚è± 2,600/43,869 | 13.7 series/s | ETA: 50.3Î∂Ñ | ÏöîÏ≤≠: 21,560 | ÏóêÎü¨: 0\n",
      "‚è± 2,800/43,869 | 14.7 series/s | ETA: 46.5Î∂Ñ | ÏöîÏ≤≠: 21,560 | ÏóêÎü¨: 0\n",
      "‚è± 3,000/43,869 | 15.8 series/s | ETA: 43.1Î∂Ñ | ÏöîÏ≤≠: 21,560 | ÏóêÎü¨: 0\n",
      "‚è± 3,200/43,869 | 16.8 series/s | ETA: 40.3Î∂Ñ | ÏöîÏ≤≠: 21,560 | ÏóêÎü¨: 0\n",
      "‚è± 3,400/43,869 | 17.9 series/s | ETA: 37.7Î∂Ñ | ÏöîÏ≤≠: 21,560 | ÏóêÎü¨: 0\n",
      "‚è± 3,600/43,869 | 18.9 series/s | ETA: 35.4Î∂Ñ | ÏöîÏ≤≠: 21,560 | ÏóêÎü¨: 0\n",
      "‚è± 3,800/43,869 | 20.0 series/s | ETA: 33.4Î∂Ñ | ÏöîÏ≤≠: 21,560 | ÏóêÎü¨: 0\n",
      "‚è± 4,000/43,869 | 21.1 series/s | ETA: 31.6Î∂Ñ | ÏöîÏ≤≠: 21,560 | ÏóêÎü¨: 0\n",
      "‚è± 4,200/43,869 | 15.4 series/s | ETA: 43.0Î∂Ñ | ÏöîÏ≤≠: 29,638 | ÏóêÎü¨: 1\n",
      "‚è± 4,400/43,869 | 16.1 series/s | ETA: 40.8Î∂Ñ | ÏöîÏ≤≠: 29,638 | ÏóêÎü¨: 1\n",
      "‚è± 4,600/43,869 | 16.8 series/s | ETA: 38.8Î∂Ñ | ÏöîÏ≤≠: 29,638 | ÏóêÎü¨: 1\n",
      "‚è± 4,800/43,869 | 17.6 series/s | ETA: 37.0Î∂Ñ | ÏöîÏ≤≠: 29,638 | ÏóêÎü¨: 1\n",
      "‚è± 5,000/43,869 | 18.3 series/s | ETA: 35.4Î∂Ñ | ÏöîÏ≤≠: 29,638 | ÏóêÎü¨: 1\n",
      "‚è± 5,200/43,869 | 19.0 series/s | ETA: 33.8Î∂Ñ | ÏöîÏ≤≠: 29,638 | ÏóêÎü¨: 1\n",
      "‚è± 5,400/43,869 | 19.8 series/s | ETA: 32.4Î∂Ñ | ÏöîÏ≤≠: 29,638 | ÏóêÎü¨: 1\n",
      "‚è± 5,600/43,869 | 20.5 series/s | ETA: 31.1Î∂Ñ | ÏöîÏ≤≠: 29,638 | ÏóêÎü¨: 1\n",
      "‚è± 5,800/43,869 | 21.2 series/s | ETA: 29.9Î∂Ñ | ÏöîÏ≤≠: 29,638 | ÏóêÎü¨: 1\n",
      "‚è± 6,000/43,869 | 22.0 series/s | ETA: 28.7Î∂Ñ | ÏöîÏ≤≠: 29,638 | ÏóêÎü¨: 1\n",
      "‚è± 6,200/43,869 | 14.4 series/s | ETA: 43.6Î∂Ñ | ÏöîÏ≤≠: 38,944 | ÏóêÎü¨: 8\n",
      "‚è± 6,400/43,869 | 14.9 series/s | ETA: 42.0Î∂Ñ | ÏöîÏ≤≠: 38,944 | ÏóêÎü¨: 8\n",
      "‚è± 6,600/43,869 | 15.3 series/s | ETA: 40.5Î∂Ñ | ÏöîÏ≤≠: 38,944 | ÏóêÎü¨: 8\n",
      "‚è± 6,800/43,869 | 15.8 series/s | ETA: 39.1Î∂Ñ | ÏöîÏ≤≠: 38,944 | ÏóêÎü¨: 8\n",
      "‚è± 7,000/43,869 | 16.3 series/s | ETA: 37.8Î∂Ñ | ÏöîÏ≤≠: 38,944 | ÏóêÎü¨: 8\n",
      "‚è± 7,200/43,869 | 16.7 series/s | ETA: 36.5Î∂Ñ | ÏöîÏ≤≠: 38,944 | ÏóêÎü¨: 8\n",
      "‚è± 7,400/43,869 | 17.2 series/s | ETA: 35.3Î∂Ñ | ÏöîÏ≤≠: 38,944 | ÏóêÎü¨: 8\n",
      "‚è± 7,600/43,869 | 17.7 series/s | ETA: 34.2Î∂Ñ | ÏöîÏ≤≠: 38,944 | ÏóêÎü¨: 8\n",
      "‚è± 7,800/43,869 | 18.1 series/s | ETA: 33.2Î∂Ñ | ÏöîÏ≤≠: 38,944 | ÏóêÎü¨: 8\n",
      "‚è± 8,000/43,869 | 18.6 series/s | ETA: 32.1Î∂Ñ | ÏöîÏ≤≠: 38,944 | ÏóêÎü¨: 8\n",
      "‚è± 8,200/43,869 | 14.1 series/s | ETA: 42.2Î∂Ñ | ÏöîÏ≤≠: 47,873 | ÏóêÎü¨: 12\n",
      "‚è± 8,400/43,869 | 14.4 series/s | ETA: 40.9Î∂Ñ | ÏöîÏ≤≠: 47,873 | ÏóêÎü¨: 12\n",
      "‚è± 8,600/43,869 | 14.8 series/s | ETA: 39.8Î∂Ñ | ÏöîÏ≤≠: 47,873 | ÏóêÎü¨: 12\n",
      "‚è± 8,800/43,869 | 15.1 series/s | ETA: 38.6Î∂Ñ | ÏöîÏ≤≠: 47,873 | ÏóêÎü¨: 12\n",
      "‚è± 9,000/43,869 | 15.5 series/s | ETA: 37.6Î∂Ñ | ÏöîÏ≤≠: 47,873 | ÏóêÎü¨: 12\n",
      "‚è± 9,200/43,869 | 15.8 series/s | ETA: 36.5Î∂Ñ | ÏöîÏ≤≠: 47,873 | ÏóêÎü¨: 12\n",
      "‚è± 9,400/43,869 | 16.2 series/s | ETA: 35.5Î∂Ñ | ÏöîÏ≤≠: 47,873 | ÏóêÎü¨: 12\n",
      "‚è± 9,600/43,869 | 16.5 series/s | ETA: 34.6Î∂Ñ | ÏöîÏ≤≠: 47,873 | ÏóêÎü¨: 12\n",
      "‚è± 9,800/43,869 | 16.9 series/s | ETA: 33.7Î∂Ñ | ÏöîÏ≤≠: 47,873 | ÏóêÎü¨: 12\n",
      "‚è± 10,000/43,869 | 17.2 series/s | ETA: 32.8Î∂Ñ | ÏöîÏ≤≠: 47,873 | ÏóêÎü¨: 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 581\u001b[39m\n\u001b[32m    578\u001b[39m     executor.shutdown(wait=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py:323\u001b[39m, in \u001b[36mSelectSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    321\u001b[39m ready = []\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     r, w, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py:314\u001b[39m, in \u001b[36mSelectSelector._select\u001b[39m\u001b[34m(self, r, w, _, timeout)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     r, w, x = select.select(r, w, w, timeout)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w + x, []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# TMDB TV SERIES + SEASONS ULTRA-FAST ASYNC COLLECTOR\n",
    "# Series 51 cols / Seasons 15 cols\n",
    "# 2005-01-01 ~ 2015-12-31\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# ==========================================================\n",
    "# Jupyter async ÏßÄÏõê\n",
    "# ==========================================================\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"‚úÖ nest_asyncio Ï†ÅÏö© ÏôÑÎ£å\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ==========================================================\n",
    "# Í∏∞Î≥∏ ÏÑ§Ï†ï (Í∑πÌïú ÏµúÏ†ÅÌôî)\n",
    "# ==========================================================\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"TMDB_API_KEY missing\")\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "MAX_CALLS_PER_SECOND = 50  # TMDB Í≥µÏãù Ï†úÌïúÏóê Í∞ÄÍπùÍ≤å\n",
    "TIMEOUT = aiohttp.ClientTimeout(total=20, connect=5, sock_read=10)\n",
    "MAX_RETRIES = 2  # Îπ†Î•∏ Ïã§Ìå®\n",
    "\n",
    "START_DATE = \"2005-01-01\"\n",
    "END_DATE = \"2015-12-31\"\n",
    "\n",
    "# temp\n",
    "SERIES_TEMP = Path(\"tv_series_temp.csv\")\n",
    "SEASONS_TEMP = Path(\"tv_seasons_temp.csv\")\n",
    "\n",
    "# final\n",
    "SERIES_CSV = \"tv_series_2005_2015_FULL.csv\"\n",
    "SEASONS_CSV = \"tv_seasons_2005_2015_FULL.csv\"\n",
    "SERIES_PARQ = \"tv_series_2005_2015_FULL.parquet\"\n",
    "SEASONS_PARQ = \"tv_seasons_2005_2015_FULL.parquet\"\n",
    "\n",
    "# ÏÑ±Îä• Î©îÌä∏Î¶≠\n",
    "stats = {\n",
    "    \"requests\": 0,\n",
    "    \"errors\": 0,\n",
    "    \"start_time\": None\n",
    "}\n",
    "\n",
    "# ==========================================================\n",
    "# Token Bucket Rate Limiter (Îçî Îπ†Î¶Ñ)\n",
    "# ==========================================================\n",
    "class TokenBucketLimiter:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.tokens = rate\n",
    "        self.updated_at = time.monotonic()\n",
    "        self.lock = asyncio.Lock()\n",
    "    \n",
    "    async def acquire(self):\n",
    "        async with self.lock:\n",
    "            now = time.monotonic()\n",
    "            # ÌÜ†ÌÅ∞ Î≥¥Ï∂©\n",
    "            elapsed = now - self.updated_at\n",
    "            self.tokens = min(self.rate, self.tokens + elapsed * self.rate)\n",
    "            self.updated_at = now\n",
    "            \n",
    "            # ÌÜ†ÌÅ∞ Î∂ÄÏ°±Ïãú ÎåÄÍ∏∞\n",
    "            if self.tokens < 1:\n",
    "                sleep_time = (1 - self.tokens) / self.rate\n",
    "                await asyncio.sleep(sleep_time)\n",
    "                self.tokens = 0\n",
    "            else:\n",
    "                self.tokens -= 1\n",
    "\n",
    "rate_limiter = TokenBucketLimiter(MAX_CALLS_PER_SECOND)\n",
    "\n",
    "# ==========================================================\n",
    "# TMDB GET (Í∑πÌïú ÏµúÏ†ÅÌôî)\n",
    "# ==========================================================\n",
    "async def tmdb_get(session, path, params=None, retry=0):\n",
    "    if retry >= MAX_RETRIES:\n",
    "        stats[\"errors\"] += 1\n",
    "        return None\n",
    "\n",
    "    params = params or {}\n",
    "    params.setdefault(\"api_key\", API_KEY)\n",
    "    params.setdefault(\"language\", \"en-US\")\n",
    "\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "    \n",
    "    await rate_limiter.acquire()\n",
    "    stats[\"requests\"] += 1\n",
    "\n",
    "    try:\n",
    "        async with session.get(url, params=params, headers=HEADERS) as resp:\n",
    "            if resp.status == 429:\n",
    "                await asyncio.sleep(1)\n",
    "                return await tmdb_get(session, path, params, retry + 1)\n",
    "            \n",
    "            if resp.status == 404:\n",
    "                return None\n",
    "            \n",
    "            if 500 <= resp.status < 600:\n",
    "                await asyncio.sleep(0.2)\n",
    "                return await tmdb_get(session, path, params, retry + 1)\n",
    "            \n",
    "            resp.raise_for_status()\n",
    "            return await resp.json()\n",
    "    \n",
    "    except asyncio.TimeoutError:\n",
    "        stats[\"errors\"] += 1\n",
    "        return None\n",
    "    except Exception:\n",
    "        stats[\"errors\"] += 1\n",
    "        if retry < MAX_RETRIES - 1:\n",
    "            await asyncio.sleep(0.1)\n",
    "            return await tmdb_get(session, path, params, retry + 1)\n",
    "        return None\n",
    "\n",
    "# ==========================================================\n",
    "# discover page\n",
    "# ==========================================================\n",
    "async def fetch_discover_page(session, page, gte, lte):\n",
    "    params = {\n",
    "        \"sort_by\": \"popularity.desc\",\n",
    "        \"first_air_date.gte\": gte,\n",
    "        \"first_air_date.lte\": lte,\n",
    "        \"page\": page,\n",
    "        \"include_adult\": \"true\",\n",
    "    }\n",
    "    data = await tmdb_get(session, \"/discover/tv\", params)\n",
    "    if not data:\n",
    "        return [], 1, 0\n",
    "    return data.get(\"results\", []), data.get(\"total_pages\", 1), data.get(\"total_results\", 0)\n",
    "\n",
    "# ==========================================================\n",
    "# Î∂ÑÌï† ÏàòÏßë (ÏµúÎåÄ Î≥ëÎ†¨Ìôî)\n",
    "# ==========================================================\n",
    "async def collect_ids_in_range(session, start, end, depth=0):\n",
    "    res1, total_pages, total_results = await fetch_discover_page(session, 1, start, end)\n",
    "\n",
    "    if total_results == 0:\n",
    "        return set()\n",
    "\n",
    "    if total_pages <= 500:\n",
    "        ids = {r[\"id\"] for r in res1}\n",
    "        if total_pages > 1:\n",
    "            # ÌïúÎ≤àÏóê Ï†ÑÎ∂Ä ÏöîÏ≤≠\n",
    "            results = await asyncio.gather(\n",
    "                *[fetch_discover_page(session, p, start, end) for p in range(2, total_pages + 1)],\n",
    "                return_exceptions=True\n",
    "            )\n",
    "            for r in results:\n",
    "                if isinstance(r, tuple):\n",
    "                    ids.update(x[\"id\"] for x in r[0])\n",
    "        return ids\n",
    "\n",
    "    # ÎÇ†Ïßú Î∂ÑÌï†\n",
    "    start_dt = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    mid_dt = start_dt + (end_dt - start_dt) / 2\n",
    "    mid = mid_dt.strftime(\"%Y-%m-%d\")\n",
    "    right_start = (mid_dt + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    left, right = await asyncio.gather(\n",
    "        collect_ids_in_range(session, start, mid, depth + 1),\n",
    "        collect_ids_in_range(session, right_start, end, depth + 1)\n",
    "    )\n",
    "    return left | right\n",
    "\n",
    "# ==========================================================\n",
    "# helpers\n",
    "# ==========================================================\n",
    "def list_to_str(lst, key=\"name\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key, \"\")) for i in lst if i.get(key))\n",
    "\n",
    "def list_ids_to_str(lst, key=\"id\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key)) for i in lst if i.get(key))\n",
    "\n",
    "# ==========================================================\n",
    "# providers\n",
    "# ==========================================================\n",
    "async def fetch_providers(session, sid):\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/watch/providers\")\n",
    "    if not data:\n",
    "        return \"\"\n",
    "    out = set()\n",
    "    results = data.get(\"results\", {})\n",
    "    for cc in [\"US\", \"GB\", \"KR\"]:\n",
    "        if cc in results:\n",
    "            for x in results[cc].get(\"flatrate\", []):\n",
    "                if x.get(\"provider_name\"):\n",
    "                    out.add(x[\"provider_name\"])\n",
    "    return \", \".join(sorted(out))\n",
    "\n",
    "# ==========================================================\n",
    "# single season\n",
    "# ==========================================================\n",
    "async def fetch_single_season(session, sid, sname, net_names, net_ids, meta):\n",
    "    sn = meta.get(\"season_number\")\n",
    "    if sn is None:\n",
    "        return None\n",
    "\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/season/{sn}\")\n",
    "\n",
    "    if not data:\n",
    "        return {\n",
    "            \"_id\": f\"{sid}_{sn}\",\n",
    "            \"season_id\": meta.get(\"id\"),\n",
    "            \"series_id\": sid,\n",
    "            \"series_name\": sname,\n",
    "            \"season_number\": sn,\n",
    "            \"name\": meta.get(\"name\"),\n",
    "            \"air_date\": meta.get(\"air_date\"),\n",
    "            \"overview\": meta.get(\"overview\"),\n",
    "            \"vote_average\": meta.get(\"vote_average\"),\n",
    "            \"vote_count\": meta.get(\"vote_count\"),\n",
    "            \"network_names\": net_names,\n",
    "            \"network_ids\": net_ids,\n",
    "            \"total_episodes\": meta.get(\"episode_count\"),\n",
    "            \"avg_episode_runtime\": None,\n",
    "            \"poster_path\": meta.get(\"poster_path\"),\n",
    "        }\n",
    "\n",
    "    eps = data.get(\"episodes\") or []\n",
    "    runtimes = [ep.get(\"runtime\") for ep in eps if isinstance(ep.get(\"runtime\"), (int, float))]\n",
    "    avg_rt = sum(runtimes) / len(runtimes) if runtimes else None\n",
    "\n",
    "    return {\n",
    "        \"_id\": data.get(\"_id\") or f\"{sid}_{sn}\",\n",
    "        \"season_id\": data.get(\"id\"),\n",
    "        \"series_id\": sid,\n",
    "        \"series_name\": sname,\n",
    "        \"season_number\": data.get(\"season_number\"),\n",
    "        \"name\": data.get(\"name\"),\n",
    "        \"air_date\": data.get(\"air_date\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "        \"network_names\": net_names,\n",
    "        \"network_ids\": net_ids,\n",
    "        \"total_episodes\": len(eps),\n",
    "        \"avg_episode_runtime\": avg_rt,\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# TV ÏÉÅÏÑ∏ + ÏãúÏ¶å (ÏôÑÏ†Ñ Î≥ëÎ†¨Ìôî)\n",
    "# ==========================================================\n",
    "async def fetch_tv_details_and_seasons(session, sid):\n",
    "    params = {\"append_to_response\": \"reviews,keywords,aggregate_credits\"}\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}\", params)\n",
    "    if not data:\n",
    "        return None, []\n",
    "\n",
    "    genres = data.get(\"genres\") or []\n",
    "    networks = data.get(\"networks\") or []\n",
    "    last_ep = data.get(\"last_episode_to_air\") or {}\n",
    "\n",
    "    net_names = list_to_str(networks)\n",
    "    net_ids = list_ids_to_str(networks)\n",
    "\n",
    "    series = {\n",
    "        \"id\": data.get(\"id\"),\n",
    "        \"title\": data.get(\"name\"),\n",
    "        \"type\": \"tv_series\",\n",
    "        \"adult\": data.get(\"adult\"),\n",
    "        \"backdrop_path\": data.get(\"backdrop_path\"),\n",
    "        \"created_by\": list_to_str(data.get(\"created_by\")),\n",
    "        \"episode_run_time\": (\n",
    "            \", \".join(map(str, data.get(\"episode_run_time\", [])))\n",
    "            if data.get(\"episode_run_time\") else \"\"\n",
    "        ),\n",
    "        \"first_air_date\": data.get(\"first_air_date\"),\n",
    "        \"genres\": list_to_str(genres),\n",
    "        \"genre_ids\": \", \".join(str(g.get(\"id\")) for g in genres if g.get(\"id\")),\n",
    "        \"homepage\": data.get(\"homepage\"),\n",
    "        \"in_production\": data.get(\"in_production\"),\n",
    "        \"languages\": \", \".join(data.get(\"languages\", [])) if data.get(\"languages\") else \"\",\n",
    "        \"last_air_date\": data.get(\"last_air_date\"),\n",
    "\n",
    "        \"last_episode_to_air_id\": last_ep.get(\"id\"),\n",
    "        \"last_episode_to_air_name\": last_ep.get(\"name\"),\n",
    "        \"last_episode_to_air_overview\": last_ep.get(\"overview\"),\n",
    "        \"last_episode_to_air_vote_average\": last_ep.get(\"vote_average\"),\n",
    "        \"last_episode_to_air_vote_count\": last_ep.get(\"vote_count\"),\n",
    "        \"last_episode_to_air_air_date\": last_ep.get(\"air_date\"),\n",
    "        \"last_episode_to_air_episode_number\": last_ep.get(\"episode_number\"),\n",
    "        \"last_episode_to_air_production_code\": last_ep.get(\"production_code\"),\n",
    "        \"last_episode_to_air_runtime\": last_ep.get(\"runtime\"),\n",
    "        \"last_episode_to_air_season_number\": last_ep.get(\"season_number\"),\n",
    "        \"last_episode_to_air_show_id\": last_ep.get(\"show_id\"),\n",
    "        \"last_episode_to_air_still_path\": last_ep.get(\"still_path\"),\n",
    "\n",
    "        \"next_episode_to_air\": \"\" if not data.get(\"next_episode_to_air\") else str(data.get(\"next_episode_to_air\")),\n",
    "        \"networks\": net_names,\n",
    "        \"number_of_episodes\": data.get(\"number_of_episodes\"),\n",
    "        \"number_of_seasons\": data.get(\"number_of_seasons\"),\n",
    "\n",
    "        \"origin_country\": \", \".join(data.get(\"origin_country\", [])) if data.get(\"origin_country\") else \"\",\n",
    "        \"original_language\": data.get(\"original_language\"),\n",
    "        \"original_name\": data.get(\"original_name\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"popularity\": data.get(\"popularity\"),\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "\n",
    "        \"production_companies\": list_to_str(data.get(\"production_companies\")),\n",
    "        \"production_countries\": list_to_str(data.get(\"production_countries\")),\n",
    "\n",
    "        \"seasons\": \"; \".join(\n",
    "            f\"S{s.get('season_number')}: {s.get('name')} ({s.get('episode_count')} eps)\"\n",
    "            for s in (data.get(\"seasons\") or [])\n",
    "            if s.get(\"season_number\") is not None\n",
    "        ),\n",
    "\n",
    "        \"spoken_languages\": list_to_str(data.get(\"spoken_languages\")),\n",
    "        \"status\": data.get(\"status\"),\n",
    "        \"tagline\": data.get(\"tagline\"),\n",
    "        \"type_detail\": data.get(\"type\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "    }\n",
    "\n",
    "    # Î¶¨Î∑∞\n",
    "    rv_block = data.get(\"reviews\", {})\n",
    "    rv_items = rv_block.get(\"results\", []) if isinstance(rv_block, dict) else []\n",
    "    rev = []\n",
    "    for r in rv_items[:5]:\n",
    "        author = r.get(\"author\", \"\")\n",
    "        rating = r.get(\"author_details\", {}).get(\"rating\")\n",
    "        rt = f\"({rating})\" if rating is not None else \"\"\n",
    "        cont = (r.get(\"content\") or \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \")[:200]\n",
    "        rev.append(f\"{author}{rt}: {cont}\")\n",
    "    series[\"review\"] = \" || \".join(rev)\n",
    "\n",
    "    # ÌÇ§ÏõåÎìú\n",
    "    kw_block = data.get(\"keywords\", {})\n",
    "    kw_items = kw_block.get(\"results\", []) if isinstance(kw_block, dict) else []\n",
    "    series[\"keyword\"] = \", \".join([k.get(\"name\") for k in kw_items[:20] if k.get(\"name\")])\n",
    "\n",
    "    # top cast & crew\n",
    "    credits = data.get(\"aggregate_credits\", {})\n",
    "    cast = credits.get(\"cast\") or []\n",
    "    crew = credits.get(\"crew\") or []\n",
    "\n",
    "    series[\"top_cast\"] = \", \".join([c.get(\"name\") for c in cast[:4] if c.get(\"name\")])\n",
    "\n",
    "    dirs = set()\n",
    "    wrs = set()\n",
    "    for c in crew:\n",
    "        nm = c.get(\"name\")\n",
    "        if not nm:\n",
    "            continue\n",
    "        for job in (c.get(\"jobs\") or []):\n",
    "            jn = job.get(\"job\", \"\")\n",
    "            if \"Director\" in jn:\n",
    "                dirs.add(nm)\n",
    "            if jn in [\"Writer\", \"Screenplay\", \"Story\"]:\n",
    "                wrs.add(nm)\n",
    "\n",
    "    series[\"directors\"] = \", \".join(sorted(dirs)[:10])\n",
    "    series[\"writers\"] = \", \".join(sorted(wrs)[:10])\n",
    "\n",
    "    # providers & seasons Î≥ëÎ†¨ ÏàòÏßë\n",
    "    seasons_meta = data.get(\"seasons\") or []\n",
    "    \n",
    "    tasks = [fetch_providers(session, sid)]\n",
    "    tasks.extend([\n",
    "        fetch_single_season(session, sid, data.get(\"name\") or \"\", net_names, net_ids, s)\n",
    "        for s in seasons_meta\n",
    "        if s.get(\"season_number\") is not None\n",
    "    ])\n",
    "    \n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    series[\"providers\"] = results[0] if isinstance(results[0], str) else \"\"\n",
    "    season_records = [r for r in results[1:] if r and not isinstance(r, Exception)]\n",
    "\n",
    "    return series, season_records\n",
    "\n",
    "# ==========================================================\n",
    "# ÎπÑÎèôÍ∏∞ CSV Ïì∞Í∏∞ (ThreadPoolExecutor ÏÇ¨Ïö©)\n",
    "# ==========================================================\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "async def save_to_csv_async(df, path, mode, header):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    await loop.run_in_executor(\n",
    "        executor,\n",
    "        lambda: df.to_csv(path, mode=mode, header=header, index=False, encoding=\"utf-8-sig\")\n",
    "    )\n",
    "\n",
    "# ==========================================================\n",
    "# ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò\n",
    "# ==========================================================\n",
    "def fill_series_gaps(series_df, seasons_df):\n",
    "    print(\"\\nüìä ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ ÏãúÏûë\")\n",
    "\n",
    "    rtmap = seasons_df.groupby(\"series_id\")[\"avg_episode_runtime\"].mean().round()\n",
    "    mask = series_df[\"episode_run_time\"].astype(str).str.strip().isin([\"\", \"nan\"])\n",
    "    series_df.loc[mask, \"episode_run_time\"] = (\n",
    "        series_df.loc[mask, \"id\"].map(rtmap).fillna(0).astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    epmap = seasons_df.groupby(\"series_id\")[\"total_episodes\"].sum()\n",
    "    mask2 = series_df[\"number_of_episodes\"].isna()\n",
    "    series_df.loc[mask2, \"number_of_episodes\"] = series_df.loc[mask2, \"id\"].map(epmap)\n",
    "\n",
    "    seasmap = seasons_df.groupby(\"series_id\")[\"season_number\"].nunique()\n",
    "    mask3 = series_df[\"number_of_seasons\"].isna()\n",
    "    series_df.loc[mask3, \"number_of_seasons\"] = series_df.loc[mask3, \"id\"].map(seasmap)\n",
    "\n",
    "    lastair = seasons_df.sort_values(\"air_date\").groupby(\"series_id\")[\"air_date\"].last()\n",
    "    ladmask = series_df[\"last_air_date\"].astype(str).str.strip().isin([\"\", \"nan\", \"NaT\"])\n",
    "    series_df.loc[ladmask, \"last_air_date\"] = series_df.loc[ladmask, \"id\"].map(lastair)\n",
    "\n",
    "    print(\"‚úÖ Î≥¥ÏôÑ ÏôÑÎ£å\\n\")\n",
    "    return series_df\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN (Í∑πÌïú ÏµúÏ†ÅÌôî)\n",
    "# ==========================================================\n",
    "async def main():\n",
    "    print(\"=\" * 90)\n",
    "    print(\"üöÄ TMDB ULTRA-FAST SERIES + SEASONS COLLECTOR\")\n",
    "    print(\"=\" * 90)\n",
    "    stats[\"start_time\"] = datetime.now()\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    # Í∑πÌïú ÏµúÏ†ÅÌôî ÏÑ§Ï†ï\n",
    "    connector = aiohttp.TCPConnector(\n",
    "        limit=200,  # 100 ‚Üí 200\n",
    "        limit_per_host=100,  # 50 ‚Üí 100\n",
    "        ttl_dns_cache=600,\n",
    "        force_close=False,\n",
    "        enable_cleanup_closed=True\n",
    "    )\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as session:\n",
    "        # 1) ID ÏàòÏßë\n",
    "        print(\"\\nüìå 1Îã®Í≥Ñ: ID ÏàòÏßë\")\n",
    "        ids = sorted(list(await collect_ids_in_range(session, START_DATE, END_DATE)))\n",
    "        print(f\"‚ú® Ï¥ù ID: {len(ids):,}Í∞ú\")\n",
    "\n",
    "        # 2) ÏÉÅÏÑ∏ + ÏãúÏ¶å ÏàòÏßë\n",
    "        print(\"\\nüìå 2Îã®Í≥Ñ: ÏÉÅÏÑ∏ + ÏãúÏ¶å ÏàòÏßë\")\n",
    "        batch_size = 2000  # 1000 ‚Üí 2000\n",
    "        sem = asyncio.Semaphore(80)  # 40 ‚Üí 80\n",
    "\n",
    "        async def fetch_one(i):\n",
    "            async with sem:\n",
    "                return await fetch_tv_details_and_seasons(session, i)\n",
    "\n",
    "        processed = 0\n",
    "        for start_idx in range(0, len(ids), batch_size):\n",
    "            part = ids[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # Ï†ÑÎ∂Ä Î≥ëÎ†¨Î°ú ÏàòÏßë\n",
    "            results = await asyncio.gather(\n",
    "                *[fetch_one(x) for x in part],\n",
    "                return_exceptions=True\n",
    "            )\n",
    "            \n",
    "            bs = []\n",
    "            bse = []\n",
    "            \n",
    "            for r in results:\n",
    "                if isinstance(r, tuple) and r[0]:\n",
    "                    s, ss = r\n",
    "                    bs.append(s)\n",
    "                    if ss:\n",
    "                        bse.extend(ss)\n",
    "                \n",
    "                processed += 1\n",
    "                if processed % 200 == 0:\n",
    "                    elapsed = (datetime.now() - t0).total_seconds()\n",
    "                    rate = processed / elapsed if elapsed > 0 else 0\n",
    "                    eta = (len(ids) - processed) / rate / 60 if rate > 0 else 0\n",
    "                    print(f\"‚è± {processed:,}/{len(ids):,} | {rate:.1f} series/s | ETA: {eta:.1f}Î∂Ñ | ÏöîÏ≤≠: {stats['requests']:,} | ÏóêÎü¨: {stats['errors']}\")\n",
    "\n",
    "            # ÎπÑÎèôÍ∏∞ CSV Ïì∞Í∏∞\n",
    "            if bs:\n",
    "                await save_to_csv_async(\n",
    "                    pd.DataFrame(bs),\n",
    "                    SERIES_TEMP,\n",
    "                    \"a\",\n",
    "                    not SERIES_TEMP.exists()\n",
    "                )\n",
    "            if bse:\n",
    "                await save_to_csv_async(\n",
    "                    pd.DataFrame(bse),\n",
    "                    SEASONS_TEMP,\n",
    "                    \"a\",\n",
    "                    not SEASONS_TEMP.exists()\n",
    "                )\n",
    "\n",
    "    # 3) ÏµúÏ¢Ö Ï≤òÎ¶¨\n",
    "    df_series = pd.read_csv(SERIES_TEMP)\n",
    "    df_seasons = pd.read_csv(SEASONS_TEMP) if SEASONS_TEMP.exists() else pd.DataFrame()\n",
    "\n",
    "    SERIES_COLS = [\n",
    "        \"id\",\"title\",\"type\",\"adult\",\"backdrop_path\",\"created_by\",\"episode_run_time\",\n",
    "        \"first_air_date\",\"genres\",\"genre_ids\",\"homepage\",\"in_production\",\"languages\",\n",
    "        \"last_air_date\",\"last_episode_to_air_id\",\"last_episode_to_air_name\",\n",
    "        \"last_episode_to_air_overview\",\"last_episode_to_air_vote_average\",\n",
    "        \"last_episode_to_air_vote_count\",\"last_episode_to_air_air_date\",\n",
    "        \"last_episode_to_air_episode_number\",\"last_episode_to_air_production_code\",\n",
    "        \"last_episode_to_air_runtime\",\"last_episode_to_air_season_number\",\n",
    "        \"last_episode_to_air_show_id\",\"last_episode_to_air_still_path\",\n",
    "        \"next_episode_to_air\",\"networks\",\"number_of_episodes\",\"number_of_seasons\",\n",
    "        \"origin_country\",\"original_language\",\"original_name\",\"overview\",\"popularity\",\n",
    "        \"poster_path\",\n",
    "        \"production_companies\",\"production_countries\",\"seasons\",\"spoken_languages\",\n",
    "        \"status\",\"tagline\",\"type_detail\",\"vote_average\",\"vote_count\",\"review\",\n",
    "        \"keyword\",\"top_cast\",\"directors\",\"writers\",\"providers\"\n",
    "    ]\n",
    "\n",
    "    SEASON_COLS = [\n",
    "        \"_id\",\"season_id\",\"series_id\",\"series_name\",\"season_number\",\"name\",\"air_date\",\n",
    "        \"overview\",\"vote_average\",\"vote_count\",\"network_names\",\"network_ids\",\n",
    "        \"total_episodes\",\"avg_episode_runtime\",\"poster_path\"\n",
    "    ]\n",
    "\n",
    "    df_series = df_series[SERIES_COLS].drop_duplicates(subset=[\"id\"])\n",
    "    if not df_seasons.empty:\n",
    "        df_seasons = df_seasons[SEASON_COLS].drop_duplicates(subset=[\"series_id\",\"season_number\"])\n",
    "\n",
    "    if not df_seasons.empty:\n",
    "        df_series = fill_series_gaps(df_series, df_seasons)\n",
    "\n",
    "    df_series.to_csv(SERIES_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    df_seasons.to_csv(SEASONS_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    try:\n",
    "        df_series.to_parquet(SERIES_PARQ, index=False)\n",
    "        df_seasons.to_parquet(SEASONS_PARQ, index=False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    elapsed = (datetime.now() - t0).total_seconds()/60\n",
    "\n",
    "    print(\"=\"*90)\n",
    "    print(\"üéâ ÏàòÏßë ÏôÑÎ£å\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"üìå Series Ïàò: {len(df_series):,}Í∞ú\")\n",
    "    print(f\"üìå Season Ïàò: {len(df_seasons):,}Í∞ú\")\n",
    "    print(f\"üìå Ï¥ù API ÏöîÏ≤≠: {stats['requests']:,}Ìöå\")\n",
    "    print(f\"üìå ÏóêÎü¨ Ïàò: {stats['errors']:,}Ìöå\")\n",
    "    print(f\"üìå ÌèâÍ∑† ÏÜçÎèÑ: {len(df_series)/elapsed:.1f} series/Î∂Ñ\")\n",
    "    print(f\"‚è± Ï¥ù ÏÜåÏöîÏãúÍ∞Ñ: {elapsed:.1f}Î∂Ñ\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    executor.shutdown(wait=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04c690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fb988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ nest_asyncio Ï†ÅÏö© ÏôÑÎ£å\n",
      "==========================================================================================\n",
      "üöÄ TMDB HYPER-OPTIMIZED ASYNC COLLECTOR\n",
      "==========================================================================================\n",
      "\n",
      "üìå 1Îã®Í≥Ñ: ID ÏàòÏßë\n",
      "‚ú® Ï¥ù ID: 43,869Í∞ú\n",
      "\n",
      "üìå 2Îã®Í≥Ñ: ÏÉÅÏÑ∏ + ÏãúÏ¶å ÏàòÏßë\n",
      "‚è± 1,500/43,869 (3.4%) | 48.7/s | ETA: 15Î∂Ñ | Batch: 23.9s | ÏöîÏ≤≠: 9,916 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 3,000/43,869 (6.8%) | 59.4/s | ETA: 11Î∂Ñ | Batch: 19.7s | ÏöîÏ≤≠: 16,700 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 4,500/43,869 (10.3%) | 62.8/s | ETA: 10Î∂Ñ | Batch: 21.1s | ÏöîÏ≤≠: 23,488 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 6,000/43,869 (13.7%) | 65.2/s | ETA: 10Î∂Ñ | Batch: 20.4s | ÏöîÏ≤≠: 29,619 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 7,500/43,869 (17.1%) | 66.8/s | ETA: 9Î∂Ñ | Batch: 20.2s | ÏöîÏ≤≠: 36,695 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 9,000/43,869 (20.5%) | 68.3/s | ETA: 9Î∂Ñ | Batch: 19.6s | ÏöîÏ≤≠: 42,946 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 10,500/43,869 (23.9%) | 69.0/s | ETA: 8Î∂Ñ | Batch: 20.4s | ÏöîÏ≤≠: 49,881 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 12,000/43,869 (27.4%) | 69.4/s | ETA: 8Î∂Ñ | Batch: 20.8s | ÏöîÏ≤≠: 56,557 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 13,500/43,869 (30.8%) | 69.9/s | ETA: 7Î∂Ñ | Batch: 20.2s | ÏöîÏ≤≠: 63,094 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 15,000/43,869 (34.2%) | 70.2/s | ETA: 7Î∂Ñ | Batch: 20.3s | ÏöîÏ≤≠: 69,740 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 16,500/43,869 (37.6%) | 70.3/s | ETA: 6Î∂Ñ | Batch: 21.1s | ÏöîÏ≤≠: 77,000 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 18,000/43,869 (41.0%) | 70.7/s | ETA: 6Î∂Ñ | Batch: 19.9s | ÏöîÏ≤≠: 83,945 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 19,500/43,869 (44.5%) | 71.0/s | ETA: 6Î∂Ñ | Batch: 20.1s | ÏöîÏ≤≠: 91,334 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 21,000/43,869 (47.9%) | 71.5/s | ETA: 5Î∂Ñ | Batch: 19.0s | ÏöîÏ≤≠: 98,223 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 22,500/43,869 (51.3%) | 72.1/s | ETA: 5Î∂Ñ | Batch: 18.3s | ÏöîÏ≤≠: 104,821 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 24,000/43,869 (54.7%) | 72.8/s | ETA: 5Î∂Ñ | Batch: 17.7s | ÏöîÏ≤≠: 110,993 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 25,500/43,869 (58.1%) | 73.4/s | ETA: 4Î∂Ñ | Batch: 18.0s | ÏöîÏ≤≠: 117,280 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 27,000/43,869 (61.5%) | 74.0/s | ETA: 4Î∂Ñ | Batch: 17.2s | ÏöîÏ≤≠: 123,273 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 28,500/43,869 (65.0%) | 74.5/s | ETA: 3Î∂Ñ | Batch: 17.8s | ÏöîÏ≤≠: 129,882 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 30,000/43,869 (68.4%) | 74.6/s | ETA: 3Î∂Ñ | Batch: 19.6s | ÏöîÏ≤≠: 136,512 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 31,500/43,869 (71.8%) | 75.0/s | ETA: 3Î∂Ñ | Batch: 17.6s | ÏöîÏ≤≠: 142,136 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 33,000/43,869 (75.2%) | 75.6/s | ETA: 2Î∂Ñ | Batch: 16.8s | ÏöîÏ≤≠: 147,831 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 34,500/43,869 (78.6%) | 76.0/s | ETA: 2Î∂Ñ | Batch: 17.2s | ÏöîÏ≤≠: 153,235 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 36,000/43,869 (82.1%) | 76.4/s | ETA: 2Î∂Ñ | Batch: 17.2s | ÏöîÏ≤≠: 159,112 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 37,500/43,869 (85.5%) | 75.2/s | ETA: 1Î∂Ñ | Batch: 27.8s | ÏöîÏ≤≠: 164,483 | ÏóêÎü¨: 18 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 39,000/43,869 (88.9%) | 65.3/s | ETA: 1Î∂Ñ | Batch: 98.9s | ÏöîÏ≤≠: 170,131 | ÏóêÎü¨: 18 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 40,500/43,869 (92.3%) | 57.6/s | ETA: 1Î∂Ñ | Batch: 105.0s | ÏöîÏ≤≠: 175,631 | ÏóêÎü¨: 18 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 42,000/43,869 (95.7%) | 51.7/s | ETA: 1Î∂Ñ | Batch: 109.4s | ÏöîÏ≤≠: 181,733 | ÏóêÎü¨: 18 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 43,500/43,869 (99.2%) | 47.5/s | ETA: 0Î∂Ñ | Batch: 103.8s | ÏöîÏ≤≠: 187,319 | ÏóêÎü¨: 18 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 43,869/43,869 (100.0%) | 46.6/s | ETA: 0Î∂Ñ | Batch: 25.6s | ÏöîÏ≤≠: 188,697 | ÏóêÎü¨: 18 | Ïû¨ÏãúÎèÑ: 0\n",
      "\n",
      "üìå 3Îã®Í≥Ñ: ÏµúÏ¢Ö Ï≤òÎ¶¨\n",
      "\n",
      "üìä ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ\n",
      "‚úÖ ÏôÑÎ£å\n",
      "\n",
      "==========================================================================================\n",
      "üéâ ÏàòÏßë ÏôÑÎ£å!\n",
      "==========================================================================================\n",
      "üìå Series: 43,869Í∞ú | Seasons: 98,740Í∞ú\n",
      "üìå API ÏöîÏ≤≠: 188,697Ìöå | ÏóêÎü¨: 18 | Ïû¨ÏãúÎèÑ: 0\n",
      "üìå ÌèâÍ∑† ÏÜçÎèÑ: 2778.5 series/Î∂Ñ\n",
      "‚è± Ï¥ù ÏãúÍ∞Ñ: 15.8Î∂Ñ (0.3ÏãúÍ∞Ñ)\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# TMDB TV SERIES + SEASONS HYPER-OPTIMIZED ASYNC COLLECTOR\n",
    "# ÎπÑÎèôÍ∏∞ Î∞©Ïãù + Î≥ëÎ™© Ï†úÍ±∞ + Ï∫êÏã±\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from functools import lru_cache\n",
    "\n",
    "# ==========================================================\n",
    "# Jupyter async ÏßÄÏõê\n",
    "# ==========================================================\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"‚úÖ nest_asyncio Ï†ÅÏö© ÏôÑÎ£å\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ==========================================================\n",
    "# ÏÑ§Ï†ï\n",
    "# ==========================================================\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API_KEY missing\")\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "\n",
    "# üî• ÌïµÏã¨: TMDB Ïã§Ï†ú Ï†úÌïúÏùÄ 50/Ï¥àÏù¥ÏßÄÎßå, Î≤ÑÏä§Ìä∏Î•º Í≥†Î†§Ìï¥ 40ÏúºÎ°ú ÏïàÏ†ïÌôî\n",
    "MAX_CALLS_PER_SECOND = 25\n",
    "TIMEOUT = aiohttp.ClientTimeout(total=15, connect=5, sock_read=10)\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "START_DATE = \"2005-01-01\"\n",
    "END_DATE = \"2015-12-31\"\n",
    "\n",
    "SERIES_TEMP = Path(\"tv_series_temp.csv\")\n",
    "SEASONS_TEMP = Path(\"tv_seasons_temp.csv\")\n",
    "\n",
    "SERIES_CSV = \"tv_series_2005_2015_FULL.csv\"\n",
    "SEASONS_CSV = \"tv_seasons_2005_2015_FULL.csv\"\n",
    "SERIES_PARQ = \"tv_series_2005_2015_FULL.parquet\"\n",
    "SEASONS_PARQ = \"tv_seasons_2005_2015_FULL.parquet\"\n",
    "\n",
    "# ÌÜµÍ≥Ñ\n",
    "stats = {\"requests\": 0, \"errors\": 0, \"retries\": 0, \"start_time\": None}\n",
    "\n",
    "# ==========================================================\n",
    "# Í≥†ÏÑ±Îä• Rate Limiter (Lock-free Î≤ÑÏ†Ñ)\n",
    "# ==========================================================\n",
    "class LockFreeRateLimiter:\n",
    "    \"\"\"Lock ÏóÜÏù¥ ÎèôÏûëÌïòÎäî Ï¥àÍ≥†ÏÜç Rate Limiter\"\"\"\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.tokens = rate\n",
    "        self.updated_at = time.monotonic()\n",
    "    \n",
    "    async def acquire(self):\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self.updated_at\n",
    "        \n",
    "        # ÌÜ†ÌÅ∞ Î≥¥Ï∂© (Lock ÏóÜÏù¥)\n",
    "        self.tokens = min(self.rate, self.tokens + elapsed * self.rate)\n",
    "        self.updated_at = now\n",
    "        \n",
    "        # ÌÜ†ÌÅ∞ Î∂ÄÏ°±Ïãú ÏµúÏÜå ÎåÄÍ∏∞\n",
    "        if self.tokens < 1:\n",
    "            sleep_time = (1 - self.tokens) / self.rate\n",
    "            await asyncio.sleep(sleep_time)\n",
    "            self.tokens = 1\n",
    "        \n",
    "        self.tokens -= 1\n",
    "\n",
    "rate_limiter = LockFreeRateLimiter(MAX_CALLS_PER_SECOND)\n",
    "\n",
    "# ==========================================================\n",
    "# TMDB GET (Ïû¨ÏãúÎèÑ Î°úÏßÅ Í∞úÏÑ†)\n",
    "# ==========================================================\n",
    "async def tmdb_get(session, path, params=None, retry=0):\n",
    "    if retry >= MAX_RETRIES:\n",
    "        stats[\"errors\"] += 1\n",
    "        return None\n",
    "\n",
    "    params = params or {}\n",
    "    params.setdefault(\"api_key\", API_KEY)\n",
    "    params.setdefault(\"language\", \"en-US\")\n",
    "\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "    \n",
    "    await rate_limiter.acquire()\n",
    "    stats[\"requests\"] += 1\n",
    "\n",
    "    try:\n",
    "        async with session.get(url, params=params, headers=HEADERS) as resp:\n",
    "            # 429: Rate limit\n",
    "            if resp.status == 429:\n",
    "                stats[\"retries\"] += 1\n",
    "                retry_after = int(resp.headers.get(\"Retry-After\", 2))\n",
    "                await asyncio.sleep(retry_after)\n",
    "                return await tmdb_get(session, path, params, retry + 1)\n",
    "            \n",
    "            # 404: Not found (Ï†ïÏÉÅ ÏºÄÏù¥Ïä§)\n",
    "            if resp.status == 404:\n",
    "                return None\n",
    "            \n",
    "            # 5xx: ÏÑúÎ≤Ñ ÏóêÎü¨\n",
    "            if 500 <= resp.status < 600:\n",
    "                stats[\"retries\"] += 1\n",
    "                await asyncio.sleep(0.5 ** retry)  # ÏßÄÏàò Í∞êÏÜå\n",
    "                return await tmdb_get(session, path, params, retry + 1)\n",
    "            \n",
    "            resp.raise_for_status()\n",
    "            return await resp.json()\n",
    "    \n",
    "    except asyncio.TimeoutError:\n",
    "        stats[\"errors\"] += 1\n",
    "        if retry < MAX_RETRIES - 1:\n",
    "            await asyncio.sleep(0.2)\n",
    "            return await tmdb_get(session, path, params, retry + 1)\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        stats[\"errors\"] += 1\n",
    "        if retry < MAX_RETRIES - 1:\n",
    "            await asyncio.sleep(0.1)\n",
    "            return await tmdb_get(session, path, params, retry + 1)\n",
    "        return None\n",
    "\n",
    "# ==========================================================\n",
    "# Discover\n",
    "# ==========================================================\n",
    "async def fetch_discover_page(session, page, gte, lte):\n",
    "    params = {\n",
    "        \"sort_by\": \"popularity.desc\",\n",
    "        \"first_air_date.gte\": gte,\n",
    "        \"first_air_date.lte\": lte,\n",
    "        \"page\": page,\n",
    "        \"include_adult\": \"true\",\n",
    "    }\n",
    "    data = await tmdb_get(session, \"/discover/tv\", params)\n",
    "    if not data:\n",
    "        return [], 1, 0\n",
    "    return data.get(\"results\", []), data.get(\"total_pages\", 1), data.get(\"total_results\", 0)\n",
    "\n",
    "# ==========================================================\n",
    "# ID ÏàòÏßë (Í∞úÏÑ†)\n",
    "# ==========================================================\n",
    "async def collect_ids_in_range(session, start, end, depth=0):\n",
    "    res1, total_pages, total_results = await fetch_discover_page(session, 1, start, end)\n",
    "\n",
    "    if total_results == 0:\n",
    "        return set()\n",
    "\n",
    "    if total_pages <= 500:\n",
    "        ids = {r[\"id\"] for r in res1}\n",
    "        \n",
    "        if total_pages > 1:\n",
    "            # üî• Í∞úÏÑ†: Ï≤≠ÌÅ¨Î°ú ÎÇòÎà†ÏÑú Î©îÎ™®Î¶¨ Ìö®Ïú® Í∞úÏÑ†\n",
    "            chunk_size = 100\n",
    "            for chunk_start in range(2, total_pages + 1, chunk_size):\n",
    "                chunk_end = min(chunk_start + chunk_size, total_pages + 1)\n",
    "                results = await asyncio.gather(\n",
    "                    *[fetch_discover_page(session, p, start, end) for p in range(chunk_start, chunk_end)],\n",
    "                    return_exceptions=True\n",
    "                )\n",
    "                for r in results:\n",
    "                    if isinstance(r, tuple):\n",
    "                        ids.update(x[\"id\"] for x in r[0])\n",
    "        \n",
    "        return ids\n",
    "\n",
    "    # ÎÇ†Ïßú Î∂ÑÌï†\n",
    "    start_dt = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    mid_dt = start_dt + (end_dt - start_dt) / 2\n",
    "    mid = mid_dt.strftime(\"%Y-%m-%d\")\n",
    "    right_start = (mid_dt + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    left, right = await asyncio.gather(\n",
    "        collect_ids_in_range(session, start, mid, depth + 1),\n",
    "        collect_ids_in_range(session, right_start, end, depth + 1)\n",
    "    )\n",
    "    return left | right\n",
    "\n",
    "# ==========================================================\n",
    "# Helpers (Ï∫êÏã± Ï∂îÍ∞Ä)\n",
    "# ==========================================================\n",
    "@lru_cache(maxsize=1024)\n",
    "def list_to_str_cached(lst_tuple, key=\"name\"):\n",
    "    \"\"\"Î¶¨Ïä§Ìä∏Î•º Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò (Ï∫êÏã±)\"\"\"\n",
    "    if not lst_tuple:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key, \"\")) for i in lst_tuple if i.get(key))\n",
    "\n",
    "def list_to_str(lst, key=\"name\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    # ÎîïÏÖîÎÑàÎ¶¨ Î¶¨Ïä§Ìä∏Î•º ÌäúÌîåÎ°ú Î≥ÄÌôòÌïòÏó¨ Ï∫êÏã± Í∞ÄÎä•ÌïòÍ≤å\n",
    "    try:\n",
    "        lst_tuple = tuple(tuple(d.items()) if isinstance(d, dict) else d for d in lst)\n",
    "        return list_to_str_cached(lst_tuple, key)\n",
    "    except:\n",
    "        return \", \".join(str(i.get(key, \"\")) for i in lst if i.get(key))\n",
    "\n",
    "def list_ids_to_str(lst, key=\"id\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key)) for i in lst if i.get(key))\n",
    "\n",
    "# ==========================================================\n",
    "# Providers\n",
    "# ==========================================================\n",
    "async def fetch_providers(session, sid):\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/watch/providers\")\n",
    "    if not data:\n",
    "        return \"\"\n",
    "    out = set()\n",
    "    results = data.get(\"results\", {})\n",
    "    for cc in [\"US\", \"GB\", \"KR\"]:\n",
    "        if cc in results:\n",
    "            for x in results[cc].get(\"flatrate\", []):\n",
    "                pn = x.get(\"provider_name\")\n",
    "                if pn:\n",
    "                    out.add(pn)\n",
    "    return \", \".join(sorted(out))\n",
    "\n",
    "# ==========================================================\n",
    "# Single Season\n",
    "# ==========================================================\n",
    "async def fetch_single_season(session, sid, sname, net_names, net_ids, meta):\n",
    "    sn = meta.get(\"season_number\")\n",
    "    if sn is None:\n",
    "        return None\n",
    "\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/season/{sn}\")\n",
    "\n",
    "    if not data:\n",
    "        return {\n",
    "            \"_id\": f\"{sid}_{sn}\",\n",
    "            \"season_id\": meta.get(\"id\"),\n",
    "            \"series_id\": sid,\n",
    "            \"series_name\": sname,\n",
    "            \"season_number\": sn,\n",
    "            \"name\": meta.get(\"name\"),\n",
    "            \"air_date\": meta.get(\"air_date\"),\n",
    "            \"overview\": meta.get(\"overview\"),\n",
    "            \"vote_average\": meta.get(\"vote_average\"),\n",
    "            \"vote_count\": meta.get(\"vote_count\"),\n",
    "            \"network_names\": net_names,\n",
    "            \"network_ids\": net_ids,\n",
    "            \"total_episodes\": meta.get(\"episode_count\"),\n",
    "            \"avg_episode_runtime\": None,\n",
    "            \"poster_path\": meta.get(\"poster_path\"),\n",
    "        }\n",
    "\n",
    "    eps = data.get(\"episodes\") or []\n",
    "    runtimes = [ep[\"runtime\"] for ep in eps if ep.get(\"runtime\")]\n",
    "    avg_rt = sum(runtimes) / len(runtimes) if runtimes else None\n",
    "\n",
    "    return {\n",
    "        \"_id\": data.get(\"_id\") or f\"{sid}_{sn}\",\n",
    "        \"season_id\": data.get(\"id\"),\n",
    "        \"series_id\": sid,\n",
    "        \"series_name\": sname,\n",
    "        \"season_number\": data.get(\"season_number\"),\n",
    "        \"name\": data.get(\"name\"),\n",
    "        \"air_date\": data.get(\"air_date\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "        \"network_names\": net_names,\n",
    "        \"network_ids\": net_ids,\n",
    "        \"total_episodes\": len(eps),\n",
    "        \"avg_episode_runtime\": avg_rt,\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# TV Details + Seasons\n",
    "# ==========================================================\n",
    "async def fetch_tv_details_and_seasons(session, sid):\n",
    "    params = {\"append_to_response\": \"reviews,keywords,aggregate_credits\"}\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}\", params)\n",
    "    if not data:\n",
    "        return None, []\n",
    "\n",
    "    genres = data.get(\"genres\") or []\n",
    "    networks = data.get(\"networks\") or []\n",
    "    last_ep = data.get(\"last_episode_to_air\") or {}\n",
    "\n",
    "    net_names = list_to_str(networks)\n",
    "    net_ids = list_ids_to_str(networks)\n",
    "\n",
    "    series = {\n",
    "        \"id\": data.get(\"id\"),\n",
    "        \"title\": data.get(\"name\"),\n",
    "        \"type\": \"tv_series\",\n",
    "        \"adult\": data.get(\"adult\"),\n",
    "        \"backdrop_path\": data.get(\"backdrop_path\"),\n",
    "        \"created_by\": list_to_str(data.get(\"created_by\")),\n",
    "        \"episode_run_time\": \", \".join(map(str, data.get(\"episode_run_time\", []))) or \"\",\n",
    "        \"first_air_date\": data.get(\"first_air_date\"),\n",
    "        \"genres\": list_to_str(genres),\n",
    "        \"genre_ids\": \", \".join(str(g[\"id\"]) for g in genres if g.get(\"id\")),\n",
    "        \"homepage\": data.get(\"homepage\"),\n",
    "        \"in_production\": data.get(\"in_production\"),\n",
    "        \"languages\": \", \".join(data.get(\"languages\", [])) or \"\",\n",
    "        \"last_air_date\": data.get(\"last_air_date\"),\n",
    "        \"last_episode_to_air_id\": last_ep.get(\"id\"),\n",
    "        \"last_episode_to_air_name\": last_ep.get(\"name\"),\n",
    "        \"last_episode_to_air_overview\": last_ep.get(\"overview\"),\n",
    "        \"last_episode_to_air_vote_average\": last_ep.get(\"vote_average\"),\n",
    "        \"last_episode_to_air_vote_count\": last_ep.get(\"vote_count\"),\n",
    "        \"last_episode_to_air_air_date\": last_ep.get(\"air_date\"),\n",
    "        \"last_episode_to_air_episode_number\": last_ep.get(\"episode_number\"),\n",
    "        \"last_episode_to_air_production_code\": last_ep.get(\"production_code\"),\n",
    "        \"last_episode_to_air_runtime\": last_ep.get(\"runtime\"),\n",
    "        \"last_episode_to_air_season_number\": last_ep.get(\"season_number\"),\n",
    "        \"last_episode_to_air_show_id\": last_ep.get(\"show_id\"),\n",
    "        \"last_episode_to_air_still_path\": last_ep.get(\"still_path\"),\n",
    "        \"next_episode_to_air\": str(data.get(\"next_episode_to_air\")) if data.get(\"next_episode_to_air\") else \"\",\n",
    "        \"networks\": net_names,\n",
    "        \"number_of_episodes\": data.get(\"number_of_episodes\"),\n",
    "        \"number_of_seasons\": data.get(\"number_of_seasons\"),\n",
    "        \"origin_country\": \", \".join(data.get(\"origin_country\", [])) or \"\",\n",
    "        \"original_language\": data.get(\"original_language\"),\n",
    "        \"original_name\": data.get(\"original_name\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"popularity\": data.get(\"popularity\"),\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "        \"production_companies\": list_to_str(data.get(\"production_companies\")),\n",
    "        \"production_countries\": list_to_str(data.get(\"production_countries\")),\n",
    "        \"seasons\": \"; \".join(\n",
    "            f\"S{s['season_number']}: {s.get('name')} ({s.get('episode_count')} eps)\"\n",
    "            for s in (data.get(\"seasons\") or []) if s.get(\"season_number\") is not None\n",
    "        ),\n",
    "        \"spoken_languages\": list_to_str(data.get(\"spoken_languages\")),\n",
    "        \"status\": data.get(\"status\"),\n",
    "        \"tagline\": data.get(\"tagline\"),\n",
    "        \"type_detail\": data.get(\"type\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "    }\n",
    "\n",
    "    # Î¶¨Î∑∞\n",
    "    rv_items = data.get(\"reviews\", {}).get(\"results\", [])\n",
    "    series[\"review\"] = \" || \".join([\n",
    "        f\"{r.get('author', '')}({r.get('author_details', {}).get('rating', '')}): {(r.get('content') or '').replace(chr(10), ' ')[:200]}\"\n",
    "        for r in rv_items[:5]\n",
    "    ])\n",
    "\n",
    "    # ÌÇ§ÏõåÎìú\n",
    "    kw_items = data.get(\"keywords\", {}).get(\"results\", [])\n",
    "    series[\"keyword\"] = \", \".join([k[\"name\"] for k in kw_items[:20] if k.get(\"name\")])\n",
    "\n",
    "    # Cast & Crew\n",
    "    credits = data.get(\"aggregate_credits\", {})\n",
    "    cast = credits.get(\"cast\") or []\n",
    "    crew = credits.get(\"crew\") or []\n",
    "\n",
    "    series[\"top_cast\"] = \", \".join([c[\"name\"] for c in cast[:4] if c.get(\"name\")])\n",
    "\n",
    "    dirs, wrs = set(), set()\n",
    "    for c in crew:\n",
    "        nm = c.get(\"name\")\n",
    "        if nm:\n",
    "            for job in (c.get(\"jobs\") or []):\n",
    "                jn = job.get(\"job\", \"\")\n",
    "                if \"Director\" in jn:\n",
    "                    dirs.add(nm)\n",
    "                if jn in [\"Writer\", \"Screenplay\", \"Story\"]:\n",
    "                    wrs.add(nm)\n",
    "\n",
    "    series[\"directors\"] = \", \".join(sorted(dirs)[:10])\n",
    "    series[\"writers\"] = \", \".join(sorted(wrs)[:10])\n",
    "\n",
    "    # üî• providers + seasons Î≥ëÎ†¨ ÏàòÏßë\n",
    "    seasons_meta = [s for s in (data.get(\"seasons\") or []) if s.get(\"season_number\") is not None]\n",
    "    \n",
    "    tasks = [fetch_providers(session, sid)]\n",
    "    tasks.extend([\n",
    "        fetch_single_season(session, sid, data.get(\"name\", \"\"), net_names, net_ids, s)\n",
    "        for s in seasons_meta\n",
    "    ])\n",
    "    \n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    series[\"providers\"] = results[0] if isinstance(results[0], str) else \"\"\n",
    "    season_records = [r for r in results[1:] if r and not isinstance(r, Exception)]\n",
    "\n",
    "    return series, season_records\n",
    "\n",
    "# ==========================================================\n",
    "# ÎπÑÎèôÍ∏∞ CSV Ïì∞Í∏∞\n",
    "# ==========================================================\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "async def save_to_csv_async(df, path, mode, header):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    await loop.run_in_executor(\n",
    "        executor,\n",
    "        lambda: df.to_csv(path, mode=mode, header=header, index=False, encoding=\"utf-8-sig\")\n",
    "    )\n",
    "\n",
    "# ==========================================================\n",
    "# Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ\n",
    "# ==========================================================\n",
    "def fill_series_gaps(series_df, seasons_df):\n",
    "    print(\"\\nüìä ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ\")\n",
    "\n",
    "    rtmap = seasons_df.groupby(\"series_id\")[\"avg_episode_runtime\"].mean().round()\n",
    "    mask = series_df[\"episode_run_time\"].astype(str).str.strip().isin([\"\", \"nan\"])\n",
    "    series_df.loc[mask, \"episode_run_time\"] = (\n",
    "        series_df.loc[mask, \"id\"].map(rtmap).fillna(0).astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    epmap = seasons_df.groupby(\"series_id\")[\"total_episodes\"].sum()\n",
    "    mask2 = series_df[\"number_of_episodes\"].isna()\n",
    "    series_df.loc[mask2, \"number_of_episodes\"] = series_df.loc[mask2, \"id\"].map(epmap)\n",
    "\n",
    "    seasmap = seasons_df.groupby(\"series_id\")[\"season_number\"].nunique()\n",
    "    mask3 = series_df[\"number_of_seasons\"].isna()\n",
    "    series_df.loc[mask3, \"number_of_seasons\"] = series_df.loc[mask3, \"id\"].map(seasmap)\n",
    "\n",
    "    lastair = seasons_df.sort_values(\"air_date\").groupby(\"series_id\")[\"air_date\"].last()\n",
    "    ladmask = series_df[\"last_air_date\"].astype(str).str.strip().isin([\"\", \"nan\", \"NaT\"])\n",
    "    series_df.loc[ladmask, \"last_air_date\"] = series_df.loc[ladmask, \"id\"].map(lastair)\n",
    "\n",
    "    print(\"‚úÖ ÏôÑÎ£å\\n\")\n",
    "    return series_df\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN\n",
    "# ==========================================================\n",
    "async def main():\n",
    "    print(\"=\" * 90)\n",
    "    print(\"üöÄ TMDB HYPER-OPTIMIZED ASYNC COLLECTOR\")\n",
    "    print(\"=\" * 90)\n",
    "    stats[\"start_time\"] = datetime.now()\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    # üî• ÏµúÏ†Å ÏÑ§Ï†ï\n",
    "    connector = aiohttp.TCPConnector(\n",
    "        limit=150,\n",
    "        limit_per_host=75,\n",
    "        ttl_dns_cache=600,\n",
    "        force_close=False,\n",
    "        enable_cleanup_closed=True\n",
    "    )\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as session:\n",
    "        # 1) ID ÏàòÏßë\n",
    "        print(\"\\nüìå 1Îã®Í≥Ñ: ID ÏàòÏßë\")\n",
    "        ids = sorted(list(await collect_ids_in_range(session, START_DATE, END_DATE)))\n",
    "        print(f\"‚ú® Ï¥ù ID: {len(ids):,}Í∞ú\")\n",
    "\n",
    "        # 2) ÏÉÅÏÑ∏ ÏàòÏßë\n",
    "        print(\"\\nüìå 2Îã®Í≥Ñ: ÏÉÅÏÑ∏ + ÏãúÏ¶å ÏàòÏßë\")\n",
    "        batch_size = 1500\n",
    "        sem = asyncio.Semaphore(30)  # üî• ÏïàÏ†ïÏ†ÅÏù∏ ÎèôÏãúÏÑ±\n",
    "\n",
    "        async def fetch_one(i):\n",
    "            async with sem:\n",
    "                return await fetch_tv_details_and_seasons(session, i)\n",
    "\n",
    "        processed = 0\n",
    "        last_print = time.time()\n",
    "        \n",
    "        for start_idx in range(0, len(ids), batch_size):\n",
    "            batch_start = time.time()\n",
    "            part = ids[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            results = await asyncio.gather(*[fetch_one(x) for x in part], return_exceptions=True)\n",
    "            \n",
    "            bs, bse = [], []\n",
    "            for r in results:\n",
    "                if isinstance(r, tuple) and r[0]:\n",
    "                    s, ss = r\n",
    "                    bs.append(s)\n",
    "                    bse.extend(ss)\n",
    "                processed += 1\n",
    "\n",
    "            # CSV Ïì∞Í∏∞\n",
    "            if bs:\n",
    "                await save_to_csv_async(pd.DataFrame(bs), SERIES_TEMP, \"a\", not SERIES_TEMP.exists())\n",
    "            if bse:\n",
    "                await save_to_csv_async(pd.DataFrame(bse), SEASONS_TEMP, \"a\", not SEASONS_TEMP.exists())\n",
    "\n",
    "            # ÏßÑÌñâ ÏÉÅÌô© (5Ï¥àÎßàÎã§)\n",
    "            now = time.time()\n",
    "            if now - last_print >= 5:\n",
    "                elapsed = (datetime.now() - t0).total_seconds()\n",
    "                rate = processed / elapsed if elapsed > 0 else 0\n",
    "                eta = (len(ids) - processed) / rate / 60 if rate > 0 else 0\n",
    "                batch_time = now - batch_start\n",
    "                print(f\"‚è± {processed:,}/{len(ids):,} ({processed/len(ids)*100:.1f}%) | \"\n",
    "                      f\"{rate:.1f}/s | ETA: {eta:.0f}Î∂Ñ | Batch: {batch_time:.1f}s | \"\n",
    "                      f\"ÏöîÏ≤≠: {stats['requests']:,} | ÏóêÎü¨: {stats['errors']} | Ïû¨ÏãúÎèÑ: {stats['retries']}\")\n",
    "                last_print = now\n",
    "\n",
    "    # 3) ÏµúÏ¢Ö Ï≤òÎ¶¨\n",
    "    print(\"\\nüìå 3Îã®Í≥Ñ: ÏµúÏ¢Ö Ï≤òÎ¶¨\")\n",
    "    df_series = pd.read_csv(SERIES_TEMP)\n",
    "    df_seasons = pd.read_csv(SEASONS_TEMP) if SEASONS_TEMP.exists() else pd.DataFrame()\n",
    "\n",
    "    SERIES_COLS = [\n",
    "        \"id\",\"title\",\"type\",\"adult\",\"backdrop_path\",\"created_by\",\"episode_run_time\",\n",
    "        \"first_air_date\",\"genres\",\"genre_ids\",\"homepage\",\"in_production\",\"languages\",\n",
    "        \"last_air_date\",\"last_episode_to_air_id\",\"last_episode_to_air_name\",\n",
    "        \"last_episode_to_air_overview\",\"last_episode_to_air_vote_average\",\n",
    "        \"last_episode_to_air_vote_count\",\"last_episode_to_air_air_date\",\n",
    "        \"last_episode_to_air_episode_number\",\"last_episode_to_air_production_code\",\n",
    "        \"last_episode_to_air_runtime\",\"last_episode_to_air_season_number\",\n",
    "        \"last_episode_to_air_show_id\",\"last_episode_to_air_still_path\",\n",
    "        \"next_episode_to_air\",\"networks\",\"number_of_episodes\",\"number_of_seasons\",\n",
    "        \"origin_country\",\"original_language\",\"original_name\",\"overview\",\"popularity\",\n",
    "        \"poster_path\",\"production_companies\",\"production_countries\",\"seasons\",\n",
    "        \"spoken_languages\",\"status\",\"tagline\",\"type_detail\",\"vote_average\",\n",
    "        \"vote_count\",\"review\",\"keyword\",\"top_cast\",\"directors\",\"writers\",\"providers\"\n",
    "    ]\n",
    "\n",
    "    SEASON_COLS = [\n",
    "        \"_id\",\"season_id\",\"series_id\",\"series_name\",\"season_number\",\"name\",\"air_date\",\n",
    "        \"overview\",\"vote_average\",\"vote_count\",\"network_names\",\"network_ids\",\n",
    "        \"total_episodes\",\"avg_episode_runtime\",\"poster_path\"\n",
    "    ]\n",
    "\n",
    "    df_series = df_series[SERIES_COLS].drop_duplicates(subset=[\"id\"])\n",
    "    if not df_seasons.empty:\n",
    "        df_seasons = df_seasons[SEASON_COLS].drop_duplicates(subset=[\"series_id\",\"season_number\"])\n",
    "        df_series = fill_series_gaps(df_series, df_seasons)\n",
    "\n",
    "    df_series.to_csv(SERIES_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    df_seasons.to_csv(SEASONS_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    try:\n",
    "        df_series.to_parquet(SERIES_PARQ, index=False)\n",
    "        df_seasons.to_parquet(SEASONS_PARQ, index=False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    elapsed = (datetime.now() - t0).total_seconds()/60\n",
    "\n",
    "    print(\"=\"*90)\n",
    "    print(\"üéâ ÏàòÏßë ÏôÑÎ£å!\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"üìå Series: {len(df_series):,}Í∞ú | Seasons: {len(df_seasons):,}Í∞ú\")\n",
    "    print(f\"üìå API ÏöîÏ≤≠: {stats['requests']:,}Ìöå | ÏóêÎü¨: {stats['errors']} | Ïû¨ÏãúÎèÑ: {stats['retries']}\")\n",
    "    print(f\"üìå ÌèâÍ∑† ÏÜçÎèÑ: {len(df_series)/elapsed:.1f} series/Î∂Ñ\")\n",
    "    print(f\"‚è± Ï¥ù ÏãúÍ∞Ñ: {elapsed:.1f}Î∂Ñ ({elapsed/60:.1f}ÏãúÍ∞Ñ)\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    executor.shutdown(wait=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9700188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ nest_asyncio Ï†ÅÏö© ÏôÑÎ£å\n",
      "==========================================================================================\n",
      "üöÄ TMDB HYPER-OPTIMIZED ASYNC COLLECTOR\n",
      "‚ú® ÏµúÏ¢Ö Í∞úÏÑ†: providers(flatrate/rent/buy Î∂ÑÎ¶¨) + IMDB ID Ï∂îÍ∞Ä\n",
      "==========================================================================================\n",
      "\n",
      "üìå 1Îã®Í≥Ñ: ID ÏàòÏßë\n",
      "‚ú® Ï¥ù ID: 37,971Í∞ú\n",
      "\n",
      "üìå 2Îã®Í≥Ñ: ÏÉÅÏÑ∏ + ÏãúÏ¶å + IMDB ID + Providers ÏàòÏßë\n",
      "‚è± 1,500/37,971 (4.0%) | 27.2/s | ETA: 22Î∂Ñ | Batch: 47.1s | ÏöîÏ≤≠: 11,344 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 0\n",
      "‚è± 3,000/37,971 (7.9%) | 14.1/s | ETA: 41Î∂Ñ | Batch: 158.1s | ÏöîÏ≤≠: 19,974 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 36\n",
      "‚è± 4,500/37,971 (11.9%) | 12.9/s | ETA: 43Î∂Ñ | Batch: 136.8s | ÏöîÏ≤≠: 27,772 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 38\n",
      "‚è± 6,000/37,971 (15.8%) | 11.8/s | ETA: 45Î∂Ñ | Batch: 159.1s | ÏöîÏ≤≠: 36,247 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 42\n",
      "‚è± 7,500/37,971 (19.8%) | 11.5/s | ETA: 44Î∂Ñ | Batch: 145.4s | ÏöîÏ≤≠: 44,155 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 48\n",
      "‚è± 9,000/37,971 (23.7%) | 11.1/s | ETA: 44Î∂Ñ | Batch: 157.0s | ÏöîÏ≤≠: 52,513 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 67\n",
      "‚è± 10,500/37,971 (27.7%) | 10.9/s | ETA: 42Î∂Ñ | Batch: 153.3s | ÏöîÏ≤≠: 60,759 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 81\n",
      "‚è± 12,000/37,971 (31.6%) | 10.8/s | ETA: 40Î∂Ñ | Batch: 144.5s | ÏöîÏ≤≠: 68,976 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 92\n",
      "‚è± 13,500/37,971 (35.6%) | 10.7/s | ETA: 38Î∂Ñ | Batch: 155.9s | ÏöîÏ≤≠: 77,336 | ÏóêÎü¨: 5 | Ïû¨ÏãúÎèÑ: 121\n",
      "‚è± 15,000/37,971 (39.5%) | 10.5/s | ETA: 37Î∂Ñ | Batch: 169.7s | ÏöîÏ≤≠: 86,099 | ÏóêÎü¨: 7 | Ïû¨ÏãúÎèÑ: 158\n",
      "‚è± 16,500/37,971 (43.5%) | 10.3/s | ETA: 35Î∂Ñ | Batch: 167.1s | ÏöîÏ≤≠: 94,885 | ÏóêÎü¨: 7 | Ïû¨ÏãúÎèÑ: 169\n",
      "‚è± 18,000/37,971 (47.4%) | 10.2/s | ETA: 33Î∂Ñ | Batch: 161.2s | ÏöîÏ≤≠: 103,332 | ÏóêÎü¨: 7 | Ïû¨ÏãúÎèÑ: 193\n",
      "‚è± 19,500/37,971 (51.4%) | 10.2/s | ETA: 30Î∂Ñ | Batch: 155.5s | ÏöîÏ≤≠: 111,354 | ÏóêÎü¨: 7 | Ïû¨ÏãúÎèÑ: 202\n",
      "‚è± 21,000/37,971 (55.3%) | 10.2/s | ETA: 28Î∂Ñ | Batch: 144.9s | ÏöîÏ≤≠: 119,065 | ÏóêÎü¨: 7 | Ïû¨ÏãúÎèÑ: 202\n",
      "‚è± 22,500/37,971 (59.3%) | 10.2/s | ETA: 25Î∂Ñ | Batch: 142.2s | ÏöîÏ≤≠: 126,510 | ÏóêÎü¨: 7 | Ïû¨ÏãúÎèÑ: 205\n",
      "‚è± 24,000/37,971 (63.2%) | 10.2/s | ETA: 23Î∂Ñ | Batch: 144.0s | ÏöîÏ≤≠: 134,276 | ÏóêÎü¨: 7 | Ïû¨ÏãúÎèÑ: 212\n",
      "‚è± 25,500/37,971 (67.2%) | 10.2/s | ETA: 20Î∂Ñ | Batch: 147.0s | ÏöîÏ≤≠: 143,159 | ÏóêÎü¨: 823 | Ïû¨ÏãúÎèÑ: 213\n",
      "‚è± 27,000/37,971 (71.1%) | 10.3/s | ETA: 18Î∂Ñ | Batch: 135.0s | ÏöîÏ≤≠: 150,280 | ÏóêÎü¨: 823 | Ïû¨ÏãúÎèÑ: 226\n",
      "‚è± 28,500/37,971 (75.1%) | 10.3/s | ETA: 15Î∂Ñ | Batch: 134.2s | ÏöîÏ≤≠: 157,501 | ÏóêÎü¨: 823 | Ïû¨ÏãúÎèÑ: 234\n",
      "‚è± 30,000/37,971 (79.0%) | 10.4/s | ETA: 13Î∂Ñ | Batch: 126.4s | ÏöîÏ≤≠: 164,496 | ÏóêÎü¨: 823 | Ïû¨ÏãúÎèÑ: 234\n",
      "‚è± 31,500/37,971 (83.0%) | 10.4/s | ETA: 10Î∂Ñ | Batch: 131.9s | ÏöîÏ≤≠: 171,711 | ÏóêÎü¨: 823 | Ïû¨ÏãúÎèÑ: 234\n",
      "‚è± 33,000/37,971 (86.9%) | 10.5/s | ETA: 8Î∂Ñ | Batch: 123.0s | ÏöîÏ≤≠: 178,584 | ÏóêÎü¨: 823 | Ïû¨ÏãúÎèÑ: 236\n",
      "‚è± 34,500/37,971 (90.9%) | 10.5/s | ETA: 5Î∂Ñ | Batch: 124.1s | ÏöîÏ≤≠: 185,727 | ÏóêÎü¨: 823 | Ïû¨ÏãúÎèÑ: 236\n",
      "‚è± 36,000/37,971 (94.8%) | 10.6/s | ETA: 3Î∂Ñ | Batch: 131.3s | ÏöîÏ≤≠: 193,077 | ÏóêÎü¨: 823 | Ïû¨ÏãúÎèÑ: 238\n",
      "‚è± 37,500/37,971 (98.8%) | 10.6/s | ETA: 1Î∂Ñ | Batch: 131.2s | ÏöîÏ≤≠: 200,318 | ÏóêÎü¨: 823 | Ïû¨ÏãúÎèÑ: 238\n",
      "‚è± 37,971/37,971 (100.0%) | 10.6/s | ETA: 0Î∂Ñ | Batch: 41.3s | ÏöîÏ≤≠: 202,534 | ÏóêÎü¨: 823 | Ïû¨ÏãúÎèÑ: 238\n",
      "\n",
      "üìå 3Îã®Í≥Ñ: ÏµúÏ¢Ö Ï≤òÎ¶¨\n",
      "\n",
      "üìä ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ\n",
      "‚úÖ ÏôÑÎ£å\n",
      "\n",
      "==========================================================================================\n",
      "üéâ ÏàòÏßë ÏôÑÎ£å!\n",
      "==========================================================================================\n",
      "üìå Series: 37,970Í∞ú | Seasons: 85,634Í∞ú\n",
      "üìå API ÏöîÏ≤≠: 202,534Ìöå | ÏóêÎü¨: 823 | Ïû¨ÏãúÎèÑ: 238\n",
      "üìå ÌèâÍ∑† ÏÜçÎèÑ: 636.2 series/Î∂Ñ\n",
      "‚è± Ï¥ù ÏãúÍ∞Ñ: 59.7Î∂Ñ (1.0ÏãúÍ∞Ñ)\n",
      "==========================================================================================\n",
      "\n",
      "üìä ÏÉòÌîå Providers Îç∞Ïù¥ÌÑ∞:\n",
      "Title: The Amazing Mrs Pritchard\n",
      "Flatrate: {\"AU\": [\"BritBox\", \"Britbox Apple TV Channel \", \"BritBox Amazon Channel\"], \"CA\": [\"Amazon Prime Video\", \"Amazon Prime Video with Ads\"], \"GB\": [\"Amazon Prime Video\", \"Amazon Prime Video with Ads\"], \"GG...\n",
      "IMDB ID: tt0807980\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# TMDB TV SERIES + SEASONS HYPER-OPTIMIZED ASYNC COLLECTOR\n",
    "# ÎπÑÎèôÍ∏∞ Î∞©Ïãù + Î≥ëÎ™© Ï†úÍ±∞ + Ï∫êÏã±\n",
    "# ‚ú® ÏµúÏ¢Ö Í∞úÏÑ†: providersÎ•º flatrate/rent/buyÎ°ú Î∂ÑÎ¶¨ + imdb_id Ï∂îÍ∞Ä\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "# ==========================================================\n",
    "# Jupyter async ÏßÄÏõê\n",
    "# ==========================================================\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"‚úÖ nest_asyncio Ï†ÅÏö© ÏôÑÎ£å\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ==========================================================\n",
    "# ÏÑ§Ï†ï\n",
    "# ==========================================================\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API_KEY missing\")\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "\n",
    "# üî• ÌïµÏã¨: TMDB Ïã§Ï†ú Ï†úÌïúÏùÄ 50/Ï¥àÏù¥ÏßÄÎßå, Î≤ÑÏä§Ìä∏Î•º Í≥†Î†§Ìï¥ 40ÏúºÎ°ú ÏïàÏ†ïÌôî\n",
    "MAX_CALLS_PER_SECOND = 25\n",
    "TIMEOUT = aiohttp.ClientTimeout(total=15, connect=5, sock_read=10)\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "START_DATE = \"2005-01-01\"\n",
    "END_DATE = \"2015-12-31\"\n",
    "\n",
    "SERIES_TEMP = Path(\"tv_series_temp.csv\")\n",
    "SEASONS_TEMP = Path(\"tv_seasons_temp.csv\")\n",
    "\n",
    "SERIES_CSV = \"tv_series_2005_2015_FULL.csv\"\n",
    "SEASONS_CSV = \"tv_seasons_2005_2015_FULL.csv\"\n",
    "SERIES_PARQ = \"tv_series_2005_2015_FULL.parquet\"\n",
    "SEASONS_PARQ = \"tv_seasons_2005_2015_FULL.parquet\"\n",
    "\n",
    "# ÌÜµÍ≥Ñ\n",
    "stats = {\"requests\": 0, \"errors\": 0, \"retries\": 0, \"start_time\": None}\n",
    "\n",
    "# ==========================================================\n",
    "# Í≥†ÏÑ±Îä• Rate Limiter (Lock-free Î≤ÑÏ†Ñ)\n",
    "# ==========================================================\n",
    "class LockFreeRateLimiter:\n",
    "    \"\"\"Lock ÏóÜÏù¥ ÎèôÏûëÌïòÎäî Ï¥àÍ≥†ÏÜç Rate Limiter\"\"\"\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.tokens = rate\n",
    "        self.updated_at = time.monotonic()\n",
    "    \n",
    "    async def acquire(self):\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self.updated_at\n",
    "        \n",
    "        # ÌÜ†ÌÅ∞ Î≥¥Ï∂© (Lock ÏóÜÏù¥)\n",
    "        self.tokens = min(self.rate, self.tokens + elapsed * self.rate)\n",
    "        self.updated_at = now\n",
    "        \n",
    "        # ÌÜ†ÌÅ∞ Î∂ÄÏ°±Ïãú ÏµúÏÜå ÎåÄÍ∏∞\n",
    "        if self.tokens < 1:\n",
    "            sleep_time = (1 - self.tokens) / self.rate\n",
    "            await asyncio.sleep(sleep_time)\n",
    "            self.tokens = 1\n",
    "        \n",
    "        self.tokens -= 1\n",
    "\n",
    "rate_limiter = LockFreeRateLimiter(MAX_CALLS_PER_SECOND)\n",
    "\n",
    "# ==========================================================\n",
    "# TMDB GET (Ïû¨ÏãúÎèÑ Î°úÏßÅ Í∞úÏÑ†)\n",
    "# ==========================================================\n",
    "async def tmdb_get(session, path, params=None, retry=0):\n",
    "    if retry >= MAX_RETRIES:\n",
    "        stats[\"errors\"] += 1\n",
    "        return None\n",
    "\n",
    "    params = params or {}\n",
    "    params.setdefault(\"api_key\", API_KEY)\n",
    "    params.setdefault(\"language\", \"en-US\")\n",
    "\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "    \n",
    "    await rate_limiter.acquire()\n",
    "    stats[\"requests\"] += 1\n",
    "\n",
    "    try:\n",
    "        async with session.get(url, params=params, headers=HEADERS) as resp:\n",
    "            # 429: Rate limit\n",
    "            if resp.status == 429:\n",
    "                stats[\"retries\"] += 1\n",
    "                retry_after = int(resp.headers.get(\"Retry-After\", 2))\n",
    "                await asyncio.sleep(retry_after)\n",
    "                return await tmdb_get(session, path, params, retry + 1)\n",
    "            \n",
    "            # 404: Not found (Ï†ïÏÉÅ ÏºÄÏù¥Ïä§)\n",
    "            if resp.status == 404:\n",
    "                return None\n",
    "            \n",
    "            # 5xx: ÏÑúÎ≤Ñ ÏóêÎü¨\n",
    "            if 500 <= resp.status < 600:\n",
    "                stats[\"retries\"] += 1\n",
    "                await asyncio.sleep(0.5 ** retry)  # ÏßÄÏàò Í∞êÏÜå\n",
    "                return await tmdb_get(session, path, params, retry + 1)\n",
    "            \n",
    "            resp.raise_for_status()\n",
    "            return await resp.json()\n",
    "    \n",
    "    except asyncio.TimeoutError:\n",
    "        stats[\"errors\"] += 1\n",
    "        if retry < MAX_RETRIES - 1:\n",
    "            await asyncio.sleep(0.2)\n",
    "            return await tmdb_get(session, path, params, retry + 1)\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        stats[\"errors\"] += 1\n",
    "        if retry < MAX_RETRIES - 1:\n",
    "            await asyncio.sleep(0.1)\n",
    "            return await tmdb_get(session, path, params, retry + 1)\n",
    "        return None\n",
    "\n",
    "# ==========================================================\n",
    "# Discover\n",
    "# ==========================================================\n",
    "async def fetch_discover_page(session, page, gte, lte):\n",
    "    params = {\n",
    "        \"sort_by\": \"popularity.desc\",\n",
    "        \"first_air_date.gte\": gte,\n",
    "        \"first_air_date.lte\": lte,\n",
    "        \"page\": page,\n",
    "        \"include_adult\": \"true\",\n",
    "    }\n",
    "    data = await tmdb_get(session, \"/discover/tv\", params)\n",
    "    if not data:\n",
    "        return [], 1, 0\n",
    "    return data.get(\"results\", []), data.get(\"total_pages\", 1), data.get(\"total_results\", 0)\n",
    "\n",
    "# ==========================================================\n",
    "# ID ÏàòÏßë (Í∞úÏÑ†)\n",
    "# ==========================================================\n",
    "async def collect_ids_in_range(session, start, end, depth=0):\n",
    "    res1, total_pages, total_results = await fetch_discover_page(session, 1, start, end)\n",
    "\n",
    "    if total_results == 0:\n",
    "        return set()\n",
    "\n",
    "    if total_pages <= 500:\n",
    "        ids = {r[\"id\"] for r in res1}\n",
    "        \n",
    "        if total_pages > 1:\n",
    "            # üî• Í∞úÏÑ†: Ï≤≠ÌÅ¨Î°ú ÎÇòÎà†ÏÑú Î©îÎ™®Î¶¨ Ìö®Ïú® Í∞úÏÑ†\n",
    "            chunk_size = 100\n",
    "            for chunk_start in range(2, total_pages + 1, chunk_size):\n",
    "                chunk_end = min(chunk_start + chunk_size, total_pages + 1)\n",
    "                results = await asyncio.gather(\n",
    "                    *[fetch_discover_page(session, p, start, end) for p in range(chunk_start, chunk_end)],\n",
    "                    return_exceptions=True\n",
    "                )\n",
    "                for r in results:\n",
    "                    if isinstance(r, tuple):\n",
    "                        ids.update(x[\"id\"] for x in r[0])\n",
    "        \n",
    "        return ids\n",
    "\n",
    "    # ÎÇ†Ïßú Î∂ÑÌï†\n",
    "    start_dt = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    mid_dt = start_dt + (end_dt - start_dt) / 2\n",
    "    mid = mid_dt.strftime(\"%Y-%m-%d\")\n",
    "    right_start = (mid_dt + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    left, right = await asyncio.gather(\n",
    "        collect_ids_in_range(session, start, mid, depth + 1),\n",
    "        collect_ids_in_range(session, right_start, end, depth + 1)\n",
    "    )\n",
    "    return left | right\n",
    "\n",
    "# ==========================================================\n",
    "# Helpers (Ï∫êÏã± Ï∂îÍ∞Ä)\n",
    "# ==========================================================\n",
    "@lru_cache(maxsize=1024)\n",
    "def list_to_str_cached(lst_tuple, key=\"name\"):\n",
    "    \"\"\"Î¶¨Ïä§Ìä∏Î•º Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò (Ï∫êÏã±)\"\"\"\n",
    "    if not lst_tuple:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key, \"\")) for i in lst_tuple if i.get(key))\n",
    "\n",
    "def list_to_str(lst, key=\"name\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    # ÎîïÏÖîÎÑàÎ¶¨ Î¶¨Ïä§Ìä∏Î•º ÌäúÌîåÎ°ú Î≥ÄÌôòÌïòÏó¨ Ï∫êÏã± Í∞ÄÎä•ÌïòÍ≤å\n",
    "    try:\n",
    "        lst_tuple = tuple(tuple(d.items()) if isinstance(d, dict) else d for d in lst)\n",
    "        return list_to_str_cached(lst_tuple, key)\n",
    "    except:\n",
    "        return \", \".join(str(i.get(key, \"\")) for i in lst if i.get(key))\n",
    "\n",
    "def list_ids_to_str(lst, key=\"id\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key)) for i in lst if i.get(key))\n",
    "\n",
    "# ==========================================================\n",
    "# ‚ú® NEW: External IDs (IMDB ID)\n",
    "# ==========================================================\n",
    "async def fetch_external_ids(session, sid):\n",
    "    \"\"\"ÏãúÎ¶¨Ï¶àÏùò external IDsÎ•º Í∞ÄÏ†∏ÏôÄÏÑú IMDB ID Î∞òÌôò\"\"\"\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/external_ids\")\n",
    "    if not data:\n",
    "        return \"\"\n",
    "    return data.get(\"imdb_id\", \"\")\n",
    "\n",
    "# ==========================================================\n",
    "# ‚ú® UPDATED: Providers (flatrate/rent/buy Î∂ÑÎ¶¨)\n",
    "# ==========================================================\n",
    "async def fetch_providers(session, sid):\n",
    "    \"\"\"\n",
    "    Î™®Îì† Íµ≠Í∞ÄÏùò streaming providers ÏàòÏßë\n",
    "    flatrate(Íµ¨ÎèÖÌòï), rent(ÎåÄÏó¨), buy(Íµ¨Îß§) ÌÉÄÏûÖÎ≥ÑÎ°ú Î∂ÑÎ¶¨\n",
    "    Î∞òÌôò: (flatrate_dict, rent_dict, buy_dict)\n",
    "    \"\"\"\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/watch/providers\")\n",
    "    if not data:\n",
    "        return \"{}\", \"{}\", \"{}\"\n",
    "    \n",
    "    providers_results = data.get(\"results\", {})\n",
    "    \n",
    "    # Íµ≠Í∞ÄÎ≥Ñ OTT Ï†ïÎ≥¥ ÏàòÏßë\n",
    "    providers_flatrate = {}\n",
    "    providers_rent = {}\n",
    "    providers_buy = {}\n",
    "    \n",
    "    for country, info in providers_results.items():\n",
    "        # flatrate: Íµ¨ÎèÖÌòï (Netflix, Disney+ Îì±)\n",
    "        flatrate = info.get(\"flatrate\", [])\n",
    "        if flatrate:\n",
    "            provider_names = [p.get(\"provider_name\") for p in flatrate if p.get(\"provider_name\")]\n",
    "            if provider_names:\n",
    "                providers_flatrate[country] = provider_names\n",
    "        \n",
    "        # rent: ÎåÄÏó¨\n",
    "        rent = info.get(\"rent\", [])\n",
    "        if rent:\n",
    "            provider_names = [p.get(\"provider_name\") for p in rent if p.get(\"provider_name\")]\n",
    "            if provider_names:\n",
    "                providers_rent[country] = provider_names\n",
    "        \n",
    "        # buy: Íµ¨Îß§\n",
    "        buy = info.get(\"buy\", [])\n",
    "        if buy:\n",
    "            provider_names = [p.get(\"provider_name\") for p in buy if p.get(\"provider_name\")]\n",
    "            if provider_names:\n",
    "                providers_buy[country] = provider_names\n",
    "    \n",
    "    # JSON Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôòÌïòÏó¨ Î∞òÌôò\n",
    "    return (\n",
    "        json.dumps(providers_flatrate, ensure_ascii=False) if providers_flatrate else \"{}\",\n",
    "        json.dumps(providers_rent, ensure_ascii=False) if providers_rent else \"{}\",\n",
    "        json.dumps(providers_buy, ensure_ascii=False) if providers_buy else \"{}\"\n",
    "    )\n",
    "\n",
    "# ==========================================================\n",
    "# Single Season\n",
    "# ==========================================================\n",
    "async def fetch_single_season(session, sid, sname, net_names, net_ids, meta):\n",
    "    sn = meta.get(\"season_number\")\n",
    "    if sn is None:\n",
    "        return None\n",
    "\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/season/{sn}\")\n",
    "\n",
    "    if not data:\n",
    "        return {\n",
    "            \"_id\": f\"{sid}_{sn}\",\n",
    "            \"season_id\": meta.get(\"id\"),\n",
    "            \"series_id\": sid,\n",
    "            \"series_name\": sname,\n",
    "            \"season_number\": sn,\n",
    "            \"name\": meta.get(\"name\"),\n",
    "            \"air_date\": meta.get(\"air_date\"),\n",
    "            \"overview\": meta.get(\"overview\"),\n",
    "            \"vote_average\": meta.get(\"vote_average\"),\n",
    "            \"vote_count\": meta.get(\"vote_count\"),\n",
    "            \"network_names\": net_names,\n",
    "            \"network_ids\": net_ids,\n",
    "            \"total_episodes\": meta.get(\"episode_count\"),\n",
    "            \"avg_episode_runtime\": None,\n",
    "            \"poster_path\": meta.get(\"poster_path\"),\n",
    "        }\n",
    "\n",
    "    eps = data.get(\"episodes\") or []\n",
    "    runtimes = [ep[\"runtime\"] for ep in eps if ep.get(\"runtime\")]\n",
    "    avg_rt = sum(runtimes) / len(runtimes) if runtimes else None\n",
    "\n",
    "    return {\n",
    "        \"_id\": data.get(\"_id\") or f\"{sid}_{sn}\",\n",
    "        \"season_id\": data.get(\"id\"),\n",
    "        \"series_id\": sid,\n",
    "        \"series_name\": sname,\n",
    "        \"season_number\": data.get(\"season_number\"),\n",
    "        \"name\": data.get(\"name\"),\n",
    "        \"air_date\": data.get(\"air_date\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "        \"network_names\": net_names,\n",
    "        \"network_ids\": net_ids,\n",
    "        \"total_episodes\": len(eps),\n",
    "        \"avg_episode_runtime\": avg_rt,\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# ‚ú® UPDATED: TV Details + Seasons + IMDB ID + Providers\n",
    "# ==========================================================\n",
    "async def fetch_tv_details_and_seasons(session, sid):\n",
    "    params = {\"append_to_response\": \"reviews,keywords,aggregate_credits\"}\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}\", params)\n",
    "    if not data:\n",
    "        return None, []\n",
    "\n",
    "    genres = data.get(\"genres\") or []\n",
    "    networks = data.get(\"networks\") or []\n",
    "    last_ep = data.get(\"last_episode_to_air\") or {}\n",
    "\n",
    "    net_names = list_to_str(networks)\n",
    "    net_ids = list_ids_to_str(networks)\n",
    "\n",
    "    series = {\n",
    "        \"id\": data.get(\"id\"),\n",
    "        \"title\": data.get(\"name\"),\n",
    "        \"type\": \"tv_series\",\n",
    "        \"adult\": data.get(\"adult\"),\n",
    "        \"backdrop_path\": data.get(\"backdrop_path\"),\n",
    "        \"created_by\": list_to_str(data.get(\"created_by\")),\n",
    "        \"episode_run_time\": \", \".join(map(str, data.get(\"episode_run_time\", []))) or \"\",\n",
    "        \"first_air_date\": data.get(\"first_air_date\"),\n",
    "        \"genres\": list_to_str(genres),\n",
    "        \"genre_ids\": \", \".join(str(g[\"id\"]) for g in genres if g.get(\"id\")),\n",
    "        \"homepage\": data.get(\"homepage\"),\n",
    "        \"in_production\": data.get(\"in_production\"),\n",
    "        \"languages\": \", \".join(data.get(\"languages\", [])) or \"\",\n",
    "        \"last_air_date\": data.get(\"last_air_date\"),\n",
    "        \"last_episode_to_air_id\": last_ep.get(\"id\"),\n",
    "        \"last_episode_to_air_name\": last_ep.get(\"name\"),\n",
    "        \"last_episode_to_air_overview\": last_ep.get(\"overview\"),\n",
    "        \"last_episode_to_air_vote_average\": last_ep.get(\"vote_average\"),\n",
    "        \"last_episode_to_air_vote_count\": last_ep.get(\"vote_count\"),\n",
    "        \"last_episode_to_air_air_date\": last_ep.get(\"air_date\"),\n",
    "        \"last_episode_to_air_episode_number\": last_ep.get(\"episode_number\"),\n",
    "        \"last_episode_to_air_production_code\": last_ep.get(\"production_code\"),\n",
    "        \"last_episode_to_air_runtime\": last_ep.get(\"runtime\"),\n",
    "        \"last_episode_to_air_season_number\": last_ep.get(\"season_number\"),\n",
    "        \"last_episode_to_air_show_id\": last_ep.get(\"show_id\"),\n",
    "        \"last_episode_to_air_still_path\": last_ep.get(\"still_path\"),\n",
    "        \"next_episode_to_air\": str(data.get(\"next_episode_to_air\")) if data.get(\"next_episode_to_air\") else \"\",\n",
    "        \"networks\": net_names,\n",
    "        \"number_of_episodes\": data.get(\"number_of_episodes\"),\n",
    "        \"number_of_seasons\": data.get(\"number_of_seasons\"),\n",
    "        \"origin_country\": \", \".join(data.get(\"origin_country\", [])) or \"\",\n",
    "        \"original_language\": data.get(\"original_language\"),\n",
    "        \"original_name\": data.get(\"original_name\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"popularity\": data.get(\"popularity\"),\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "        \"production_companies\": list_to_str(data.get(\"production_companies\")),\n",
    "        \"production_countries\": list_to_str(data.get(\"production_countries\")),\n",
    "        \"seasons\": \"; \".join(\n",
    "            f\"S{s['season_number']}: {s.get('name')} ({s.get('episode_count')} eps)\"\n",
    "            for s in (data.get(\"seasons\") or []) if s.get(\"season_number\") is not None\n",
    "        ),\n",
    "        \"spoken_languages\": list_to_str(data.get(\"spoken_languages\")),\n",
    "        \"status\": data.get(\"status\"),\n",
    "        \"tagline\": data.get(\"tagline\"),\n",
    "        \"type_detail\": data.get(\"type\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "    }\n",
    "\n",
    "    # Î¶¨Î∑∞\n",
    "    rv_items = data.get(\"reviews\", {}).get(\"results\", [])\n",
    "    series[\"review\"] = \" || \".join([\n",
    "        f\"{r.get('author', '')}({r.get('author_details', {}).get('rating', '')}): {(r.get('content') or '').replace(chr(10), ' ')[:200]}\"\n",
    "        for r in rv_items[:5]\n",
    "    ])\n",
    "\n",
    "    # ÌÇ§ÏõåÎìú\n",
    "    kw_items = data.get(\"keywords\", {}).get(\"results\", [])\n",
    "    series[\"keyword\"] = \", \".join([k[\"name\"] for k in kw_items[:20] if k.get(\"name\")])\n",
    "\n",
    "    # Cast & Crew\n",
    "    credits = data.get(\"aggregate_credits\", {})\n",
    "    cast = credits.get(\"cast\") or []\n",
    "    crew = credits.get(\"crew\") or []\n",
    "\n",
    "    series[\"top_cast\"] = \", \".join([c[\"name\"] for c in cast[:4] if c.get(\"name\")])\n",
    "\n",
    "    dirs, wrs = set(), set()\n",
    "    for c in crew:\n",
    "        nm = c.get(\"name\")\n",
    "        if nm:\n",
    "            for job in (c.get(\"jobs\") or []):\n",
    "                jn = job.get(\"job\", \"\")\n",
    "                if \"Director\" in jn:\n",
    "                    dirs.add(nm)\n",
    "                if jn in [\"Writer\", \"Screenplay\", \"Story\"]:\n",
    "                    wrs.add(nm)\n",
    "\n",
    "    series[\"directors\"] = \", \".join(sorted(dirs)[:10])\n",
    "    series[\"writers\"] = \", \".join(sorted(wrs)[:10])\n",
    "\n",
    "    # üî• providers + external_ids + seasons Î≥ëÎ†¨ ÏàòÏßë\n",
    "    seasons_meta = [s for s in (data.get(\"seasons\") or []) if s.get(\"season_number\") is not None]\n",
    "    \n",
    "    tasks = [\n",
    "        fetch_providers(session, sid),      # 0: (flatrate, rent, buy)\n",
    "        fetch_external_ids(session, sid),   # 1: imdb_id\n",
    "    ]\n",
    "    tasks.extend([\n",
    "        fetch_single_season(session, sid, data.get(\"name\", \"\"), net_names, net_ids, s)\n",
    "        for s in seasons_meta\n",
    "    ])\n",
    "    \n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Providers Ï≤òÎ¶¨ (flatrate, rent, buy)\n",
    "    if isinstance(results[0], tuple) and len(results[0]) == 3:\n",
    "        series[\"providers_flatrate\"] = results[0][0]\n",
    "        series[\"providers_rent\"] = results[0][1]\n",
    "        series[\"providers_buy\"] = results[0][2]\n",
    "    else:\n",
    "        series[\"providers_flatrate\"] = \"{}\"\n",
    "        series[\"providers_rent\"] = \"{}\"\n",
    "        series[\"providers_buy\"] = \"{}\"\n",
    "    \n",
    "    # IMDB ID Ï≤òÎ¶¨\n",
    "    series[\"imdb_id\"] = results[1] if isinstance(results[1], str) else \"\"\n",
    "    \n",
    "    # Seasons Ï≤òÎ¶¨\n",
    "    season_records = [r for r in results[2:] if r and not isinstance(r, Exception)]\n",
    "\n",
    "    return series, season_records\n",
    "\n",
    "# ==========================================================\n",
    "# ÎπÑÎèôÍ∏∞ CSV Ïì∞Í∏∞\n",
    "# ==========================================================\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "async def save_to_csv_async(df, path, mode, header):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    await loop.run_in_executor(\n",
    "        executor,\n",
    "        lambda: df.to_csv(path, mode=mode, header=header, index=False, encoding=\"utf-8-sig\")\n",
    "    )\n",
    "\n",
    "# ==========================================================\n",
    "# Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ\n",
    "# ==========================================================\n",
    "def fill_series_gaps(series_df, seasons_df):\n",
    "    print(\"\\nüìä ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ\")\n",
    "\n",
    "    rtmap = seasons_df.groupby(\"series_id\")[\"avg_episode_runtime\"].mean().round()\n",
    "    mask = series_df[\"episode_run_time\"].astype(str).str.strip().isin([\"\", \"nan\"])\n",
    "    series_df.loc[mask, \"episode_run_time\"] = (\n",
    "        series_df.loc[mask, \"id\"].map(rtmap).fillna(0).astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    epmap = seasons_df.groupby(\"series_id\")[\"total_episodes\"].sum()\n",
    "    mask2 = series_df[\"number_of_episodes\"].isna()\n",
    "    series_df.loc[mask2, \"number_of_episodes\"] = series_df.loc[mask2, \"id\"].map(epmap)\n",
    "\n",
    "    seasmap = seasons_df.groupby(\"series_id\")[\"season_number\"].nunique()\n",
    "    mask3 = series_df[\"number_of_seasons\"].isna()\n",
    "    series_df.loc[mask3, \"number_of_seasons\"] = series_df.loc[mask3, \"id\"].map(seasmap)\n",
    "\n",
    "    lastair = seasons_df.sort_values(\"air_date\").groupby(\"series_id\")[\"air_date\"].last()\n",
    "    ladmask = series_df[\"last_air_date\"].astype(str).str.strip().isin([\"\", \"nan\", \"NaT\"])\n",
    "    series_df.loc[ladmask, \"last_air_date\"] = series_df.loc[ladmask, \"id\"].map(lastair)\n",
    "\n",
    "    print(\"‚úÖ ÏôÑÎ£å\\n\")\n",
    "    return series_df\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN\n",
    "# ==========================================================\n",
    "async def main():\n",
    "    print(\"=\" * 90)\n",
    "    print(\"üöÄ TMDB HYPER-OPTIMIZED ASYNC COLLECTOR\")\n",
    "    print(\"‚ú® ÏµúÏ¢Ö Í∞úÏÑ†: providers(flatrate/rent/buy Î∂ÑÎ¶¨) + IMDB ID Ï∂îÍ∞Ä\")\n",
    "    print(\"=\" * 90)\n",
    "    stats[\"start_time\"] = datetime.now()\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    # üî• ÏµúÏ†Å ÏÑ§Ï†ï\n",
    "    connector = aiohttp.TCPConnector(\n",
    "        limit=150,\n",
    "        limit_per_host=75,\n",
    "        ttl_dns_cache=600,\n",
    "        force_close=False,\n",
    "        enable_cleanup_closed=True\n",
    "    )\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as session:\n",
    "        # 1) ID ÏàòÏßë\n",
    "        print(\"\\nüìå 1Îã®Í≥Ñ: ID ÏàòÏßë\")\n",
    "        ids = sorted(list(await collect_ids_in_range(session, START_DATE, END_DATE)))\n",
    "        print(f\"‚ú® Ï¥ù ID: {len(ids):,}Í∞ú\")\n",
    "\n",
    "        # 2) ÏÉÅÏÑ∏ ÏàòÏßë\n",
    "        print(\"\\nüìå 2Îã®Í≥Ñ: ÏÉÅÏÑ∏ + ÏãúÏ¶å + IMDB ID + Providers ÏàòÏßë\")\n",
    "        batch_size = 1500\n",
    "        sem = asyncio.Semaphore(30)  # üî• ÏïàÏ†ïÏ†ÅÏù∏ ÎèôÏãúÏÑ±\n",
    "\n",
    "        async def fetch_one(i):\n",
    "            async with sem:\n",
    "                return await fetch_tv_details_and_seasons(session, i)\n",
    "\n",
    "        processed = 0\n",
    "        last_print = time.time()\n",
    "        \n",
    "        for start_idx in range(0, len(ids), batch_size):\n",
    "            batch_start = time.time()\n",
    "            part = ids[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            results = await asyncio.gather(*[fetch_one(x) for x in part], return_exceptions=True)\n",
    "            \n",
    "            bs, bse = [], []\n",
    "            for r in results:\n",
    "                if isinstance(r, tuple) and r[0]:\n",
    "                    s, ss = r\n",
    "                    bs.append(s)\n",
    "                    bse.extend(ss)\n",
    "                processed += 1\n",
    "\n",
    "            # CSV Ïì∞Í∏∞\n",
    "            if bs:\n",
    "                await save_to_csv_async(pd.DataFrame(bs), SERIES_TEMP, \"a\", not SERIES_TEMP.exists())\n",
    "            if bse:\n",
    "                await save_to_csv_async(pd.DataFrame(bse), SEASONS_TEMP, \"a\", not SEASONS_TEMP.exists())\n",
    "\n",
    "            # ÏßÑÌñâ ÏÉÅÌô© (5Ï¥àÎßàÎã§)\n",
    "            now = time.time()\n",
    "            if now - last_print >= 5:\n",
    "                elapsed = (datetime.now() - t0).total_seconds()\n",
    "                rate = processed / elapsed if elapsed > 0 else 0\n",
    "                eta = (len(ids) - processed) / rate / 60 if rate > 0 else 0\n",
    "                batch_time = now - batch_start\n",
    "                print(f\"‚è± {processed:,}/{len(ids):,} ({processed/len(ids)*100:.1f}%) | \"\n",
    "                      f\"{rate:.1f}/s | ETA: {eta:.0f}Î∂Ñ | Batch: {batch_time:.1f}s | \"\n",
    "                      f\"ÏöîÏ≤≠: {stats['requests']:,} | ÏóêÎü¨: {stats['errors']} | Ïû¨ÏãúÎèÑ: {stats['retries']}\")\n",
    "                last_print = now\n",
    "\n",
    "    # 3) ÏµúÏ¢Ö Ï≤òÎ¶¨\n",
    "    print(\"\\nüìå 3Îã®Í≥Ñ: ÏµúÏ¢Ö Ï≤òÎ¶¨\")\n",
    "    df_series = pd.read_csv(SERIES_TEMP)\n",
    "    df_seasons = pd.read_csv(SEASONS_TEMP) if SEASONS_TEMP.exists() else pd.DataFrame()\n",
    "\n",
    "    SERIES_COLS = [\n",
    "        \"id\",\"title\",\"type\",\"adult\",\"backdrop_path\",\"created_by\",\"episode_run_time\",\n",
    "        \"first_air_date\",\"genres\",\"genre_ids\",\"homepage\",\"in_production\",\"languages\",\n",
    "        \"last_air_date\",\"last_episode_to_air_id\",\"last_episode_to_air_name\",\n",
    "        \"last_episode_to_air_overview\",\"last_episode_to_air_vote_average\",\n",
    "        \"last_episode_to_air_vote_count\",\"last_episode_to_air_air_date\",\n",
    "        \"last_episode_to_air_episode_number\",\"last_episode_to_air_production_code\",\n",
    "        \"last_episode_to_air_runtime\",\"last_episode_to_air_season_number\",\n",
    "        \"last_episode_to_air_show_id\",\"last_episode_to_air_still_path\",\n",
    "        \"next_episode_to_air\",\"networks\",\"number_of_episodes\",\"number_of_seasons\",\n",
    "        \"origin_country\",\"original_language\",\"original_name\",\"overview\",\"popularity\",\n",
    "        \"poster_path\",\"production_companies\",\"production_countries\",\"seasons\",\n",
    "        \"spoken_languages\",\"status\",\"tagline\",\"type_detail\",\"vote_average\",\n",
    "        \"vote_count\",\"review\",\"keyword\",\"top_cast\",\"directors\",\"writers\",\n",
    "        \"providers_flatrate\",\"providers_rent\",\"providers_buy\",\"imdb_id\"  # ‚ú® Î≥ÄÍ≤ΩÎêú Ïª¨ÎüºÎì§\n",
    "    ]\n",
    "\n",
    "    SEASON_COLS = [\n",
    "        \"_id\",\"season_id\",\"series_id\",\"series_name\",\"season_number\",\"name\",\"air_date\",\n",
    "        \"overview\",\"vote_average\",\"vote_count\",\"network_names\",\"network_ids\",\n",
    "        \"total_episodes\",\"avg_episode_runtime\",\"poster_path\"\n",
    "    ]\n",
    "\n",
    "    df_series = df_series[SERIES_COLS].drop_duplicates(subset=[\"id\"])\n",
    "    if not df_seasons.empty:\n",
    "        df_seasons = df_seasons[SEASON_COLS].drop_duplicates(subset=[\"series_id\",\"season_number\"])\n",
    "        df_series = fill_series_gaps(df_series, df_seasons)\n",
    "\n",
    "    df_series.to_csv(SERIES_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    df_seasons.to_csv(SEASONS_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    try:\n",
    "        df_series.to_parquet(SERIES_PARQ, index=False)\n",
    "        df_seasons.to_parquet(SEASONS_PARQ, index=False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    elapsed = (datetime.now() - t0).total_seconds()/60\n",
    "\n",
    "    print(\"=\"*90)\n",
    "    print(\"üéâ ÏàòÏßë ÏôÑÎ£å!\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"üìå Series: {len(df_series):,}Í∞ú | Seasons: {len(df_seasons):,}Í∞ú\")\n",
    "    print(f\"üìå API ÏöîÏ≤≠: {stats['requests']:,}Ìöå | ÏóêÎü¨: {stats['errors']} | Ïû¨ÏãúÎèÑ: {stats['retries']}\")\n",
    "    print(f\"üìå ÌèâÍ∑† ÏÜçÎèÑ: {len(df_series)/elapsed:.1f} series/Î∂Ñ\")\n",
    "    print(f\"‚è± Ï¥ù ÏãúÍ∞Ñ: {elapsed:.1f}Î∂Ñ ({elapsed/60:.1f}ÏãúÍ∞Ñ)\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    # ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ Ï∂úÎ†•\n",
    "    print(\"\\nüìä ÏÉòÌîå Providers Îç∞Ïù¥ÌÑ∞:\")\n",
    "    if not df_series.empty and 'providers_flatrate' in df_series.columns:\n",
    "        sample = df_series[df_series['providers_flatrate'] != '{}'].head(1)\n",
    "        if not sample.empty:\n",
    "            print(f\"Title: {sample.iloc[0]['title']}\")\n",
    "            print(f\"Flatrate: {sample.iloc[0]['providers_flatrate'][:200]}...\")\n",
    "            print(f\"IMDB ID: {sample.iloc[0]['imdb_id']}\")\n",
    "    \n",
    "    executor.shutdown(wait=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e7263b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ nest_asyncio Ï†ÅÏö© ÏôÑÎ£å\n",
      "==========================================================================================\n",
      "üöÄ TMDB HYPER-OPTIMIZED ASYNC COLLECTOR\n",
      "‚ú® ÏµúÏ¢Ö Í∞úÏÑ†: providers(flatrate/rent/buy Î∂ÑÎ¶¨) + IMDB ID Ï∂îÍ∞Ä\n",
      "==========================================================================================\n",
      "\n",
      "üìå 1Îã®Í≥Ñ: ID ÏàòÏßë\n",
      "‚ú® Ï¥ù ID: 768Í∞ú\n",
      "\n",
      "üìå 2Îã®Í≥Ñ: ÏÉÅÏÑ∏ + ÏãúÏ¶å + IMDB ID + Providers ÏàòÏßë\n",
      "‚è± 768/768 (100.0%) | 10.9/s | ETA: 0Î∂Ñ | Batch: 68.9s | ÏöîÏ≤≠: 3,928 | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 8\n",
      "\n",
      "üìå 3Îã®Í≥Ñ: ÏµúÏ¢Ö Ï≤òÎ¶¨\n",
      "\n",
      "üìä ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ\n",
      "‚úÖ ÏôÑÎ£å\n",
      "\n",
      "==========================================================================================\n",
      "üéâ ÏàòÏßë ÏôÑÎ£å!\n",
      "==========================================================================================\n",
      "üìå Series: 768Í∞ú | Seasons: 1,577Í∞ú\n",
      "üìå API ÏöîÏ≤≠: 3,928Ìöå | ÏóêÎü¨: 0 | Ïû¨ÏãúÎèÑ: 8\n",
      "üìå ÌèâÍ∑† ÏÜçÎèÑ: 648.6 series/Î∂Ñ\n",
      "‚è± Ï¥ù ÏãúÍ∞Ñ: 1.2Î∂Ñ (0.0ÏãúÍ∞Ñ)\n",
      "==========================================================================================\n",
      "\n",
      "üìä ÏÉòÌîå Providers Îç∞Ïù¥ÌÑ∞:\n",
      "Title: House of Cards\n",
      "Flatrate: {\"AD\": [\"Netflix\"], \"AE\": [\"Netflix\"], \"AG\": [\"Netflix\"], \"AL\": [\"Netflix\"], \"AO\": [\"Netflix\"], \"AR\": [\"Netflix\"], \"AT\": [\"Netflix\"], \"AU\": [\"Netflix\"], \"AZ\": [\"Netflix\"], \"BA\": [\"Netflix\"], \"BB\": [\"N...\n",
      "IMDB ID: tt1856010\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# TMDB TV SERIES + SEASONS HYPER-OPTIMIZED ASYNC COLLECTOR\n",
    "# ÎπÑÎèôÍ∏∞ Î∞©Ïãù + Î≥ëÎ™© Ï†úÍ±∞ + Ï∫êÏã±\n",
    "# ‚ú® ÏµúÏ¢Ö Í∞úÏÑ†: providersÎ•º flatrate/rent/buyÎ°ú Î∂ÑÎ¶¨ + imdb_id Ï∂îÍ∞Ä\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "# ==========================================================\n",
    "# Jupyter async ÏßÄÏõê\n",
    "# ==========================================================\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"‚úÖ nest_asyncio Ï†ÅÏö© ÏôÑÎ£å\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ==========================================================\n",
    "# ÏÑ§Ï†ï\n",
    "# ==========================================================\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API_KEY missing\")\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "\n",
    "# üî• ÌïµÏã¨: TMDB Ïã§Ï†ú Ï†úÌïúÏùÄ 50/Ï¥àÏù¥ÏßÄÎßå, Î≤ÑÏä§Ìä∏Î•º Í≥†Î†§Ìï¥ 40ÏúºÎ°ú ÏïàÏ†ïÌôî\n",
    "MAX_CALLS_PER_SECOND = 25\n",
    "TIMEOUT = aiohttp.ClientTimeout(total=15, connect=5, sock_read=10)\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "START_DATE = \"2013-01-01\"\n",
    "END_DATE = \"2013-02-15\"\n",
    "\n",
    "SERIES_TEMP = Path(\"tv_series_temp.csv\")\n",
    "SEASONS_TEMP = Path(\"tv_seasons_temp.csv\")\n",
    "\n",
    "SERIES_CSV = \"tv_series_2013_0101_0215_FULL.csv\"\n",
    "SEASONS_CSV = \"tv_seasons_2013_0101_0215_FULL.csv\"\n",
    "SERIES_PARQ = \"tv_series_2013_0101_0215_FULL.parquet\"\n",
    "SEASONS_PARQ = \"tv_seasons_2013_0101_0215_FULL.parquet\"\n",
    "\n",
    "# ÌÜµÍ≥Ñ\n",
    "stats = {\"requests\": 0, \"errors\": 0, \"retries\": 0, \"start_time\": None}\n",
    "\n",
    "# ==========================================================\n",
    "# Í≥†ÏÑ±Îä• Rate Limiter (Lock-free Î≤ÑÏ†Ñ)\n",
    "# ==========================================================\n",
    "class LockFreeRateLimiter:\n",
    "    \"\"\"Lock ÏóÜÏù¥ ÎèôÏûëÌïòÎäî Ï¥àÍ≥†ÏÜç Rate Limiter\"\"\"\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.tokens = rate\n",
    "        self.updated_at = time.monotonic()\n",
    "    \n",
    "    async def acquire(self):\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self.updated_at\n",
    "        \n",
    "        # ÌÜ†ÌÅ∞ Î≥¥Ï∂© (Lock ÏóÜÏù¥)\n",
    "        self.tokens = min(self.rate, self.tokens + elapsed * self.rate)\n",
    "        self.updated_at = now\n",
    "        \n",
    "        # ÌÜ†ÌÅ∞ Î∂ÄÏ°±Ïãú ÏµúÏÜå ÎåÄÍ∏∞\n",
    "        if self.tokens < 1:\n",
    "            sleep_time = (1 - self.tokens) / self.rate\n",
    "            await asyncio.sleep(sleep_time)\n",
    "            self.tokens = 1\n",
    "        \n",
    "        self.tokens -= 1\n",
    "\n",
    "rate_limiter = LockFreeRateLimiter(MAX_CALLS_PER_SECOND)\n",
    "\n",
    "# ==========================================================\n",
    "# TMDB GET (Ïû¨ÏãúÎèÑ Î°úÏßÅ Í∞úÏÑ†)\n",
    "# ==========================================================\n",
    "async def tmdb_get(session, path, params=None, retry=0):\n",
    "    if retry >= MAX_RETRIES:\n",
    "        stats[\"errors\"] += 1\n",
    "        return None\n",
    "\n",
    "    params = params or {}\n",
    "    params.setdefault(\"api_key\", API_KEY)\n",
    "    params.setdefault(\"language\", \"en-US\")\n",
    "\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "    \n",
    "    await rate_limiter.acquire()\n",
    "    stats[\"requests\"] += 1\n",
    "\n",
    "    try:\n",
    "        async with session.get(url, params=params, headers=HEADERS) as resp:\n",
    "            # 429: Rate limit\n",
    "            if resp.status == 429:\n",
    "                stats[\"retries\"] += 1\n",
    "                retry_after = int(resp.headers.get(\"Retry-After\", 2))\n",
    "                await asyncio.sleep(retry_after)\n",
    "                return await tmdb_get(session, path, params, retry + 1)\n",
    "            \n",
    "            # 404: Not found (Ï†ïÏÉÅ ÏºÄÏù¥Ïä§)\n",
    "            if resp.status == 404:\n",
    "                return None\n",
    "            \n",
    "            # 5xx: ÏÑúÎ≤Ñ ÏóêÎü¨\n",
    "            if 500 <= resp.status < 600:\n",
    "                stats[\"retries\"] += 1\n",
    "                await asyncio.sleep(0.5 ** retry)  # ÏßÄÏàò Í∞êÏÜå\n",
    "                return await tmdb_get(session, path, params, retry + 1)\n",
    "            \n",
    "            resp.raise_for_status()\n",
    "            return await resp.json()\n",
    "    \n",
    "    except asyncio.TimeoutError:\n",
    "        stats[\"errors\"] += 1\n",
    "        if retry < MAX_RETRIES - 1:\n",
    "            await asyncio.sleep(0.2)\n",
    "            return await tmdb_get(session, path, params, retry + 1)\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        stats[\"errors\"] += 1\n",
    "        if retry < MAX_RETRIES - 1:\n",
    "            await asyncio.sleep(0.1)\n",
    "            return await tmdb_get(session, path, params, retry + 1)\n",
    "        return None\n",
    "\n",
    "# ==========================================================\n",
    "# Discover\n",
    "# ==========================================================\n",
    "async def fetch_discover_page(session, page, gte, lte):\n",
    "    params = {\n",
    "        \"sort_by\": \"popularity.desc\",\n",
    "        \"first_air_date.gte\": gte,\n",
    "        \"first_air_date.lte\": lte,\n",
    "        \"page\": page,\n",
    "        \"include_adult\": \"true\",\n",
    "    }\n",
    "    data = await tmdb_get(session, \"/discover/tv\", params)\n",
    "    if not data:\n",
    "        return [], 1, 0\n",
    "    return data.get(\"results\", []), data.get(\"total_pages\", 1), data.get(\"total_results\", 0)\n",
    "\n",
    "# ==========================================================\n",
    "# ID ÏàòÏßë (Í∞úÏÑ†)\n",
    "# ==========================================================\n",
    "async def collect_ids_in_range(session, start, end, depth=0):\n",
    "    res1, total_pages, total_results = await fetch_discover_page(session, 1, start, end)\n",
    "\n",
    "    if total_results == 0:\n",
    "        return set()\n",
    "\n",
    "    if total_pages <= 500:\n",
    "        ids = {r[\"id\"] for r in res1}\n",
    "        \n",
    "        if total_pages > 1:\n",
    "            # üî• Í∞úÏÑ†: Ï≤≠ÌÅ¨Î°ú ÎÇòÎà†ÏÑú Î©îÎ™®Î¶¨ Ìö®Ïú® Í∞úÏÑ†\n",
    "            chunk_size = 100\n",
    "            for chunk_start in range(2, total_pages + 1, chunk_size):\n",
    "                chunk_end = min(chunk_start + chunk_size, total_pages + 1)\n",
    "                results = await asyncio.gather(\n",
    "                    *[fetch_discover_page(session, p, start, end) for p in range(chunk_start, chunk_end)],\n",
    "                    return_exceptions=True\n",
    "                )\n",
    "                for r in results:\n",
    "                    if isinstance(r, tuple):\n",
    "                        ids.update(x[\"id\"] for x in r[0])\n",
    "        \n",
    "        return ids\n",
    "\n",
    "    # ÎÇ†Ïßú Î∂ÑÌï†\n",
    "    start_dt = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    mid_dt = start_dt + (end_dt - start_dt) / 2\n",
    "    mid = mid_dt.strftime(\"%Y-%m-%d\")\n",
    "    right_start = (mid_dt + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    left, right = await asyncio.gather(\n",
    "        collect_ids_in_range(session, start, mid, depth + 1),\n",
    "        collect_ids_in_range(session, right_start, end, depth + 1)\n",
    "    )\n",
    "    return left | right\n",
    "\n",
    "# ==========================================================\n",
    "# Helpers (Ï∫êÏã± Ï∂îÍ∞Ä)\n",
    "# ==========================================================\n",
    "@lru_cache(maxsize=1024)\n",
    "def list_to_str_cached(lst_tuple, key=\"name\"):\n",
    "    \"\"\"Î¶¨Ïä§Ìä∏Î•º Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò (Ï∫êÏã±)\"\"\"\n",
    "    if not lst_tuple:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key, \"\")) for i in lst_tuple if i.get(key))\n",
    "\n",
    "def list_to_str(lst, key=\"name\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    # ÎîïÏÖîÎÑàÎ¶¨ Î¶¨Ïä§Ìä∏Î•º ÌäúÌîåÎ°ú Î≥ÄÌôòÌïòÏó¨ Ï∫êÏã± Í∞ÄÎä•ÌïòÍ≤å\n",
    "    try:\n",
    "        lst_tuple = tuple(tuple(d.items()) if isinstance(d, dict) else d for d in lst)\n",
    "        return list_to_str_cached(lst_tuple, key)\n",
    "    except:\n",
    "        return \", \".join(str(i.get(key, \"\")) for i in lst if i.get(key))\n",
    "\n",
    "def list_ids_to_str(lst, key=\"id\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key)) for i in lst if i.get(key))\n",
    "\n",
    "# ==========================================================\n",
    "# ‚ú® NEW: External IDs (IMDB ID)\n",
    "# ==========================================================\n",
    "async def fetch_external_ids(session, sid):\n",
    "    \"\"\"ÏãúÎ¶¨Ï¶àÏùò external IDsÎ•º Í∞ÄÏ†∏ÏôÄÏÑú IMDB ID Î∞òÌôò\"\"\"\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/external_ids\")\n",
    "    if not data:\n",
    "        return \"\"\n",
    "    return data.get(\"imdb_id\", \"\")\n",
    "\n",
    "# ==========================================================\n",
    "# ‚ú® UPDATED: Providers (flatrate/rent/buy Î∂ÑÎ¶¨)\n",
    "# ==========================================================\n",
    "async def fetch_providers(session, sid):\n",
    "    \"\"\"\n",
    "    Î™®Îì† Íµ≠Í∞ÄÏùò streaming providers ÏàòÏßë\n",
    "    flatrate(Íµ¨ÎèÖÌòï), rent(ÎåÄÏó¨), buy(Íµ¨Îß§) ÌÉÄÏûÖÎ≥ÑÎ°ú Î∂ÑÎ¶¨\n",
    "    Î∞òÌôò: (flatrate_dict, rent_dict, buy_dict)\n",
    "    \"\"\"\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/watch/providers\")\n",
    "    if not data:\n",
    "        return \"{}\", \"{}\", \"{}\"\n",
    "    \n",
    "    providers_results = data.get(\"results\", {})\n",
    "    \n",
    "    # Íµ≠Í∞ÄÎ≥Ñ OTT Ï†ïÎ≥¥ ÏàòÏßë\n",
    "    providers_flatrate = {}\n",
    "    providers_rent = {}\n",
    "    providers_buy = {}\n",
    "    \n",
    "    for country, info in providers_results.items():\n",
    "        # flatrate: Íµ¨ÎèÖÌòï (Netflix, Disney+ Îì±)\n",
    "        flatrate = info.get(\"flatrate\", [])\n",
    "        if flatrate:\n",
    "            provider_names = [p.get(\"provider_name\") for p in flatrate if p.get(\"provider_name\")]\n",
    "            if provider_names:\n",
    "                providers_flatrate[country] = provider_names\n",
    "        \n",
    "        # rent: ÎåÄÏó¨\n",
    "        rent = info.get(\"rent\", [])\n",
    "        if rent:\n",
    "            provider_names = [p.get(\"provider_name\") for p in rent if p.get(\"provider_name\")]\n",
    "            if provider_names:\n",
    "                providers_rent[country] = provider_names\n",
    "        \n",
    "        # buy: Íµ¨Îß§\n",
    "        buy = info.get(\"buy\", [])\n",
    "        if buy:\n",
    "            provider_names = [p.get(\"provider_name\") for p in buy if p.get(\"provider_name\")]\n",
    "            if provider_names:\n",
    "                providers_buy[country] = provider_names\n",
    "    \n",
    "    # JSON Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôòÌïòÏó¨ Î∞òÌôò\n",
    "    return (\n",
    "        json.dumps(providers_flatrate, ensure_ascii=False) if providers_flatrate else \"{}\",\n",
    "        json.dumps(providers_rent, ensure_ascii=False) if providers_rent else \"{}\",\n",
    "        json.dumps(providers_buy, ensure_ascii=False) if providers_buy else \"{}\"\n",
    "    )\n",
    "\n",
    "# ==========================================================\n",
    "# Single Season\n",
    "# ==========================================================\n",
    "async def fetch_single_season(session, sid, sname, net_names, net_ids, meta):\n",
    "    sn = meta.get(\"season_number\")\n",
    "    if sn is None:\n",
    "        return None\n",
    "\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/season/{sn}\")\n",
    "\n",
    "    if not data:\n",
    "        return {\n",
    "            \"_id\": f\"{sid}_{sn}\",\n",
    "            \"season_id\": meta.get(\"id\"),\n",
    "            \"series_id\": sid,\n",
    "            \"series_name\": sname,\n",
    "            \"season_number\": sn,\n",
    "            \"name\": meta.get(\"name\"),\n",
    "            \"air_date\": meta.get(\"air_date\"),\n",
    "            \"overview\": meta.get(\"overview\"),\n",
    "            \"vote_average\": meta.get(\"vote_average\"),\n",
    "            \"vote_count\": meta.get(\"vote_count\"),\n",
    "            \"network_names\": net_names,\n",
    "            \"network_ids\": net_ids,\n",
    "            \"total_episodes\": meta.get(\"episode_count\"),\n",
    "            \"avg_episode_runtime\": None,\n",
    "            \"poster_path\": meta.get(\"poster_path\"),\n",
    "        }\n",
    "\n",
    "    eps = data.get(\"episodes\") or []\n",
    "    runtimes = [ep[\"runtime\"] for ep in eps if ep.get(\"runtime\")]\n",
    "    avg_rt = sum(runtimes) / len(runtimes) if runtimes else None\n",
    "\n",
    "    return {\n",
    "        \"_id\": data.get(\"_id\") or f\"{sid}_{sn}\",\n",
    "        \"season_id\": data.get(\"id\"),\n",
    "        \"series_id\": sid,\n",
    "        \"series_name\": sname,\n",
    "        \"season_number\": data.get(\"season_number\"),\n",
    "        \"name\": data.get(\"name\"),\n",
    "        \"air_date\": data.get(\"air_date\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "        \"network_names\": net_names,\n",
    "        \"network_ids\": net_ids,\n",
    "        \"total_episodes\": len(eps),\n",
    "        \"avg_episode_runtime\": avg_rt,\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# ‚ú® UPDATED: TV Details + Seasons + IMDB ID + Providers\n",
    "# ==========================================================\n",
    "async def fetch_tv_details_and_seasons(session, sid):\n",
    "    params = {\"append_to_response\": \"reviews,keywords,aggregate_credits\"}\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}\", params)\n",
    "    if not data:\n",
    "        return None, []\n",
    "\n",
    "    genres = data.get(\"genres\") or []\n",
    "    networks = data.get(\"networks\") or []\n",
    "    last_ep = data.get(\"last_episode_to_air\") or {}\n",
    "\n",
    "    net_names = list_to_str(networks)\n",
    "    net_ids = list_ids_to_str(networks)\n",
    "\n",
    "    series = {\n",
    "        \"id\": data.get(\"id\"),\n",
    "        \"title\": data.get(\"name\"),\n",
    "        \"type\": \"tv_series\",\n",
    "        \"adult\": data.get(\"adult\"),\n",
    "        \"backdrop_path\": data.get(\"backdrop_path\"),\n",
    "        \"created_by\": list_to_str(data.get(\"created_by\")),\n",
    "        \"episode_run_time\": \", \".join(map(str, data.get(\"episode_run_time\", []))) or \"\",\n",
    "        \"first_air_date\": data.get(\"first_air_date\"),\n",
    "        \"genres\": list_to_str(genres),\n",
    "        \"genre_ids\": \", \".join(str(g[\"id\"]) for g in genres if g.get(\"id\")),\n",
    "        \"homepage\": data.get(\"homepage\"),\n",
    "        \"in_production\": data.get(\"in_production\"),\n",
    "        \"languages\": \", \".join(data.get(\"languages\", [])) or \"\",\n",
    "        \"last_air_date\": data.get(\"last_air_date\"),\n",
    "        \"last_episode_to_air_id\": last_ep.get(\"id\"),\n",
    "        \"last_episode_to_air_name\": last_ep.get(\"name\"),\n",
    "        \"last_episode_to_air_overview\": last_ep.get(\"overview\"),\n",
    "        \"last_episode_to_air_vote_average\": last_ep.get(\"vote_average\"),\n",
    "        \"last_episode_to_air_vote_count\": last_ep.get(\"vote_count\"),\n",
    "        \"last_episode_to_air_air_date\": last_ep.get(\"air_date\"),\n",
    "        \"last_episode_to_air_episode_number\": last_ep.get(\"episode_number\"),\n",
    "        \"last_episode_to_air_production_code\": last_ep.get(\"production_code\"),\n",
    "        \"last_episode_to_air_runtime\": last_ep.get(\"runtime\"),\n",
    "        \"last_episode_to_air_season_number\": last_ep.get(\"season_number\"),\n",
    "        \"last_episode_to_air_show_id\": last_ep.get(\"show_id\"),\n",
    "        \"last_episode_to_air_still_path\": last_ep.get(\"still_path\"),\n",
    "        \"next_episode_to_air\": str(data.get(\"next_episode_to_air\")) if data.get(\"next_episode_to_air\") else \"\",\n",
    "        \"networks\": net_names,\n",
    "        \"number_of_episodes\": data.get(\"number_of_episodes\"),\n",
    "        \"number_of_seasons\": data.get(\"number_of_seasons\"),\n",
    "        \"origin_country\": \", \".join(data.get(\"origin_country\", [])) or \"\",\n",
    "        \"original_language\": data.get(\"original_language\"),\n",
    "        \"original_name\": data.get(\"original_name\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"popularity\": data.get(\"popularity\"),\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "        \"production_companies\": list_to_str(data.get(\"production_companies\")),\n",
    "        \"production_countries\": list_to_str(data.get(\"production_countries\")),\n",
    "        \"seasons\": \"; \".join(\n",
    "            f\"S{s['season_number']}: {s.get('name')} ({s.get('episode_count')} eps)\"\n",
    "            for s in (data.get(\"seasons\") or []) if s.get(\"season_number\") is not None\n",
    "        ),\n",
    "        \"spoken_languages\": list_to_str(data.get(\"spoken_languages\")),\n",
    "        \"status\": data.get(\"status\"),\n",
    "        \"tagline\": data.get(\"tagline\"),\n",
    "        \"type_detail\": data.get(\"type\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "    }\n",
    "\n",
    "    # Î¶¨Î∑∞\n",
    "    rv_items = data.get(\"reviews\", {}).get(\"results\", [])\n",
    "    series[\"review\"] = \" || \".join([\n",
    "        f\"{r.get('author', '')}({r.get('author_details', {}).get('rating', '')}): {(r.get('content') or '').replace(chr(10), ' ')[:200]}\"\n",
    "        for r in rv_items[:5]\n",
    "    ])\n",
    "\n",
    "    # ÌÇ§ÏõåÎìú\n",
    "    kw_items = data.get(\"keywords\", {}).get(\"results\", [])\n",
    "    series[\"keyword\"] = \", \".join([k[\"name\"] for k in kw_items[:20] if k.get(\"name\")])\n",
    "\n",
    "    # Cast & Crew\n",
    "    credits = data.get(\"aggregate_credits\", {})\n",
    "    cast = credits.get(\"cast\") or []\n",
    "    crew = credits.get(\"crew\") or []\n",
    "\n",
    "    series[\"top_cast\"] = \", \".join([c[\"name\"] for c in cast[:4] if c.get(\"name\")])\n",
    "\n",
    "    dirs, wrs = set(), set()\n",
    "    for c in crew:\n",
    "        nm = c.get(\"name\")\n",
    "        if nm:\n",
    "            for job in (c.get(\"jobs\") or []):\n",
    "                jn = job.get(\"job\", \"\")\n",
    "                if \"Director\" in jn:\n",
    "                    dirs.add(nm)\n",
    "                if jn in [\"Writer\", \"Screenplay\", \"Story\"]:\n",
    "                    wrs.add(nm)\n",
    "\n",
    "    series[\"directors\"] = \", \".join(sorted(dirs)[:10])\n",
    "    series[\"writers\"] = \", \".join(sorted(wrs)[:10])\n",
    "\n",
    "    # üî• providers + external_ids + seasons Î≥ëÎ†¨ ÏàòÏßë\n",
    "    seasons_meta = [s for s in (data.get(\"seasons\") or []) if s.get(\"season_number\") is not None]\n",
    "    \n",
    "    tasks = [\n",
    "        fetch_providers(session, sid),      # 0: (flatrate, rent, buy)\n",
    "        fetch_external_ids(session, sid),   # 1: imdb_id\n",
    "    ]\n",
    "    tasks.extend([\n",
    "        fetch_single_season(session, sid, data.get(\"name\", \"\"), net_names, net_ids, s)\n",
    "        for s in seasons_meta\n",
    "    ])\n",
    "    \n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Providers Ï≤òÎ¶¨ (flatrate, rent, buy)\n",
    "    if isinstance(results[0], tuple) and len(results[0]) == 3:\n",
    "        series[\"providers_flatrate\"] = results[0][0]\n",
    "        series[\"providers_rent\"] = results[0][1]\n",
    "        series[\"providers_buy\"] = results[0][2]\n",
    "    else:\n",
    "        series[\"providers_flatrate\"] = \"{}\"\n",
    "        series[\"providers_rent\"] = \"{}\"\n",
    "        series[\"providers_buy\"] = \"{}\"\n",
    "    \n",
    "    # IMDB ID Ï≤òÎ¶¨\n",
    "    series[\"imdb_id\"] = results[1] if isinstance(results[1], str) else \"\"\n",
    "    \n",
    "    # Seasons Ï≤òÎ¶¨\n",
    "    season_records = [r for r in results[2:] if r and not isinstance(r, Exception)]\n",
    "\n",
    "    return series, season_records\n",
    "\n",
    "# ==========================================================\n",
    "# ÎπÑÎèôÍ∏∞ CSV Ïì∞Í∏∞\n",
    "# ==========================================================\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "async def save_to_csv_async(df, path, mode, header):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    await loop.run_in_executor(\n",
    "        executor,\n",
    "        lambda: df.to_csv(path, mode=mode, header=header, index=False, encoding=\"utf-8-sig\")\n",
    "    )\n",
    "\n",
    "# ==========================================================\n",
    "# Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ\n",
    "# ==========================================================\n",
    "def fill_series_gaps(series_df, seasons_df):\n",
    "    print(\"\\nüìä ÏãúÏ¶å Í∏∞Î∞ò Í≤∞Ï∏°Ïπò Î≥¥ÏôÑ\")\n",
    "\n",
    "    rtmap = seasons_df.groupby(\"series_id\")[\"avg_episode_runtime\"].mean().round()\n",
    "    mask = series_df[\"episode_run_time\"].astype(str).str.strip().isin([\"\", \"nan\"])\n",
    "    series_df.loc[mask, \"episode_run_time\"] = (\n",
    "        series_df.loc[mask, \"id\"].map(rtmap).fillna(0).astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    epmap = seasons_df.groupby(\"series_id\")[\"total_episodes\"].sum()\n",
    "    mask2 = series_df[\"number_of_episodes\"].isna()\n",
    "    series_df.loc[mask2, \"number_of_episodes\"] = series_df.loc[mask2, \"id\"].map(epmap)\n",
    "\n",
    "    seasmap = seasons_df.groupby(\"series_id\")[\"season_number\"].nunique()\n",
    "    mask3 = series_df[\"number_of_seasons\"].isna()\n",
    "    series_df.loc[mask3, \"number_of_seasons\"] = series_df.loc[mask3, \"id\"].map(seasmap)\n",
    "\n",
    "    lastair = seasons_df.sort_values(\"air_date\").groupby(\"series_id\")[\"air_date\"].last()\n",
    "    ladmask = series_df[\"last_air_date\"].astype(str).str.strip().isin([\"\", \"nan\", \"NaT\"])\n",
    "    series_df.loc[ladmask, \"last_air_date\"] = series_df.loc[ladmask, \"id\"].map(lastair)\n",
    "\n",
    "    print(\"‚úÖ ÏôÑÎ£å\\n\")\n",
    "    return series_df\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN\n",
    "# ==========================================================\n",
    "async def main():\n",
    "    print(\"=\" * 90)\n",
    "    print(\"üöÄ TMDB HYPER-OPTIMIZED ASYNC COLLECTOR\")\n",
    "    print(\"‚ú® ÏµúÏ¢Ö Í∞úÏÑ†: providers(flatrate/rent/buy Î∂ÑÎ¶¨) + IMDB ID Ï∂îÍ∞Ä\")\n",
    "    print(\"=\" * 90)\n",
    "    stats[\"start_time\"] = datetime.now()\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    # üî• ÏµúÏ†Å ÏÑ§Ï†ï\n",
    "    connector = aiohttp.TCPConnector(\n",
    "        limit=150,\n",
    "        limit_per_host=75,\n",
    "        ttl_dns_cache=600,\n",
    "        force_close=False,\n",
    "        enable_cleanup_closed=True\n",
    "    )\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as session:\n",
    "        # 1) ID ÏàòÏßë\n",
    "        print(\"\\nüìå 1Îã®Í≥Ñ: ID ÏàòÏßë\")\n",
    "        ids = sorted(list(await collect_ids_in_range(session, START_DATE, END_DATE)))\n",
    "        print(f\"‚ú® Ï¥ù ID: {len(ids):,}Í∞ú\")\n",
    "\n",
    "        # 2) ÏÉÅÏÑ∏ ÏàòÏßë\n",
    "        print(\"\\nüìå 2Îã®Í≥Ñ: ÏÉÅÏÑ∏ + ÏãúÏ¶å + IMDB ID + Providers ÏàòÏßë\")\n",
    "        batch_size = 1500\n",
    "        sem = asyncio.Semaphore(30)  # üî• ÏïàÏ†ïÏ†ÅÏù∏ ÎèôÏãúÏÑ±\n",
    "\n",
    "        async def fetch_one(i):\n",
    "            async with sem:\n",
    "                return await fetch_tv_details_and_seasons(session, i)\n",
    "\n",
    "        processed = 0\n",
    "        last_print = time.time()\n",
    "        \n",
    "        for start_idx in range(0, len(ids), batch_size):\n",
    "            batch_start = time.time()\n",
    "            part = ids[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            results = await asyncio.gather(*[fetch_one(x) for x in part], return_exceptions=True)\n",
    "            \n",
    "            bs, bse = [], []\n",
    "            for r in results:\n",
    "                if isinstance(r, tuple) and r[0]:\n",
    "                    s, ss = r\n",
    "                    bs.append(s)\n",
    "                    bse.extend(ss)\n",
    "                processed += 1\n",
    "\n",
    "            # CSV Ïì∞Í∏∞\n",
    "            if bs:\n",
    "                await save_to_csv_async(pd.DataFrame(bs), SERIES_TEMP, \"a\", not SERIES_TEMP.exists())\n",
    "            if bse:\n",
    "                await save_to_csv_async(pd.DataFrame(bse), SEASONS_TEMP, \"a\", not SEASONS_TEMP.exists())\n",
    "\n",
    "            # ÏßÑÌñâ ÏÉÅÌô© (5Ï¥àÎßàÎã§)\n",
    "            now = time.time()\n",
    "            if now - last_print >= 5:\n",
    "                elapsed = (datetime.now() - t0).total_seconds()\n",
    "                rate = processed / elapsed if elapsed > 0 else 0\n",
    "                eta = (len(ids) - processed) / rate / 60 if rate > 0 else 0\n",
    "                batch_time = now - batch_start\n",
    "                print(f\"‚è± {processed:,}/{len(ids):,} ({processed/len(ids)*100:.1f}%) | \"\n",
    "                      f\"{rate:.1f}/s | ETA: {eta:.0f}Î∂Ñ | Batch: {batch_time:.1f}s | \"\n",
    "                      f\"ÏöîÏ≤≠: {stats['requests']:,} | ÏóêÎü¨: {stats['errors']} | Ïû¨ÏãúÎèÑ: {stats['retries']}\")\n",
    "                last_print = now\n",
    "\n",
    "    # 3) ÏµúÏ¢Ö Ï≤òÎ¶¨\n",
    "    print(\"\\nüìå 3Îã®Í≥Ñ: ÏµúÏ¢Ö Ï≤òÎ¶¨\")\n",
    "    df_series = pd.read_csv(SERIES_TEMP)\n",
    "    df_seasons = pd.read_csv(SEASONS_TEMP) if SEASONS_TEMP.exists() else pd.DataFrame()\n",
    "\n",
    "    SERIES_COLS = [\n",
    "        \"id\",\"title\",\"type\",\"adult\",\"backdrop_path\",\"created_by\",\"episode_run_time\",\n",
    "        \"first_air_date\",\"genres\",\"genre_ids\",\"homepage\",\"in_production\",\"languages\",\n",
    "        \"last_air_date\",\"last_episode_to_air_id\",\"last_episode_to_air_name\",\n",
    "        \"last_episode_to_air_overview\",\"last_episode_to_air_vote_average\",\n",
    "        \"last_episode_to_air_vote_count\",\"last_episode_to_air_air_date\",\n",
    "        \"last_episode_to_air_episode_number\",\"last_episode_to_air_production_code\",\n",
    "        \"last_episode_to_air_runtime\",\"last_episode_to_air_season_number\",\n",
    "        \"last_episode_to_air_show_id\",\"last_episode_to_air_still_path\",\n",
    "        \"next_episode_to_air\",\"networks\",\"number_of_episodes\",\"number_of_seasons\",\n",
    "        \"origin_country\",\"original_language\",\"original_name\",\"overview\",\"popularity\",\n",
    "        \"poster_path\",\"production_companies\",\"production_countries\",\"seasons\",\n",
    "        \"spoken_languages\",\"status\",\"tagline\",\"type_detail\",\"vote_average\",\n",
    "        \"vote_count\",\"review\",\"keyword\",\"top_cast\",\"directors\",\"writers\",\n",
    "        \"providers_flatrate\",\"providers_rent\",\"providers_buy\",\"imdb_id\"  # ‚ú® Î≥ÄÍ≤ΩÎêú Ïª¨ÎüºÎì§\n",
    "    ]\n",
    "\n",
    "    SEASON_COLS = [\n",
    "        \"_id\",\"season_id\",\"series_id\",\"series_name\",\"season_number\",\"name\",\"air_date\",\n",
    "        \"overview\",\"vote_average\",\"vote_count\",\"network_names\",\"network_ids\",\n",
    "        \"total_episodes\",\"avg_episode_runtime\",\"poster_path\"\n",
    "    ]\n",
    "\n",
    "    df_series = df_series[SERIES_COLS].drop_duplicates(subset=[\"id\"])\n",
    "    if not df_seasons.empty:\n",
    "        df_seasons = df_seasons[SEASON_COLS].drop_duplicates(subset=[\"series_id\",\"season_number\"])\n",
    "        df_series = fill_series_gaps(df_series, df_seasons)\n",
    "\n",
    "    df_series.to_csv(SERIES_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    df_seasons.to_csv(SEASONS_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    try:\n",
    "        df_series.to_parquet(SERIES_PARQ, index=False)\n",
    "        df_seasons.to_parquet(SEASONS_PARQ, index=False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    elapsed = (datetime.now() - t0).total_seconds()/60\n",
    "\n",
    "    print(\"=\"*90)\n",
    "    print(\"üéâ ÏàòÏßë ÏôÑÎ£å!\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"üìå Series: {len(df_series):,}Í∞ú | Seasons: {len(df_seasons):,}Í∞ú\")\n",
    "    print(f\"üìå API ÏöîÏ≤≠: {stats['requests']:,}Ìöå | ÏóêÎü¨: {stats['errors']} | Ïû¨ÏãúÎèÑ: {stats['retries']}\")\n",
    "    print(f\"üìå ÌèâÍ∑† ÏÜçÎèÑ: {len(df_series)/elapsed:.1f} series/Î∂Ñ\")\n",
    "    print(f\"‚è± Ï¥ù ÏãúÍ∞Ñ: {elapsed:.1f}Î∂Ñ ({elapsed/60:.1f}ÏãúÍ∞Ñ)\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    # ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ Ï∂úÎ†•\n",
    "    print(\"\\nüìä ÏÉòÌîå Providers Îç∞Ïù¥ÌÑ∞:\")\n",
    "    if not df_series.empty and 'providers_flatrate' in df_series.columns:\n",
    "        sample = df_series[df_series['providers_flatrate'] != '{}'].head(1)\n",
    "        if not sample.empty:\n",
    "            print(f\"Title: {sample.iloc[0]['title']}\")\n",
    "            print(f\"Flatrate: {sample.iloc[0]['providers_flatrate'][:200]}...\")\n",
    "            print(f\"IMDB ID: {sample.iloc[0]['imdb_id']}\")\n",
    "    \n",
    "    executor.shutdown(wait=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c873cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Íπ®ÏßÑ Ï§ÑÏùÑ Î¨¥ÏãúÌïòÍ≥† Ï†ïÏÉÅ Ï§ÑÎßå Î™®ÏïÑÏÑú clean ÌååÏùº ÏÉùÏÑ±\n",
    "def clean_csv(input_path, output_path):\n",
    "    with open(input_path, \"r\", errors=\"ignore\") as infile, open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in infile:\n",
    "            if line.count(\",\") >= 5:  # ÏµúÏÜå 5Í∞ú Ïù¥ÏÉÅ ÏâºÌëú ÏûàÏúºÎ©¥ Ï†ïÏÉÅ ÎùºÏù∏ÏúºÎ°ú Í∞ÑÏ£º\n",
    "                outfile.write(line)\n",
    "\n",
    "clean_csv(\"tv_series_2005_2015_FULL.csv\", \"tv_series_clean.csv\")\n",
    "clean_csv(\"tv_seasons_2005_2015_FULL.csv\", \"tv_seasons_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440bcaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_series = pd.read_csv(\"tv_series_clean.csv\", engine=\"python\")\n",
    "df_seasons = pd.read_csv(\"tv_seasons_clean.csv\", engine=\"python\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94666ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TV SERIES NULL RATIO (%) =====\n",
      "next_episode_to_air                    99.586516\n",
      "last_episode_to_air_production_code    99.415328\n",
      "review                                 98.638399\n",
      "tagline                                96.597314\n",
      "last_episode_to_air_still_path         80.208059\n",
      "writers                                78.182776\n",
      "created_by                             74.601001\n",
      "last_episode_to_air_overview           69.109824\n",
      "keyword                                67.761391\n",
      "homepage                               67.121412\n",
      "directors                              67.058204\n",
      "production_companies                   58.401370\n",
      "imdb_id                                48.206479\n",
      "backdrop_path                          47.102976\n",
      "last_episode_to_air_runtime            46.233869\n",
      "production_countries                   43.745062\n",
      "overview                               43.305241\n",
      "networks                               37.263629\n",
      "top_cast                               30.990255\n",
      "genre_ids                              29.007111\n",
      "genres                                 29.007111\n",
      "spoken_languages                       26.381354\n",
      "languages                              26.099552\n",
      "poster_path                            25.140901\n",
      "origin_country                         10.237029\n",
      "last_episode_to_air_name                0.150119\n",
      "last_episode_to_air_episode_number      0.144851\n",
      "last_episode_to_air_show_id             0.144851\n",
      "last_episode_to_air_season_number       0.144851\n",
      "last_episode_to_air_vote_average        0.144851\n",
      "last_episode_to_air_id                  0.144851\n",
      "last_episode_to_air_air_date            0.144851\n",
      "last_episode_to_air_vote_count          0.144851\n",
      "last_air_date                           0.079010\n",
      "adult                                   0.002634\n",
      "title                                   0.002634\n",
      "first_air_date                          0.002634\n",
      "seasons                                 0.002634\n",
      "original_name                           0.002634\n",
      "in_production                           0.000000\n",
      "Áô§ÌìÅd                                     0.000000\n",
      "type                                    0.000000\n",
      "episode_run_time                        0.000000\n",
      "original_language                       0.000000\n",
      "number_of_episodes                      0.000000\n",
      "number_of_seasons                       0.000000\n",
      "popularity                              0.000000\n",
      "vote_average                            0.000000\n",
      "type_detail                             0.000000\n",
      "status                                  0.000000\n",
      "vote_count                              0.000000\n",
      "providers_flatrate                      0.000000\n",
      "providers_rent                          0.000000\n",
      "providers_buy                           0.000000\n",
      "dtype: float64\n",
      "\n",
      "===== TV SEASONS NULL RATIO (%) =====\n",
      "vote_count             100.000000\n",
      "overview                91.912091\n",
      "poster_path             65.509027\n",
      "avg_episode_runtime     40.871616\n",
      "network_ids             24.183152\n",
      "network_names           24.183152\n",
      "air_date                 9.057150\n",
      "name                     0.001168\n",
      "series_name              0.001168\n",
      "season_id                0.000000\n",
      "series_id                0.000000\n",
      "season_number            0.000000\n",
      "Áô§_id                     0.000000\n",
      "vote_average             0.000000\n",
      "total_episodes           0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def get_null_ratio(df):\n",
    "    null_ratio = (df.isnull().sum() / len(df)) * 100\n",
    "    return null_ratio.sort_values(ascending=False)\n",
    "\n",
    "series_null_ratio = get_null_ratio(df_series)\n",
    "seasons_null_ratio = get_null_ratio(df_seasons)\n",
    "\n",
    "print(\"\\n===== TV SERIES NULL RATIO (%) =====\")\n",
    "print(series_null_ratio)\n",
    "\n",
    "print(\"\\n===== TV SEASONS NULL RATIO (%) =====\")\n",
    "print(seasons_null_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09a8cfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_11924\\4136345038.py:4: DtypeWarning: Columns (3,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_series = pd.read_csv(\"tv_series_2005_2015_FULL.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== tv_series_clean.csv Í≤∞Ï∏°Î•† ===\n",
      "id                                      0.000000\n",
      "title                                   0.002634\n",
      "type                                    0.000000\n",
      "adult                                   0.002634\n",
      "backdrop_path                          47.102976\n",
      "created_by                             74.601001\n",
      "episode_run_time                        0.000000\n",
      "first_air_date                          0.002634\n",
      "genres                                 29.007111\n",
      "genre_ids                              29.007111\n",
      "homepage                               67.121412\n",
      "in_production                           0.000000\n",
      "languages                              26.099552\n",
      "last_air_date                           0.079010\n",
      "last_episode_to_air_id                  0.144851\n",
      "last_episode_to_air_name                0.150119\n",
      "last_episode_to_air_overview           69.109824\n",
      "last_episode_to_air_vote_average        0.144851\n",
      "last_episode_to_air_vote_count          0.144851\n",
      "last_episode_to_air_air_date            0.144851\n",
      "last_episode_to_air_episode_number      0.144851\n",
      "last_episode_to_air_production_code    99.415328\n",
      "last_episode_to_air_runtime            46.233869\n",
      "last_episode_to_air_season_number       0.144851\n",
      "last_episode_to_air_show_id             0.144851\n",
      "last_episode_to_air_still_path         80.208059\n",
      "next_episode_to_air                    99.586516\n",
      "networks                               37.263629\n",
      "number_of_episodes                      0.000000\n",
      "number_of_seasons                       0.000000\n",
      "origin_country                         10.237029\n",
      "original_language                       0.000000\n",
      "original_name                           0.002634\n",
      "overview                               43.305241\n",
      "popularity                              0.000000\n",
      "poster_path                            25.140901\n",
      "production_companies                   58.401370\n",
      "production_countries                   43.745062\n",
      "seasons                                 0.002634\n",
      "spoken_languages                       26.381354\n",
      "status                                  0.000000\n",
      "tagline                                96.597314\n",
      "type_detail                             0.000000\n",
      "vote_average                            0.000000\n",
      "vote_count                              0.000000\n",
      "review                                 98.638399\n",
      "keyword                                67.761391\n",
      "top_cast                               30.990255\n",
      "directors                              67.058204\n",
      "writers                                78.182776\n",
      "providers_flatrate                      0.000000\n",
      "providers_rent                          0.000000\n",
      "providers_buy                           0.000000\n",
      "imdb_id                                48.206479\n",
      "dtype: float64\n",
      "\n",
      "=== tv_seasons_clean.csv Í≤∞Ï∏°Î•† ===\n",
      "_id                      0.000000\n",
      "season_id                0.000000\n",
      "series_id                0.000000\n",
      "series_name              0.001168\n",
      "season_number            0.000000\n",
      "name                     0.001168\n",
      "air_date                 9.057150\n",
      "overview                91.912091\n",
      "vote_average             0.000000\n",
      "vote_count             100.000000\n",
      "network_names           24.183152\n",
      "network_ids             24.183152\n",
      "total_episodes           0.000000\n",
      "avg_episode_runtime     40.871616\n",
      "poster_path             65.509027\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "df_series = pd.read_csv(\"tv_series_2005_2015_FULL.csv\")\n",
    "df_seasons = pd.read_csv(\"tv_seasons_2005_2015_FULL.csv\")\n",
    "\n",
    "# Í≤∞Ï∏°Î•† Í≥ÑÏÇ∞ Ìï®Ïàò\n",
    "def missing_rate(df):\n",
    "    return df.isnull().mean() * 100   # ÌçºÏÑºÌä∏Î°ú ÌëúÏãú\n",
    "\n",
    "# Í∞Å ÌååÏùºÏùò Ïª¨ÎüºÎ≥Ñ Í≤∞Ï∏°Î•†\n",
    "series_missing = missing_rate(df_series)\n",
    "seasons_missing = missing_rate(df_seasons)\n",
    "\n",
    "print(\"=== tv_series_clean.csv Í≤∞Ï∏°Î•† ===\")\n",
    "print(series_missing)\n",
    "\n",
    "print(\"\\n=== tv_seasons_clean.csv Í≤∞Ï∏°Î•† ===\")\n",
    "print(seasons_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1af5e92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37970, 54)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2489d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3851\n"
     ]
    }
   ],
   "source": [
    "# Î∞©Î≤ï 1: shape[0] ÏÇ¨Ïö©\n",
    "count = df_series[(df_series['vote_count'] >= 10) & (df_series['imdb_id'].notna())].shape[0]\n",
    "print(count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
