# ==========================================================
# IMDB RATING COLLECTOR (2005-2015)
# - IMDB Rating & Rating Countë§Œ ìˆ˜ì§‘
# - ëŒ€ìƒ: 2005-01-01 ~ 2015-12-31 TV Series
# ==========================================================

import asyncio
import aiohttp
import pandas as pd
import json
from datetime import datetime
from pathlib import Path
import time
import re
import argparse

# ==========================================================
# ì„¤ì •
# ==========================================================

# Rate Limiting
MAX_CALLS_PER_SECOND = 2
TIMEOUT = aiohttp.ClientTimeout(total=30, connect=10)
MAX_RETRIES = 3

# User-Agent
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Safari/537.36"
)

# ì¶œë ¥ íŒŒì¼
OUTPUT_CSV = "imdb_ratings_2005_2015.csv"
CHECKPOINT_FILE = "imdb_rating_checkpoint.json"

# í†µê³„
stats = {
    "series_total": 0,
    "series_success": 0,
    "series_failed": 0,
    "requests": 0,
    "start_time": None
}

# ==========================================================
# Rate Limiter
# ==========================================================
class RateLimiter:
    def __init__(self, rate):
        self.rate = rate
        self.tokens = rate
        self.updated_at = time.monotonic()
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        async with self.lock:
            now = time.monotonic()
            elapsed = now - self.updated_at
            self.tokens = min(self.rate, self.tokens + elapsed * self.rate)
            self.updated_at = now
            
            if self.tokens < 1:
                sleep_time = (1 - self.tokens) / self.rate
                await asyncio.sleep(sleep_time)
                self.tokens = 1
            
            self.tokens -= 1

rate_limiter = RateLimiter(MAX_CALLS_PER_SECOND)

# ==========================================================
# HTTP í˜¸ì¶œ í•¨ìˆ˜
# ==========================================================
async def get_html(session, url, retry=0):
    """IMDB title HTMLìš©"""
    if retry >= MAX_RETRIES:
        return None
    
    await rate_limiter.acquire()
    stats["requests"] += 1

    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
    }

    try:
        async with session.get(url, headers=headers, timeout=TIMEOUT) as resp:
            if resp.status == 429 and retry < MAX_RETRIES - 1:
                wait_time = 5 * (retry + 1)
                print(f"âš ï¸  Rate limited, waiting {wait_time}s...")
                await asyncio.sleep(wait_time)
                return await get_html(session, url, retry + 1)
            
            if resp.status != 200:
                if retry < MAX_RETRIES - 1:
                    await asyncio.sleep(2 ** retry)
                    return await get_html(session, url, retry + 1)
                return None
            
            return await resp.text()
    except asyncio.TimeoutError:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await get_html(session, url, retry + 1)
        return None
    except Exception:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await get_html(session, url, retry + 1)
        return None

# ==========================================================
# IMDB Rating ì¶”ì¶œ
# ==========================================================
def parse_rating_from_html(imdb_id, html_text):
    """
    IMDB title HTMLì—ì„œ ratingValue, ratingCount ì¶”ì¶œ (JSON-LD)
    """
    imdb_rating = None
    imdb_rating_count = None

    # JSON-LD ë¸”ë¡ ì¶”ì¶œ
    ld_match = re.search(
        r'<script type="application/ld\+json">(.*?)</script>',
        html_text,
        re.S
    )
    if ld_match:
        try:
            data = json.loads(ld_match.group(1))
            agg = data.get("aggregateRating", {})
            imdb_rating = agg.get("ratingValue")
            imdb_rating_count = agg.get("ratingCount")
        except Exception as e:
            print(f"âš ï¸  JSON-LD parse error ({imdb_id}): {e}")

    return {
        "imdb_id": imdb_id,
        "imdb_rating": imdb_rating,
        "imdb_rating_count": imdb_rating_count,
    }

async def fetch_imdb_rating(session, imdb_id):
    url = f"https://www.imdb.com/title/{imdb_id}/"
    html_text = await get_html(session, url)
    if not html_text:
        print(f"âš ï¸  {imdb_id}: HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
        return {
            "imdb_id": imdb_id,
            "imdb_rating": None,
            "imdb_rating_count": None,
        }
    return parse_rating_from_html(imdb_id, html_text)

# ==========================================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ==========================================================
def save_checkpoint(processed_ids):
    checkpoint = {
        'processed_ids': list(processed_ids),
        'timestamp': datetime.now().isoformat()
    }
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(checkpoint, f)

def load_checkpoint():
    processed_ids = set()
    
    # 1) ì²´í¬í¬ì¸íŠ¸ íŒŒì¼
    if Path(CHECKPOINT_FILE).exists():
        try:
            with open(CHECKPOINT_FILE, 'r') as f:
                checkpoint = json.load(f)
                processed_ids.update(checkpoint.get('processed_ids', []))
                print(f"ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ {len(checkpoint.get('processed_ids', [])):,}ê°œ ID ë¡œë“œ")
        except (json.JSONDecodeError, Exception) as e:
            print(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì†ìƒë¨, ì‚­ì œí•˜ê³  ì§„í–‰: {e}")
            try:
                Path(CHECKPOINT_FILE).unlink()
            except:
                pass
    
    # 2) ê¸°ì¡´ CSV ê¸°ì¤€
    if Path(OUTPUT_CSV).exists():
        try:
            df_existing = pd.read_csv(OUTPUT_CSV)
            if 'imdb_id' in df_existing.columns:
                existing_ids = df_existing['imdb_id'].unique()
                processed_ids.update(existing_ids)
                print(f"ğŸ“Œ ê¸°ì¡´ CSVì—ì„œ {len(existing_ids):,}ê°œ ì‹œë¦¬ì¦ˆ ë°œê²¬")
        except Exception as e:
            print(f"âš ï¸  ê¸°ì¡´ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return processed_ids

# ==========================================================
# ë‚ ì§œ í•„í„°ë§
# ==========================================================
def filter_by_date_range(df, start_date='2005-01-01', end_date='2015-12-31'):
    """
    first_air_date ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ í•„í„°ë§
    """
    if 'first_air_date' not in df.columns:
        print("âš ï¸  'first_air_date' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œ í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df
    
    df['first_air_date'] = pd.to_datetime(df['first_air_date'], errors='coerce')
    start = pd.to_datetime(start_date)
    end = pd.to_datetime(end_date)
    
    df_filtered = df[(df['first_air_date'] >= start) & (df['first_air_date'] <= end)]
    
    print(f"ğŸ“… ë‚ ì§œ í•„í„°ë§: {start_date} ~ {end_date}")
    print(f"   ì›ë³¸: {len(df):,}ê°œ â†’ í•„í„°ë§ í›„: {len(df_filtered):,}ê°œ")
    
    return df_filtered

# ==========================================================
# ë©”ì¸ ì‹¤í–‰
# ==========================================================
async def main(input_csv_path, vote_threshold=30):
    print("=" * 90)
    print("ğŸš€ IMDB RATING COLLECTOR (2005-2015)")
    print("=" * 90)
    
    stats["start_time"] = datetime.now()
    t0 = datetime.now()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv(input_csv_path)
    
    # ë‚ ì§œ í•„í„°ë§ (2005-2015)
    df = filter_by_date_range(df, '2005-01-01', '2015-12-31')
    
    # vote_count í•„í„°ë§
    df_filtered = df[(df['vote_count'] >= vote_threshold) & (df['imdb_id'].notna())]
    df_filtered = df_filtered.drop_duplicates(subset=['imdb_id'])
    
    print(f"âœ… ìµœì¢… í•„í„°ë§ (vote_count>={vote_threshold} & imdb_id ì¡´ì¬): {len(df_filtered):,}ê°œ")
    
    if len(df_filtered) == 0:
        print("âš ï¸  ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    processed_ids = load_checkpoint()
    series_list = df_filtered[['imdb_id']].to_dict('records')
    
    if processed_ids:
        print(f"ğŸ“Œ ì´ë¯¸ ì²˜ë¦¬ëœ ì‹œë¦¬ì¦ˆ: {len(processed_ids):,}ê°œ")
        series_list = [s for s in series_list if s['imdb_id'] not in processed_ids]
        print(f"ğŸ“Œ ë‚¨ì€ ì‘ì—…: {len(series_list):,}ê°œ")
    
    if len(series_list) == 0:
        print("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    stats["series_total"] = len(series_list)
    
    # 3. í¬ë¡¤ë§ ì„¤ì •
    print(f"\nğŸš€ í¬ë¡¤ë§ ì‹œì‘")
    print(f"âš™ï¸  Rate Limit: {MAX_CALLS_PER_SECOND}íšŒ/ì´ˆ")
    
    estimated_time = len(series_list) / MAX_CALLS_PER_SECOND / 60
    print(f"â±ï¸  ì˜ˆìƒ ì‹œê°„: {estimated_time:.0f}ë¶„")
    
    connector = aiohttp.TCPConnector(
        limit=20,
        force_close=False,
        enable_cleanup_closed=True
    )
    
    all_results = []
    batch_size = 10
    
    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as session:
        for i in range(0, len(series_list), batch_size):
            batch = series_list[i:i+batch_size]
            
            tasks = [fetch_imdb_rating(session, s['imdb_id']) for s in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in batch_results:
                if isinstance(result, Exception):
                    stats["series_failed"] += 1
                    continue
                
                if isinstance(result, dict):
                    all_results.append(result)
                    processed_ids.add(result['imdb_id'])
                    stats["series_success"] += 1
                else:
                    stats["series_failed"] += 1
            
            # ì£¼ê¸°ì  ì €ì¥
            if (i + batch_size) % 50 == 0:
                save_checkpoint(processed_ids)
                
                if all_results:
                    df_batch = pd.DataFrame(all_results)
                    df_batch = df_batch.drop_duplicates(subset=['imdb_id'])
                    file_exists = Path(OUTPUT_CSV).exists()
                    df_batch.to_csv(
                        OUTPUT_CSV,
                        mode='a',
                        header=not file_exists,
                        index=False,
                        encoding='utf-8-sig'
                    )
                    all_results.clear()
                    print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ ({len(df_batch):,}ê°œ)")
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            elapsed = (datetime.now() - t0).total_seconds() / 60
            progress = stats["series_success"] + stats["series_failed"]
            rate = progress / elapsed if elapsed > 0 else 0
            eta = (stats["series_total"] - progress) / rate if rate > 0 else 0
            
            print(
                f"ğŸ“Š ì§„í–‰: {progress}/{stats['series_total']} "
                f"({progress/stats['series_total']*100:.1f}%) | "
                f"ì„±ê³µ: {stats['series_success']} | ì‹¤íŒ¨: {stats['series_failed']} | "
                f"ìš”ì²­: {stats['requests']:,}íšŒ | "
                f"ì†ë„: {rate:.1f}ê°œ/ë¶„ | ETA: {eta:.0f}ë¶„"
            )
    
    # 4. ìµœì¢… ì €ì¥
    print("\nğŸ’¾ ìµœì¢… ì €ì¥ ì¤‘...")

    if all_results:
        df_batch = pd.DataFrame(all_results)
        df_batch = df_batch.drop_duplicates(subset=['imdb_id'])
        file_exists = Path(OUTPUT_CSV).exists()
        df_batch.to_csv(
            OUTPUT_CSV,
            mode='a',
            header=not file_exists,
            index=False,
            encoding='utf-8-sig'
        )

    # ì „ì²´ ì¤‘ë³µ ì œê±°
    if Path(OUTPUT_CSV).exists():
        df_results = pd.read_csv(OUTPUT_CSV)
        df_results = df_results.drop_duplicates(subset=['imdb_id'])
        df_results.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    else:
        df_results = pd.DataFrame()

    # ì²´í¬í¬ì¸íŠ¸ ì œê±°
    if Path(CHECKPOINT_FILE).exists():
        Path(CHECKPOINT_FILE).unlink()
    
    # 5. ìµœì¢… í†µê³„ ì¶œë ¥
    elapsed = (datetime.now() - t0).total_seconds() / 60
    
    print("\n" + "=" * 90)
    print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 90)
    print(f"ğŸ“Œ ì‹œë¦¬ì¦ˆ: {stats['series_success']:,}/{stats['series_total']:,}ê°œ ì„±ê³µ")
    
    if not df_results.empty:
        print(f"ğŸ“Œ ì´ ìˆ˜ì§‘: {len(df_results):,}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
        
        # ratingì´ ìˆëŠ” ë°ì´í„° í†µê³„
        has_rating = df_results['imdb_rating'].notna().sum()
        print(f"ğŸ“Œ Rating ë³´ìœ : {has_rating:,}ê°œ ({has_rating/len(df_results)*100:.1f}%)")
        
        if has_rating > 0:
            print(f"ğŸ“Œ í‰ê·  Rating: {df_results['imdb_rating'].mean():.2f}")
            print(f"ğŸ“Œ í‰ê·  Rating Count: {df_results['imdb_rating_count'].mean():.0f}")
    else:
        print("ğŸ“Œ ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")
    
    print(f"ğŸ“Œ ì´ ìš”ì²­: {stats['requests']:,}íšŒ")
    print(f"â±ï¸  ì´ ì‹œê°„: {elapsed:.1f}ë¶„ ({elapsed/60:.2f}ì‹œê°„)")
    
    if stats['series_success'] > 0 and elapsed > 0:
        print(f"ğŸ“Š ì†ë„: {stats['series_success']/elapsed:.1f}ê°œ/ë¶„")
    
    print("=" * 90)
    
    # ìƒ˜í”Œ ì¶œë ¥
    if not df_results.empty:
        print("\nğŸ“Š ê²°ê³¼ ìƒ˜í”Œ:")
        print(df_results.head(10).to_string())
        print(f"\nâœ… ê²°ê³¼ íŒŒì¼: {OUTPUT_CSV}")

# ==========================================================
# ì‹¤í–‰
# ==========================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='IMDB RATING COLLECTOR (2005-2015)')
    parser.add_argument('--input', '-i', default='TMDB_tv_series_2005_2015_FULL.csv',
                        help='ì…ë ¥ CSV íŒŒì¼ (ê¸°ë³¸: TMDB_tv_series_2005_2015_FULL.csv)')
    parser.add_argument('--vote', '-v', type=int, default=30,
                        help='ìµœì†Œ vote_count (ê¸°ë³¸: 30)')
    
    args = parser.parse_args()
    
    if not Path(args.input).exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
    else:
        asyncio.run(main(args.input, args.vote))
