{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913325e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# TMDB TV SERIES + SEASONS HYPER-OPTIMIZED ASYNC COLLECTOR\n",
    "# ë¹„ë™ê¸° ë°©ì‹ + ë³‘ëª© ì œê±° + ìºì‹±\n",
    "# âœ¨ ìµœì¢… ê°œì„ : providersë¥¼ flatrate/rent/buyë¡œ ë¶„ë¦¬ + imdb_id ì¶”ê°€\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "# ==========================================================\n",
    "# Jupyter async ì§€ì›\n",
    "# ==========================================================\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"âœ… nest_asyncio ì ìš© ì™„ë£Œ\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ==========================================================\n",
    "# ì„¤ì •\n",
    "# ==========================================================\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API_KEY missing\")\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "\n",
    "# ğŸ”¥ í•µì‹¬: TMDB ì‹¤ì œ ì œí•œì€ 50/ì´ˆì´ì§€ë§Œ, ë²„ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•´ 40ìœ¼ë¡œ ì•ˆì •í™”\n",
    "MAX_CALLS_PER_SECOND = 25\n",
    "TIMEOUT = aiohttp.ClientTimeout(total=15, connect=5, sock_read=10)\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "START_DATE = \"2005-01-01\"\n",
    "END_DATE = \"2015-12-31\"\n",
    "\n",
    "SERIES_TEMP = Path(\"tv_series_temp.csv\")\n",
    "SEASONS_TEMP = Path(\"tv_seasons_temp.csv\")\n",
    "\n",
    "SERIES_CSV = \"tv_series_2005_2015_FULL.csv\"\n",
    "SEASONS_CSV = \"tv_seasons_2005_2015_FULL.csv\"\n",
    "SERIES_PARQ = \"tv_series_2005_2015_FULL.parquet\"\n",
    "SEASONS_PARQ = \"tv_seasons_2005_2015_FULL.parquet\"\n",
    "\n",
    "# í†µê³„\n",
    "stats = {\"requests\": 0, \"errors\": 0, \"retries\": 0, \"start_time\": None}\n",
    "\n",
    "# ==========================================================\n",
    "# ê³ ì„±ëŠ¥ Rate Limiter (Lock-free ë²„ì „)\n",
    "# ==========================================================\n",
    "class LockFreeRateLimiter:\n",
    "    \"\"\"Lock ì—†ì´ ë™ì‘í•˜ëŠ” ì´ˆê³ ì† Rate Limiter\"\"\"\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.tokens = rate\n",
    "        self.updated_at = time.monotonic()\n",
    "    \n",
    "    async def acquire(self):\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self.updated_at\n",
    "        \n",
    "        # í† í° ë³´ì¶© (Lock ì—†ì´)\n",
    "        self.tokens = min(self.rate, self.tokens + elapsed * self.rate)\n",
    "        self.updated_at = now\n",
    "        \n",
    "        # í† í° ë¶€ì¡±ì‹œ ìµœì†Œ ëŒ€ê¸°\n",
    "        if self.tokens < 1:\n",
    "            sleep_time = (1 - self.tokens) / self.rate\n",
    "            await asyncio.sleep(sleep_time)\n",
    "            self.tokens = 1\n",
    "        \n",
    "        self.tokens -= 1\n",
    "\n",
    "rate_limiter = LockFreeRateLimiter(MAX_CALLS_PER_SECOND)\n",
    "\n",
    "# ==========================================================\n",
    "# TMDB GET (ì¬ì‹œë„ ë¡œì§ ê°œì„ )\n",
    "# ==========================================================\n",
    "async def tmdb_get(session, path, params=None, retry=0):\n",
    "    if retry >= MAX_RETRIES:\n",
    "        stats[\"errors\"] += 1\n",
    "        return None\n",
    "\n",
    "    params = params or {}\n",
    "    params.setdefault(\"api_key\", API_KEY)\n",
    "    params.setdefault(\"language\", \"en-US\")\n",
    "\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "    \n",
    "    await rate_limiter.acquire()\n",
    "    stats[\"requests\"] += 1\n",
    "\n",
    "    try:\n",
    "        async with session.get(url, params=params, headers=HEADERS) as resp:\n",
    "            # 429: Rate limit\n",
    "            if resp.status == 429:\n",
    "                stats[\"retries\"] += 1\n",
    "                retry_after = int(resp.headers.get(\"Retry-After\", 2))\n",
    "                await asyncio.sleep(retry_after)\n",
    "                return await tmdb_get(session, path, params, retry + 1)\n",
    "            \n",
    "            # 404: Not found (ì •ìƒ ì¼€ì´ìŠ¤)\n",
    "            if resp.status == 404:\n",
    "                return None\n",
    "            \n",
    "            # 5xx: ì„œë²„ ì—ëŸ¬\n",
    "            if 500 <= resp.status < 600:\n",
    "                stats[\"retries\"] += 1\n",
    "                await asyncio.sleep(0.5 ** retry)  # ì§€ìˆ˜ ê°ì†Œ\n",
    "                return await tmdb_get(session, path, params, retry + 1)\n",
    "            \n",
    "            resp.raise_for_status()\n",
    "            return await resp.json()\n",
    "    \n",
    "    except asyncio.TimeoutError:\n",
    "        stats[\"errors\"] += 1\n",
    "        if retry < MAX_RETRIES - 1:\n",
    "            await asyncio.sleep(0.2)\n",
    "            return await tmdb_get(session, path, params, retry + 1)\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        stats[\"errors\"] += 1\n",
    "        if retry < MAX_RETRIES - 1:\n",
    "            await asyncio.sleep(0.1)\n",
    "            return await tmdb_get(session, path, params, retry + 1)\n",
    "        return None\n",
    "\n",
    "# ==========================================================\n",
    "# Discover\n",
    "# ==========================================================\n",
    "async def fetch_discover_page(session, page, gte, lte):\n",
    "    params = {\n",
    "        \"sort_by\": \"popularity.desc\",\n",
    "        \"first_air_date.gte\": gte,\n",
    "        \"first_air_date.lte\": lte,\n",
    "        \"page\": page,\n",
    "        \"include_adult\": \"true\",\n",
    "    }\n",
    "    data = await tmdb_get(session, \"/discover/tv\", params)\n",
    "    if not data:\n",
    "        return [], 1, 0\n",
    "    return data.get(\"results\", []), data.get(\"total_pages\", 1), data.get(\"total_results\", 0)\n",
    "\n",
    "# ==========================================================\n",
    "# ID ìˆ˜ì§‘ (ê°œì„ )\n",
    "# ==========================================================\n",
    "async def collect_ids_in_range(session, start, end, depth=0):\n",
    "    res1, total_pages, total_results = await fetch_discover_page(session, 1, start, end)\n",
    "\n",
    "    if total_results == 0:\n",
    "        return set()\n",
    "\n",
    "    if total_pages <= 500:\n",
    "        ids = {r[\"id\"] for r in res1}\n",
    "        \n",
    "        if total_pages > 1:\n",
    "            # ğŸ”¥ ê°œì„ : ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨ ê°œì„ \n",
    "            chunk_size = 100\n",
    "            for chunk_start in range(2, total_pages + 1, chunk_size):\n",
    "                chunk_end = min(chunk_start + chunk_size, total_pages + 1)\n",
    "                results = await asyncio.gather(\n",
    "                    *[fetch_discover_page(session, p, start, end) for p in range(chunk_start, chunk_end)],\n",
    "                    return_exceptions=True\n",
    "                )\n",
    "                for r in results:\n",
    "                    if isinstance(r, tuple):\n",
    "                        ids.update(x[\"id\"] for x in r[0])\n",
    "        \n",
    "        return ids\n",
    "\n",
    "    # ë‚ ì§œ ë¶„í• \n",
    "    start_dt = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    mid_dt = start_dt + (end_dt - start_dt) / 2\n",
    "    mid = mid_dt.strftime(\"%Y-%m-%d\")\n",
    "    right_start = (mid_dt + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    left, right = await asyncio.gather(\n",
    "        collect_ids_in_range(session, start, mid, depth + 1),\n",
    "        collect_ids_in_range(session, right_start, end, depth + 1)\n",
    "    )\n",
    "    return left | right\n",
    "\n",
    "# ==========================================================\n",
    "# Helpers (ìºì‹± ì¶”ê°€)\n",
    "# ==========================================================\n",
    "@lru_cache(maxsize=1024)\n",
    "def list_to_str_cached(lst_tuple, key=\"name\"):\n",
    "    \"\"\"ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ìºì‹±)\"\"\"\n",
    "    if not lst_tuple:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key, \"\")) for i in lst_tuple if i.get(key))\n",
    "\n",
    "def list_to_str(lst, key=\"name\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    # ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ íŠœí”Œë¡œ ë³€í™˜í•˜ì—¬ ìºì‹± ê°€ëŠ¥í•˜ê²Œ\n",
    "    try:\n",
    "        lst_tuple = tuple(tuple(d.items()) if isinstance(d, dict) else d for d in lst)\n",
    "        return list_to_str_cached(lst_tuple, key)\n",
    "    except:\n",
    "        return \", \".join(str(i.get(key, \"\")) for i in lst if i.get(key))\n",
    "\n",
    "def list_ids_to_str(lst, key=\"id\"):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join(str(i.get(key)) for i in lst if i.get(key))\n",
    "\n",
    "# ==========================================================\n",
    "# âœ¨ NEW: External IDs (IMDB ID)\n",
    "# ==========================================================\n",
    "async def fetch_external_ids(session, sid):\n",
    "    \"\"\"ì‹œë¦¬ì¦ˆì˜ external IDsë¥¼ ê°€ì ¸ì™€ì„œ IMDB ID ë°˜í™˜\"\"\"\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/external_ids\")\n",
    "    if not data:\n",
    "        return \"\"\n",
    "    return data.get(\"imdb_id\", \"\")\n",
    "\n",
    "# ==========================================================\n",
    "# âœ¨ UPDATED: Providers (flatrate/rent/buy ë¶„ë¦¬)\n",
    "# ==========================================================\n",
    "async def fetch_providers(session, sid):\n",
    "    \"\"\"\n",
    "    ëª¨ë“  êµ­ê°€ì˜ streaming providers ìˆ˜ì§‘\n",
    "    flatrate(êµ¬ë…í˜•), rent(ëŒ€ì—¬), buy(êµ¬ë§¤) íƒ€ì…ë³„ë¡œ ë¶„ë¦¬\n",
    "    ë°˜í™˜: (flatrate_dict, rent_dict, buy_dict)\n",
    "    \"\"\"\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/watch/providers\")\n",
    "    if not data:\n",
    "        return \"{}\", \"{}\", \"{}\"\n",
    "    \n",
    "    providers_results = data.get(\"results\", {})\n",
    "    \n",
    "    # êµ­ê°€ë³„ OTT ì •ë³´ ìˆ˜ì§‘\n",
    "    providers_flatrate = {}\n",
    "    providers_rent = {}\n",
    "    providers_buy = {}\n",
    "    \n",
    "    for country, info in providers_results.items():\n",
    "        # flatrate: êµ¬ë…í˜• (Netflix, Disney+ ë“±)\n",
    "        flatrate = info.get(\"flatrate\", [])\n",
    "        if flatrate:\n",
    "            provider_names = [p.get(\"provider_name\") for p in flatrate if p.get(\"provider_name\")]\n",
    "            if provider_names:\n",
    "                providers_flatrate[country] = provider_names\n",
    "        \n",
    "        # rent: ëŒ€ì—¬\n",
    "        rent = info.get(\"rent\", [])\n",
    "        if rent:\n",
    "            provider_names = [p.get(\"provider_name\") for p in rent if p.get(\"provider_name\")]\n",
    "            if provider_names:\n",
    "                providers_rent[country] = provider_names\n",
    "        \n",
    "        # buy: êµ¬ë§¤\n",
    "        buy = info.get(\"buy\", [])\n",
    "        if buy:\n",
    "            provider_names = [p.get(\"provider_name\") for p in buy if p.get(\"provider_name\")]\n",
    "            if provider_names:\n",
    "                providers_buy[country] = provider_names\n",
    "    \n",
    "    # JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜\n",
    "    return (\n",
    "        json.dumps(providers_flatrate, ensure_ascii=False) if providers_flatrate else \"{}\",\n",
    "        json.dumps(providers_rent, ensure_ascii=False) if providers_rent else \"{}\",\n",
    "        json.dumps(providers_buy, ensure_ascii=False) if providers_buy else \"{}\"\n",
    "    )\n",
    "\n",
    "# ==========================================================\n",
    "# Single Season\n",
    "# ==========================================================\n",
    "async def fetch_single_season(session, sid, sname, net_names, net_ids, meta):\n",
    "    sn = meta.get(\"season_number\")\n",
    "    if sn is None:\n",
    "        return None\n",
    "\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}/season/{sn}\")\n",
    "\n",
    "    if not data:\n",
    "        return {\n",
    "            \"_id\": f\"{sid}_{sn}\",\n",
    "            \"season_id\": meta.get(\"id\"),\n",
    "            \"series_id\": sid,\n",
    "            \"series_name\": sname,\n",
    "            \"season_number\": sn,\n",
    "            \"name\": meta.get(\"name\"),\n",
    "            \"air_date\": meta.get(\"air_date\"),\n",
    "            \"overview\": meta.get(\"overview\"),\n",
    "            \"vote_average\": meta.get(\"vote_average\"),\n",
    "            \"vote_count\": meta.get(\"vote_count\"),\n",
    "            \"network_names\": net_names,\n",
    "            \"network_ids\": net_ids,\n",
    "            \"total_episodes\": meta.get(\"episode_count\"),\n",
    "            \"avg_episode_runtime\": None,\n",
    "            \"poster_path\": meta.get(\"poster_path\"),\n",
    "        }\n",
    "\n",
    "    eps = data.get(\"episodes\") or []\n",
    "    runtimes = [ep[\"runtime\"] for ep in eps if ep.get(\"runtime\")]\n",
    "    avg_rt = sum(runtimes) / len(runtimes) if runtimes else None\n",
    "\n",
    "    return {\n",
    "        \"_id\": data.get(\"_id\") or f\"{sid}_{sn}\",\n",
    "        \"season_id\": data.get(\"id\"),\n",
    "        \"series_id\": sid,\n",
    "        \"series_name\": sname,\n",
    "        \"season_number\": data.get(\"season_number\"),\n",
    "        \"name\": data.get(\"name\"),\n",
    "        \"air_date\": data.get(\"air_date\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "        \"network_names\": net_names,\n",
    "        \"network_ids\": net_ids,\n",
    "        \"total_episodes\": len(eps),\n",
    "        \"avg_episode_runtime\": avg_rt,\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# âœ¨ UPDATED: TV Details + Seasons + IMDB ID + Providers\n",
    "# ==========================================================\n",
    "async def fetch_tv_details_and_seasons(session, sid):\n",
    "    params = {\"append_to_response\": \"reviews,keywords,aggregate_credits\"}\n",
    "    data = await tmdb_get(session, f\"/tv/{sid}\", params)\n",
    "    if not data:\n",
    "        return None, []\n",
    "\n",
    "    genres = data.get(\"genres\") or []\n",
    "    networks = data.get(\"networks\") or []\n",
    "    last_ep = data.get(\"last_episode_to_air\") or {}\n",
    "\n",
    "    net_names = list_to_str(networks)\n",
    "    net_ids = list_ids_to_str(networks)\n",
    "\n",
    "    series = {\n",
    "        \"id\": data.get(\"id\"),\n",
    "        \"title\": data.get(\"name\"),\n",
    "        \"type\": \"tv_series\",\n",
    "        \"adult\": data.get(\"adult\"),\n",
    "        \"backdrop_path\": data.get(\"backdrop_path\"),\n",
    "        \"created_by\": list_to_str(data.get(\"created_by\")),\n",
    "        \"episode_run_time\": \", \".join(map(str, data.get(\"episode_run_time\", []))) or \"\",\n",
    "        \"first_air_date\": data.get(\"first_air_date\"),\n",
    "        \"genres\": list_to_str(genres),\n",
    "        \"genre_ids\": \", \".join(str(g[\"id\"]) for g in genres if g.get(\"id\")),\n",
    "        \"homepage\": data.get(\"homepage\"),\n",
    "        \"in_production\": data.get(\"in_production\"),\n",
    "        \"languages\": \", \".join(data.get(\"languages\", [])) or \"\",\n",
    "        \"last_air_date\": data.get(\"last_air_date\"),\n",
    "        \"last_episode_to_air_id\": last_ep.get(\"id\"),\n",
    "        \"last_episode_to_air_name\": last_ep.get(\"name\"),\n",
    "        \"last_episode_to_air_overview\": last_ep.get(\"overview\"),\n",
    "        \"last_episode_to_air_vote_average\": last_ep.get(\"vote_average\"),\n",
    "        \"last_episode_to_air_vote_count\": last_ep.get(\"vote_count\"),\n",
    "        \"last_episode_to_air_air_date\": last_ep.get(\"air_date\"),\n",
    "        \"last_episode_to_air_episode_number\": last_ep.get(\"episode_number\"),\n",
    "        \"last_episode_to_air_production_code\": last_ep.get(\"production_code\"),\n",
    "        \"last_episode_to_air_runtime\": last_ep.get(\"runtime\"),\n",
    "        \"last_episode_to_air_season_number\": last_ep.get(\"season_number\"),\n",
    "        \"last_episode_to_air_show_id\": last_ep.get(\"show_id\"),\n",
    "        \"last_episode_to_air_still_path\": last_ep.get(\"still_path\"),\n",
    "        \"next_episode_to_air\": str(data.get(\"next_episode_to_air\")) if data.get(\"next_episode_to_air\") else \"\",\n",
    "        \"networks\": net_names,\n",
    "        \"number_of_episodes\": data.get(\"number_of_episodes\"),\n",
    "        \"number_of_seasons\": data.get(\"number_of_seasons\"),\n",
    "        \"origin_country\": \", \".join(data.get(\"origin_country\", [])) or \"\",\n",
    "        \"original_language\": data.get(\"original_language\"),\n",
    "        \"original_name\": data.get(\"original_name\"),\n",
    "        \"overview\": data.get(\"overview\"),\n",
    "        \"popularity\": data.get(\"popularity\"),\n",
    "        \"poster_path\": data.get(\"poster_path\"),\n",
    "        \"production_companies\": list_to_str(data.get(\"production_companies\")),\n",
    "        \"production_countries\": list_to_str(data.get(\"production_countries\")),\n",
    "        \"seasons\": \"; \".join(\n",
    "            f\"S{s['season_number']}: {s.get('name')} ({s.get('episode_count')} eps)\"\n",
    "            for s in (data.get(\"seasons\") or []) if s.get(\"season_number\") is not None\n",
    "        ),\n",
    "        \"spoken_languages\": list_to_str(data.get(\"spoken_languages\")),\n",
    "        \"status\": data.get(\"status\"),\n",
    "        \"tagline\": data.get(\"tagline\"),\n",
    "        \"type_detail\": data.get(\"type\"),\n",
    "        \"vote_average\": data.get(\"vote_average\"),\n",
    "        \"vote_count\": data.get(\"vote_count\"),\n",
    "    }\n",
    "\n",
    "    # ë¦¬ë·°\n",
    "    rv_items = data.get(\"reviews\", {}).get(\"results\", [])\n",
    "    series[\"review\"] = \" || \".join([\n",
    "        f\"{r.get('author', '')}({r.get('author_details', {}).get('rating', '')}): {(r.get('content') or '').replace(chr(10), ' ')[:200]}\"\n",
    "        for r in rv_items[:5]\n",
    "    ])\n",
    "\n",
    "    # í‚¤ì›Œë“œ\n",
    "    kw_items = data.get(\"keywords\", {}).get(\"results\", [])\n",
    "    series[\"keyword\"] = \", \".join([k[\"name\"] for k in kw_items[:20] if k.get(\"name\")])\n",
    "\n",
    "    # Cast & Crew\n",
    "    credits = data.get(\"aggregate_credits\", {})\n",
    "    cast = credits.get(\"cast\") or []\n",
    "    crew = credits.get(\"crew\") or []\n",
    "\n",
    "    series[\"top_cast\"] = \", \".join([c[\"name\"] for c in cast[:4] if c.get(\"name\")])\n",
    "\n",
    "    dirs, wrs = set(), set()\n",
    "    for c in crew:\n",
    "        nm = c.get(\"name\")\n",
    "        if nm:\n",
    "            for job in (c.get(\"jobs\") or []):\n",
    "                jn = job.get(\"job\", \"\")\n",
    "                if \"Director\" in jn:\n",
    "                    dirs.add(nm)\n",
    "                if jn in [\"Writer\", \"Screenplay\", \"Story\"]:\n",
    "                    wrs.add(nm)\n",
    "\n",
    "    series[\"directors\"] = \", \".join(sorted(dirs)[:10])\n",
    "    series[\"writers\"] = \", \".join(sorted(wrs)[:10])\n",
    "\n",
    "    # ğŸ”¥ providers + external_ids + seasons ë³‘ë ¬ ìˆ˜ì§‘\n",
    "    seasons_meta = [s for s in (data.get(\"seasons\") or []) if s.get(\"season_number\") is not None]\n",
    "    \n",
    "    tasks = [\n",
    "        fetch_providers(session, sid),      # 0: (flatrate, rent, buy)\n",
    "        fetch_external_ids(session, sid),   # 1: imdb_id\n",
    "    ]\n",
    "    tasks.extend([\n",
    "        fetch_single_season(session, sid, data.get(\"name\", \"\"), net_names, net_ids, s)\n",
    "        for s in seasons_meta\n",
    "    ])\n",
    "    \n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Providers ì²˜ë¦¬ (flatrate, rent, buy)\n",
    "    if isinstance(results[0], tuple) and len(results[0]) == 3:\n",
    "        series[\"providers_flatrate\"] = results[0][0]\n",
    "        series[\"providers_rent\"] = results[0][1]\n",
    "        series[\"providers_buy\"] = results[0][2]\n",
    "    else:\n",
    "        series[\"providers_flatrate\"] = \"{}\"\n",
    "        series[\"providers_rent\"] = \"{}\"\n",
    "        series[\"providers_buy\"] = \"{}\"\n",
    "    \n",
    "    # IMDB ID ì²˜ë¦¬\n",
    "    series[\"imdb_id\"] = results[1] if isinstance(results[1], str) else \"\"\n",
    "    \n",
    "    # Seasons ì²˜ë¦¬\n",
    "    season_records = [r for r in results[2:] if r and not isinstance(r, Exception)]\n",
    "\n",
    "    return series, season_records\n",
    "\n",
    "# ==========================================================\n",
    "# ë¹„ë™ê¸° CSV ì“°ê¸°\n",
    "# ==========================================================\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "async def save_to_csv_async(df, path, mode, header):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    await loop.run_in_executor(\n",
    "        executor,\n",
    "        lambda: df.to_csv(path, mode=mode, header=header, index=False, encoding=\"utf-8-sig\")\n",
    "    )\n",
    "\n",
    "# ==========================================================\n",
    "# ê²°ì¸¡ì¹˜ ë³´ì™„\n",
    "# ==========================================================\n",
    "def fill_series_gaps(series_df, seasons_df):\n",
    "    print(\"\\nğŸ“Š ì‹œì¦Œ ê¸°ë°˜ ê²°ì¸¡ì¹˜ ë³´ì™„\")\n",
    "\n",
    "    rtmap = seasons_df.groupby(\"series_id\")[\"avg_episode_runtime\"].mean().round()\n",
    "    mask = series_df[\"episode_run_time\"].astype(str).str.strip().isin([\"\", \"nan\"])\n",
    "    series_df.loc[mask, \"episode_run_time\"] = (\n",
    "        series_df.loc[mask, \"id\"].map(rtmap).fillna(0).astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    epmap = seasons_df.groupby(\"series_id\")[\"total_episodes\"].sum()\n",
    "    mask2 = series_df[\"number_of_episodes\"].isna()\n",
    "    series_df.loc[mask2, \"number_of_episodes\"] = series_df.loc[mask2, \"id\"].map(epmap)\n",
    "\n",
    "    seasmap = seasons_df.groupby(\"series_id\")[\"season_number\"].nunique()\n",
    "    mask3 = series_df[\"number_of_seasons\"].isna()\n",
    "    series_df.loc[mask3, \"number_of_seasons\"] = series_df.loc[mask3, \"id\"].map(seasmap)\n",
    "\n",
    "    lastair = seasons_df.sort_values(\"air_date\").groupby(\"series_id\")[\"air_date\"].last()\n",
    "    ladmask = series_df[\"last_air_date\"].astype(str).str.strip().isin([\"\", \"nan\", \"NaT\"])\n",
    "    series_df.loc[ladmask, \"last_air_date\"] = series_df.loc[ladmask, \"id\"].map(lastair)\n",
    "\n",
    "    print(\"âœ… ì™„ë£Œ\\n\")\n",
    "    return series_df\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN\n",
    "# ==========================================================\n",
    "async def main():\n",
    "    print(\"=\" * 90)\n",
    "    print(\"ğŸš€ TMDB HYPER-OPTIMIZED ASYNC COLLECTOR\")\n",
    "    print(\"âœ¨ ìµœì¢… ê°œì„ : providers(flatrate/rent/buy ë¶„ë¦¬) + IMDB ID ì¶”ê°€\")\n",
    "    print(\"=\" * 90)\n",
    "    stats[\"start_time\"] = datetime.now()\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    # ğŸ”¥ ìµœì  ì„¤ì •\n",
    "    connector = aiohttp.TCPConnector(\n",
    "        limit=150,\n",
    "        limit_per_host=75,\n",
    "        ttl_dns_cache=600,\n",
    "        force_close=False,\n",
    "        enable_cleanup_closed=True\n",
    "    )\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as session:\n",
    "        # 1) ID ìˆ˜ì§‘\n",
    "        print(\"\\nğŸ“Œ 1ë‹¨ê³„: ID ìˆ˜ì§‘\")\n",
    "        ids = sorted(list(await collect_ids_in_range(session, START_DATE, END_DATE)))\n",
    "        print(f\"âœ¨ ì´ ID: {len(ids):,}ê°œ\")\n",
    "\n",
    "        # 2) ìƒì„¸ ìˆ˜ì§‘\n",
    "        print(\"\\nğŸ“Œ 2ë‹¨ê³„: ìƒì„¸ + ì‹œì¦Œ + IMDB ID + Providers ìˆ˜ì§‘\")\n",
    "        batch_size = 1500\n",
    "        sem = asyncio.Semaphore(30)  # ğŸ”¥ ì•ˆì •ì ì¸ ë™ì‹œì„±\n",
    "\n",
    "        async def fetch_one(i):\n",
    "            async with sem:\n",
    "                return await fetch_tv_details_and_seasons(session, i)\n",
    "\n",
    "        processed = 0\n",
    "        last_print = time.time()\n",
    "        \n",
    "        for start_idx in range(0, len(ids), batch_size):\n",
    "            batch_start = time.time()\n",
    "            part = ids[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            results = await asyncio.gather(*[fetch_one(x) for x in part], return_exceptions=True)\n",
    "            \n",
    "            bs, bse = [], []\n",
    "            for r in results:\n",
    "                if isinstance(r, tuple) and r[0]:\n",
    "                    s, ss = r\n",
    "                    bs.append(s)\n",
    "                    bse.extend(ss)\n",
    "                processed += 1\n",
    "\n",
    "            # CSV ì“°ê¸°\n",
    "            if bs:\n",
    "                await save_to_csv_async(pd.DataFrame(bs), SERIES_TEMP, \"a\", not SERIES_TEMP.exists())\n",
    "            if bse:\n",
    "                await save_to_csv_async(pd.DataFrame(bse), SEASONS_TEMP, \"a\", not SEASONS_TEMP.exists())\n",
    "\n",
    "            # ì§„í–‰ ìƒí™© (5ì´ˆë§ˆë‹¤)\n",
    "            now = time.time()\n",
    "            if now - last_print >= 5:\n",
    "                elapsed = (datetime.now() - t0).total_seconds()\n",
    "                rate = processed / elapsed if elapsed > 0 else 0\n",
    "                eta = (len(ids) - processed) / rate / 60 if rate > 0 else 0\n",
    "                batch_time = now - batch_start\n",
    "                print(f\"â± {processed:,}/{len(ids):,} ({processed/len(ids)*100:.1f}%) | \"\n",
    "                      f\"{rate:.1f}/s | ETA: {eta:.0f}ë¶„ | Batch: {batch_time:.1f}s | \"\n",
    "                      f\"ìš”ì²­: {stats['requests']:,} | ì—ëŸ¬: {stats['errors']} | ì¬ì‹œë„: {stats['retries']}\")\n",
    "                last_print = now\n",
    "\n",
    "    # 3) ìµœì¢… ì²˜ë¦¬\n",
    "    print(\"\\nğŸ“Œ 3ë‹¨ê³„: ìµœì¢… ì²˜ë¦¬\")\n",
    "    df_series = pd.read_csv(SERIES_TEMP)\n",
    "    df_seasons = pd.read_csv(SEASONS_TEMP) if SEASONS_TEMP.exists() else pd.DataFrame()\n",
    "\n",
    "    SERIES_COLS = [\n",
    "        \"id\",\"title\",\"type\",\"adult\",\"backdrop_path\",\"created_by\",\"episode_run_time\",\n",
    "        \"first_air_date\",\"genres\",\"genre_ids\",\"homepage\",\"in_production\",\"languages\",\n",
    "        \"last_air_date\",\"last_episode_to_air_id\",\"last_episode_to_air_name\",\n",
    "        \"last_episode_to_air_overview\",\"last_episode_to_air_vote_average\",\n",
    "        \"last_episode_to_air_vote_count\",\"last_episode_to_air_air_date\",\n",
    "        \"last_episode_to_air_episode_number\",\"last_episode_to_air_production_code\",\n",
    "        \"last_episode_to_air_runtime\",\"last_episode_to_air_season_number\",\n",
    "        \"last_episode_to_air_show_id\",\"last_episode_to_air_still_path\",\n",
    "        \"next_episode_to_air\",\"networks\",\"number_of_episodes\",\"number_of_seasons\",\n",
    "        \"origin_country\",\"original_language\",\"original_name\",\"overview\",\"popularity\",\n",
    "        \"poster_path\",\"production_companies\",\"production_countries\",\"seasons\",\n",
    "        \"spoken_languages\",\"status\",\"tagline\",\"type_detail\",\"vote_average\",\n",
    "        \"vote_count\",\"review\",\"keyword\",\"top_cast\",\"directors\",\"writers\",\n",
    "        \"providers_flatrate\",\"providers_rent\",\"providers_buy\",\"imdb_id\"  # âœ¨ ë³€ê²½ëœ ì»¬ëŸ¼ë“¤\n",
    "    ]\n",
    "\n",
    "    SEASON_COLS = [\n",
    "        \"_id\",\"season_id\",\"series_id\",\"series_name\",\"season_number\",\"name\",\"air_date\",\n",
    "        \"overview\",\"vote_average\",\"vote_count\",\"network_names\",\"network_ids\",\n",
    "        \"total_episodes\",\"avg_episode_runtime\",\"poster_path\"\n",
    "    ]\n",
    "\n",
    "    df_series = df_series[SERIES_COLS].drop_duplicates(subset=[\"id\"])\n",
    "    if not df_seasons.empty:\n",
    "        df_seasons = df_seasons[SEASON_COLS].drop_duplicates(subset=[\"series_id\",\"season_number\"])\n",
    "        df_series = fill_series_gaps(df_series, df_seasons)\n",
    "\n",
    "    df_series.to_csv(SERIES_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    df_seasons.to_csv(SEASONS_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    try:\n",
    "        df_series.to_parquet(SERIES_PARQ, index=False)\n",
    "        df_seasons.to_parquet(SEASONS_PARQ, index=False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for f in [SERIES_TEMP, SEASONS_TEMP]:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "\n",
    "    elapsed = (datetime.now() - t0).total_seconds()/60\n",
    "\n",
    "    print(\"=\"*90)\n",
    "    print(\"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"ğŸ“Œ Series: {len(df_series):,}ê°œ | Seasons: {len(df_seasons):,}ê°œ\")\n",
    "    print(f\"ğŸ“Œ API ìš”ì²­: {stats['requests']:,}íšŒ | ì—ëŸ¬: {stats['errors']} | ì¬ì‹œë„: {stats['retries']}\")\n",
    "    print(f\"ğŸ“Œ í‰ê·  ì†ë„: {len(df_series)/elapsed:.1f} series/ë¶„\")\n",
    "    print(f\"â± ì´ ì‹œê°„: {elapsed:.1f}ë¶„ ({elapsed/60:.1f}ì‹œê°„)\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥\n",
    "    print(\"\\nğŸ“Š ìƒ˜í”Œ Providers ë°ì´í„°:\")\n",
    "    if not df_series.empty and 'providers_flatrate' in df_series.columns:\n",
    "        sample = df_series[df_series['providers_flatrate'] != '{}'].head(1)\n",
    "        if not sample.empty:\n",
    "            print(f\"Title: {sample.iloc[0]['title']}\")\n",
    "            print(f\"Flatrate: {sample.iloc[0]['providers_flatrate'][:200]}...\")\n",
    "            print(f\"IMDB ID: {sample.iloc[0]['imdb_id']}\")\n",
    "    \n",
    "    executor.shutdown(wait=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
