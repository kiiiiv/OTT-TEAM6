# ==========================================================
# IMDB GraphQL API ë¦¬ë·° í¬ë¡¤ëŸ¬ (ë¹„ë™ê¸° ê³ ì„±ëŠ¥)
# HTML ìŠ¤í¬ë˜í•‘ ëŒ€ì‹  ê³µì‹ GraphQL API ì‚¬ìš©
# í›¨ì”¬ ë¹ ë¥´ê³  ì•ˆì •ì !
# ==========================================================

import asyncio
import aiohttp
import pandas as pd
import json
from datetime import datetime
from pathlib import Path
import time
from urllib.parse import quote
import html

# ==========================================================
# ì„¤ì •
# ==========================================================

# GraphQL API ì„¤ì •
GRAPHQL_URL = "https://caching.graphql.imdb.com/"
OPERATION_NAME = "TitleReviewsRefine"
PERSISTED_QUERY_HASH = "d389bc70c27f09c00b663705f0112254e8a7c75cde1cfd30e63a2d98c1080c87"

# Rate Limiting (GraphQLì€ ë” ê´€ëŒ€í•¨)
MAX_CALLS_PER_SECOND = 2
TIMEOUT = aiohttp.ClientTimeout(total=30, connect=10)
MAX_RETRIES = 3

# í•œ ë²ˆì— ê°€ì ¸ì˜¬ ë¦¬ë·° ìˆ˜ (25ê°€ ìµœì )
REVIEWS_PER_REQUEST = 25

# User-Agent
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# ì¶œë ¥ íŒŒì¼
OUTPUT_CSV = "imdb_reviews_graphql.csv"
OUTPUT_PARQUET = "imdb_reviews_graphql.parquet"
CHECKPOINT_FILE = "imdb_graphql_checkpoint.json"

# í†µê³„
stats = {
    "series_total": 0,
    "series_success": 0,
    "series_failed": 0,
    "reviews_total": 0,
    "requests": 0,
    "start_time": None
}

# ==========================================================
# Rate Limiter
# ==========================================================
class RateLimiter:
    def __init__(self, rate):
        self.rate = rate
        self.tokens = rate
        self.updated_at = time.monotonic()
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        async with self.lock:
            now = time.monotonic()
            elapsed = now - self.updated_at
            self.tokens = min(self.rate, self.tokens + elapsed * self.rate)
            self.updated_at = now
            
            if self.tokens < 1:
                sleep_time = (1 - self.tokens) / self.rate
                await asyncio.sleep(sleep_time)
                self.tokens = 1
            
            self.tokens -= 1

rate_limiter = RateLimiter(MAX_CALLS_PER_SECOND)

# ==========================================================
# GraphQL ì¿¼ë¦¬ ìƒì„±
# ==========================================================
def build_graphql_url(imdb_id, after_cursor=None, first=25, sort_by="HELPFULNESS_SCORE"):
    """
    GraphQL API URL ìƒì„±
    
    Args:
        imdb_id: IMDB ID (ì˜ˆ: tt0944947)
        after_cursor: í˜ì´ì§€ë„¤ì´ì…˜ ì»¤ì„œ (ë‹¤ìŒ í˜ì´ì§€)
        first: í•œ ë²ˆì— ê°€ì ¸ì˜¬ ë¦¬ë·° ìˆ˜
        sort_by: ì •ë ¬ ê¸°ì¤€ (HELPFULNESS_SCORE, SUBMISSION_DATE, etc.)
    """
    variables = {
        "const": imdb_id,
        "first": first,
        "locale": "en-US",
        "sort": {
            "by": sort_by,
            "order": "DESC"
        },
        "filter": {}
    }
    
    if after_cursor:
        variables["after"] = after_cursor
    
    extensions = {
        "persistedQuery": {
            "sha256Hash": PERSISTED_QUERY_HASH,
            "version": 1
        }
    }
    
    # URL ì¸ì½”ë”©
    variables_json = json.dumps(variables, separators=(',', ':'))
    extensions_json = json.dumps(extensions, separators=(',', ':'))
    
    url = (
        f"{GRAPHQL_URL}?"
        f"operationName={OPERATION_NAME}"
        f"&variables={quote(variables_json)}"
        f"&extensions={quote(extensions_json)}"
    )
    
    return url

# ==========================================================
# GraphQL API í˜¸ì¶œ
# ==========================================================
async def fetch_graphql(session, url, retry=0):
    """GraphQL API í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨)"""
    if retry >= MAX_RETRIES:
        return None
    
    await rate_limiter.acquire()
    stats["requests"] += 1
    
    headers = {
        'User-Agent': USER_AGENT,
        'Accept': 'application/json',
        'Content-Type': 'application/json',
    }
    
    try:
        async with session.get(url, headers=headers, timeout=TIMEOUT) as resp:
            if resp.status == 429:
                wait_time = 5 * (retry + 1)
                print(f"âš ï¸  Rate limited, waiting {wait_time}s...")
                await asyncio.sleep(wait_time)
                return await fetch_graphql(session, url, retry + 1)
            
            if resp.status != 200:
                if retry < MAX_RETRIES - 1:
                    await asyncio.sleep(2 ** retry)
                    return await fetch_graphql(session, url, retry + 1)
                return None
            
            return await resp.json()
    
    except asyncio.TimeoutError:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await fetch_graphql(session, url, retry + 1)
        return None
    
    except Exception as e:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await fetch_graphql(session, url, retry + 1)
        return None

# ==========================================================
# ë¦¬ë·° íŒŒì‹±
# ==========================================================
def parse_review_node(node, imdb_id):
    """GraphQL responseì˜ review node íŒŒì‹±"""
    try:
        # HTML íƒœê·¸ ì œê±° í•¨ìˆ˜
        def clean_html(text):
            if not text:
                return None
            # HTML ì—”í‹°í‹° ë””ì½”ë”©
            text = html.unescape(text)
            # <br/> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ
            text = text.replace('<br/>', '\n').replace('<br>', '\n')
            # ë‚˜ë¨¸ì§€ HTML íƒœê·¸ ì œê±°
            import re
            text = re.sub('<[^<]+?>', '', text)
            return text.strip()
        
        # ê¸°ë³¸ ì •ë³´
        review_id = node.get('id')
        
        # ì‘ì„±ì
        author_data = node.get('author', {})
        username = author_data.get('username', {}).get('text')
        user_id = author_data.get('userId')
        
        # í‰ì 
        author_rating = node.get('authorRating')
        
        # Helpful íˆ¬í‘œ
        helpfulness = node.get('helpfulness', {})
        up_votes = helpfulness.get('upVotes', 0)
        down_votes = helpfulness.get('downVotes', 0)
        
        # ë‚ ì§œ
        submission_date = node.get('submissionDate')
        
        # ì œëª©
        summary = node.get('summary', {})
        review_title = summary.get('originalText')
        
        # ë‚´ìš©
        text_data = node.get('text', {}).get('originalText', {})
        review_text_html = text_data.get('plaidHtml')
        review_text = clean_html(review_text_html)
        
        # Spoiler ì—¬ë¶€
        is_spoiler = node.get('spoiler', False)
        
        return {
            'imdb_id': imdb_id,
            'review_id': review_id,
            'username': username,
            'user_id': user_id,
            'author_rating': author_rating,
            'helpful_up_votes': up_votes,
            'helpful_down_votes': down_votes,
            'helpful_total': up_votes + down_votes,
            'helpful_ratio': round(up_votes / (up_votes + down_votes), 3) if (up_votes + down_votes) > 0 else None,
            'submission_date': submission_date,
            'review_title': review_title,
            'review_text': review_text,
            'review_text_length': len(review_text) if review_text else 0,
            'is_spoiler': is_spoiler,
        }
    
    except Exception as e:
        print(f"âš ï¸  Error parsing review node: {e}")
        return None

# ==========================================================
# í•œ ì‹œë¦¬ì¦ˆì˜ ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘
# ==========================================================
async def fetch_all_reviews_for_series(session, imdb_id, series_title="", max_reviews=None):
    """
    í•œ ì‹œë¦¬ì¦ˆì˜ ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘ (GraphQL API ì‚¬ìš©)
    
    Args:
        session: aiohttp session
        imdb_id: IMDB ID
        series_title: ì‹œë¦¬ì¦ˆ ì œëª© (ë¡œê¹…ìš©)
        max_reviews: ìµœëŒ€ ë¦¬ë·° ìˆ˜ (Noneì´ë©´ ì „ì²´)
    
    Returns:
        list: ë¦¬ë·° ë¦¬ìŠ¤íŠ¸
    """
    all_reviews = []
    after_cursor = None
    page = 0
    
    try:
        while True:
            page += 1
            
            # GraphQL URL ìƒì„±
            url = build_graphql_url(imdb_id, after_cursor, REVIEWS_PER_REQUEST)
            
            # API í˜¸ì¶œ
            response = await fetch_graphql(session, url)
            
            if not response:
                break
            
            # ë°ì´í„° ì¶”ì¶œ
            data = response.get('data', {})
            title_data = data.get('title', {})
            reviews_data = title_data.get('reviews', {})
            
            # ì´ ë¦¬ë·° ìˆ˜
            total_reviews = reviews_data.get('total', 0)
            
            # ë¦¬ë·° íŒŒì‹±
            edges = reviews_data.get('edges', [])
            if not edges:
                break
            
            for edge in edges:
                node = edge.get('node', {})
                review = parse_review_node(node, imdb_id)
                if review:
                    all_reviews.append(review)
            
            # ìµœëŒ€ ë¦¬ë·° ìˆ˜ ì²´í¬
            if max_reviews and len(all_reviews) >= max_reviews:
                all_reviews = all_reviews[:max_reviews]
                break
            
            # ë‹¤ìŒ í˜ì´ì§€ ì»¤ì„œ
            page_info = reviews_data.get('pageInfo', {})
            has_next_page = page_info.get('hasNextPage', False)
            after_cursor = page_info.get('endCursor')
            
            if not has_next_page or not after_cursor:
                break
            
            # ì§§ì€ ëŒ€ê¸°
            await asyncio.sleep(0.1)
        
        stats["reviews_total"] += len(all_reviews)
        
        if all_reviews:
            print(f"âœ… {series_title} ({imdb_id}): {len(all_reviews):,}/{total_reviews} ë¦¬ë·°")
        
        return all_reviews
    
    except Exception as e:
        print(f"âŒ {series_title} ({imdb_id}): {str(e)[:100]}")
        return all_reviews

# ==========================================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ==========================================================
def save_checkpoint(processed_ids):
    """ì¤‘ê°„ ì €ì¥"""
    # stats ë³µì‚¬ (datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜)
    stats_copy = stats.copy()
    if stats_copy.get('start_time'):
        stats_copy['start_time'] = stats_copy['start_time'].isoformat()
    
    checkpoint = {
        'processed_ids': list(processed_ids),
        'stats': stats_copy,
        'timestamp': datetime.now().isoformat()
    }
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(checkpoint, f)

def load_checkpoint():
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ + ê¸°ì¡´ CSVì—ì„œ ì²˜ë¦¬ëœ ID ë¡œë“œ"""
    processed_ids = set()
    
    # 1. ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì—ì„œ ë¡œë“œ (ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€)
    if Path(CHECKPOINT_FILE).exists():
        try:
            with open(CHECKPOINT_FILE, 'r') as f:
                checkpoint = json.load(f)
                processed_ids.update(checkpoint.get('processed_ids', []))
                print(f"ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ {len(checkpoint.get('processed_ids', [])):,}ê°œ ID ë¡œë“œ")
        except (json.JSONDecodeError, Exception) as e:
            print(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì†ìƒë¨, ì‚­ì œí•˜ê³  ê³„ì† ì§„í–‰: {e}")
            try:
                Path(CHECKPOINT_FILE).unlink()
            except:
                pass
    
    # 2. ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ ë¡œë“œ (ì¤‘ë³µ ë°©ì§€)
    if Path(OUTPUT_CSV).exists():
        try:
            df_existing = pd.read_csv(OUTPUT_CSV)
            if 'imdb_id' in df_existing.columns:
                existing_ids = df_existing['imdb_id'].unique()
                processed_ids.update(existing_ids)
                print(f"ğŸ“Œ ê¸°ì¡´ CSVì—ì„œ {len(existing_ids):,}ê°œ ì‹œë¦¬ì¦ˆ ë°œê²¬")
        except Exception as e:
            print(f"âš ï¸  ê¸°ì¡´ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return processed_ids

# ==========================================================
# ë©”ì¸ ì‹¤í–‰
# ==========================================================
async def main(input_csv_path, vote_threshold=30, max_reviews_per_series=None):
    """
    ì „ì²´ ë¦¬ë·° ìˆ˜ì§‘ (GraphQL API)
    
    Args:
        input_csv_path: TMDB CSV íŒŒì¼ ê²½ë¡œ
        vote_threshold: ìµœì†Œ vote_count
        max_reviews_per_series: ì‹œë¦¬ì¦ˆë‹¹ ìµœëŒ€ ë¦¬ë·° ìˆ˜ (Noneì´ë©´ ì „ì²´)
    """
    print("=" * 90)
    print("ğŸš€ IMDB GraphQL API ë¦¬ë·° í¬ë¡¤ëŸ¬")
    print("âœ¨ HTML ìŠ¤í¬ë˜í•‘ë³´ë‹¤ 5-10ë°° ë¹ ë¥´ê³  ì•ˆì •ì !")
    print("=" * 90)
    
    stats["start_time"] = datetime.now()
    t0 = datetime.now()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv(input_csv_path)
    df_filtered = df[(df['vote_count'] >= vote_threshold) & (df['imdb_id'].notna())]
    
    print(f"âœ… ì „ì²´ ì‹œë¦¬ì¦ˆ: {len(df):,}ê°œ")
    print(f"âœ… í•„í„°ë§ (vote_count>={vote_threshold} & imdb_id ì¡´ì¬): {len(df_filtered):,}ê°œ")
    
    if len(df_filtered) == 0:
        print("âš ï¸  ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    processed_ids = load_checkpoint()
    series_list = df_filtered[['id', 'title', 'imdb_id']].to_dict('records')
    
    if processed_ids:
        print(f"ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {len(processed_ids):,}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        series_list = [s for s in series_list if s['imdb_id'] not in processed_ids]
        print(f"ğŸ“Œ ë‚¨ì€ ì‘ì—…: {len(series_list):,}ê°œ")
    
    if len(series_list) == 0:
        print("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    stats["series_total"] = len(series_list)
    
    # 3. í¬ë¡¤ë§
    print(f"\nğŸš€ í¬ë¡¤ë§ ì‹œì‘")
    print(f"âš™ï¸  Rate Limit: {MAX_CALLS_PER_SECOND}íšŒ/ì´ˆ")
    print(f"âš™ï¸  ë¦¬ë·°/ìš”ì²­: {REVIEWS_PER_REQUEST}ê°œ")
    
    if max_reviews_per_series:
        print(f"âš™ï¸  ì‹œë¦¬ì¦ˆë‹¹ ìµœëŒ€: {max_reviews_per_series}ê°œ")
        estimated_time = len(series_list) * (max_reviews_per_series / REVIEWS_PER_REQUEST / MAX_CALLS_PER_SECOND) / 60
    else:
        print(f"âš™ï¸  ì‹œë¦¬ì¦ˆë‹¹ ìµœëŒ€: ì „ì²´")
        estimated_time = len(series_list) * 5  # í‰ê·  5ë¶„ ì¶”ì •
    
    print(f"â±ï¸  ì˜ˆìƒ ì‹œê°„: {estimated_time:.0f}ë¶„")
    
    connector = aiohttp.TCPConnector(
        limit=20,
        force_close=False,
        enable_cleanup_closed=True
    )
    
    all_results = []
    batch_size = 10
    
    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as session:
        for i in range(0, len(series_list), batch_size):
            batch = series_list[i:i+batch_size]
            
            # ë°°ì¹˜ ì²˜ë¦¬
            tasks = [
                fetch_all_reviews_for_series(
                    session,
                    s['imdb_id'],
                    s['title'],
                    max_reviews_per_series
                )
                for s in batch
            ]
            
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for series, reviews in zip(batch, batch_results):
                if isinstance(reviews, list) and reviews:
                    all_results.extend(reviews)
                    processed_ids.add(series['imdb_id'])
                    stats["series_success"] += 1
                elif isinstance(reviews, list):
                    processed_ids.add(series['imdb_id'])
                    stats["series_failed"] += 1
                else:
                    stats["series_failed"] += 1
            
            # ì£¼ê¸°ì  ì €ì¥
            if (i + batch_size) % 50 == 0:
                save_checkpoint(processed_ids)
                
                # CSV ì €ì¥ (append ëª¨ë“œ)
                if all_results:
                    df_batch = pd.DataFrame(all_results)
                    # ì¤‘ë³µ ì œê±°
                    df_batch = df_batch.drop_duplicates(subset=['review_id'])
                    
                    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ì— ë”°ë¼ header ì„¤ì •
                    file_exists = Path(OUTPUT_CSV).exists()
                    df_batch.to_csv(OUTPUT_CSV, mode='a', header=not file_exists, index=False, encoding='utf-8-sig')
                    
                    # ì €ì¥ í›„ ë©”ëª¨ë¦¬ì—ì„œ ì œê±° (ë©”ëª¨ë¦¬ ì ˆì•½)
                    all_results.clear()
                    print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ ({len(df_batch):,}ê°œ ë¦¬ë·°)")

            
            # ì§„í–‰ ìƒí™©
            elapsed = (datetime.now() - t0).total_seconds() / 60
            progress = stats["series_success"] + stats["series_failed"]
            rate = progress / elapsed if elapsed > 0 else 0
            eta = (stats["series_total"] - progress) / rate if rate > 0 else 0
            
            print(f"\nğŸ“Š ì§„í–‰: {progress}/{stats['series_total']} ({progress/stats['series_total']*100:.1f}%) | "
                  f"ì„±ê³µ: {stats['series_success']} | ì‹¤íŒ¨: {stats['series_failed']} | "
                  f"ì´ ë¦¬ë·°: {stats['reviews_total']:,}ê°œ | "
                  f"ìš”ì²­: {stats['requests']:,}íšŒ | "
                  f"ì†ë„: {rate:.1f}/ë¶„ | ETA: {eta:.0f}ë¶„\n")
    
    # 4. ìµœì¢… ì €ì¥
    print("\nğŸ’¾ ìµœì¢… ì €ì¥ ì¤‘...")
    
    # ë‚¨ì€ ê²°ê³¼ ì €ì¥
    if all_results:
        df_batch = pd.DataFrame(all_results)
        df_batch = df_batch.drop_duplicates(subset=['review_id'])
        file_exists = Path(OUTPUT_CSV).exists()
        df_batch.to_csv(OUTPUT_CSV, mode='a', header=not file_exists, index=False, encoding='utf-8-sig')
    
    # ì „ì²´ CSV ë¡œë“œ ë° ì¤‘ë³µ ì œê±°
    if Path(OUTPUT_CSV).exists():
        df_results = pd.read_csv(OUTPUT_CSV)
        df_results = df_results.drop_duplicates(subset=['review_id'])
        # ì¤‘ë³µ ì œê±° í›„ ë‹¤ì‹œ ì €ì¥
        df_results.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    else:
        df_results = pd.DataFrame()
    
    try:
        if not df_results.empty:
            df_results.to_parquet(OUTPUT_PARQUET, index=False)
    except Exception as e:
        print(f"âš ï¸  Parquet ì €ì¥ ì‹¤íŒ¨: {e}")

    
    if Path(CHECKPOINT_FILE).exists():
        Path(CHECKPOINT_FILE).unlink()
    
    # 5. í†µê³„
    elapsed = (datetime.now() - t0).total_seconds() / 60
    
    print("\n" + "=" * 90)
    print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 90)
    print(f"ğŸ“Œ ì‹œë¦¬ì¦ˆ: {stats['series_success']:,}/{stats['series_total']:,}ê°œ ì„±ê³µ")
    
    if not df_results.empty:
        print(f"ğŸ“Œ ì´ ë¦¬ë·°: {len(df_results):,}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
        print(f"ğŸ“Œ í‰ê· : {len(df_results)/stats['series_success']:.1f}ê°œ/ì‹œë¦¬ì¦ˆ" if stats['series_success'] > 0 else "")
    else:
        print(f"ğŸ“Œ ì´ ë¦¬ë·°: 0ê°œ")
    
    print(f"ğŸ“Œ ì´ ìš”ì²­: {stats['requests']:,}íšŒ")
    print(f"â±ï¸  ì´ ì‹œê°„: {elapsed:.1f}ë¶„ ({elapsed/60:.2f}ì‹œê°„)")
    
    if stats['series_success'] > 0:
        print(f"ğŸ“Š ì†ë„: {stats['series_success']/elapsed:.1f}ê°œ/ë¶„")
        if not df_results.empty:
            print(f"ğŸ“Š ë¦¬ë·° ìˆ˜ì§‘ ì†ë„: {len(df_results)/elapsed:.0f}ê°œ/ë¶„")
    
    print("=" * 90)
    
    # ìƒ˜í”Œ
    if not df_results.empty:
        print("\nğŸ“Š ìƒ˜í”Œ ë°ì´í„°:")
        print(df_results.head(3).to_string())
        print(f"\nâœ… ê²°ê³¼ íŒŒì¼: {OUTPUT_CSV}")
        
        # í†µê³„ ì •ë³´
        print("\nğŸ“ˆ ë¦¬ë·° í†µê³„:")
        if 'author_rating' in df_results.columns:
            print(f"  í‰ê·  í‰ì : {df_results['author_rating'].mean():.2f}/10")
        if 'review_text_length' in df_results.columns:
            print(f"  í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {df_results['review_text_length'].mean():.0f}ì")
        if 'is_spoiler' in df_results.columns:
            print(f"  Spoiler ë¦¬ë·°: {df_results['is_spoiler'].sum():,}ê°œ ({df_results['is_spoiler'].sum()/len(df_results)*100:.1f}%)")
        if 'helpful_total' in df_results.columns:
            print(f"  í‰ê·  helpful íˆ¬í‘œ: {df_results['helpful_total'].mean():.1f}ê°œ")
    else:
        print("\nâš ï¸  ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ==========================================================
# ì‹¤í–‰
# ==========================================================
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='IMDB GraphQL API ë¦¬ë·° í¬ë¡¤ëŸ¬')
    parser.add_argument('--input', '-i', default='tv_series_2005_2015_FULL.csv',
                        help='ì…ë ¥ CSV íŒŒì¼')
    parser.add_argument('--vote', '-v', type=int, default=30,
                        help='ìµœì†Œ vote_count (ê¸°ë³¸: 30)')
    parser.add_argument('--max-reviews', '-m', type=int, default=None,
                        help='ì‹œë¦¬ì¦ˆë‹¹ ìµœëŒ€ ë¦¬ë·° ìˆ˜ (ê¸°ë³¸: ì „ì²´)')
    
    args = parser.parse_args()
    
    if not Path(args.input).exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
    else:
        asyncio.run(main(args.input, args.vote, args.max_reviews))
