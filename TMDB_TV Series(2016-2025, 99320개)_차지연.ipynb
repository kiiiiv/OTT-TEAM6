{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMDB APIë¡œ ë°ì´í„° ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TMDB TV Series ë°ì´í„° ì „ìˆ˜ ìˆ˜ì§‘\n",
    "\n",
    "1. ìˆ˜ì§‘ ëŒ€ìƒ: TMDB TV Series(ë“œë¼ë§ˆ) ë°ì´í„°\n",
    "2. ìˆ˜ì§‘ ê¸°ê°„: 2016ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 11ì›” 29ì¼ (ì•½ 11ë…„ì¹˜)\n",
    "3. ìˆ˜ì§‘ ë²”ìœ„: ì „ì²´ í˜ì´ì§€ (TMDB API ì´ˆë‹¹ ìš”ì²­ ìˆ˜(40 req/s) ì œí•œ, 500í˜ì´ì§€ ì œí•œ ë“± ì£¼ì˜)\n",
    "4. ìˆ˜ì§‘ ê²°ê³¼: 53ê°œ ì»¬ëŸ¼, 99320ê°œ ë°ì´í„°, ì•½ 144ë¶„ ì†Œìš”\n",
    "5. ìˆ˜ì§‘ ë°©ì‹: ë™ê¸° + ë©€í‹°ìŠ¤ë ˆë“œ(ThreadPoolExecutor) ë°©ì‹\n",
    "5. ê¸°íƒ€ ì‚¬í•­: ê²°ì¸¡ì¹˜ ë³´ì™„ í•„ìš” -> episode_level ìˆ˜ì§‘ ì‹œë„í–ˆìœ¼ë‚˜, ë„ˆë¬´ ë§ì•„ì„œ ì œì™¸ -> ì‹œì¦Œ ì •ë³´ ì¶”ê°€ ìˆ˜ì§‘ ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lizzy\\ìŠ¤íŒŒë¥´íƒ€_íŒŒì´ì¬\\tmdbapiì‹¤ìŠµ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\lizzy\\ìŠ¤íŒŒë¥´íƒ€_íŒŒì´ì¬\\tmdbapiì‹¤ìŠµ\")  # ì›í•˜ëŠ” ê²½ë¡œë¡œ ë³€ê²½\n",
    "print(os.getcwd())  # ì˜ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # í˜„ì¬ ë””ë ‰í„°ë¦¬ì˜ .env íŒŒì¼ ì½ì–´ì„œ í™˜ê²½ë³€ìˆ˜ë¡œ ë“±ë¡\n",
    "\n",
    "API_KEY = os.getenv(\"API_KEY\")  # .envì—ì„œ API í‚¤ ì½ì–´ì˜¤ê¸°\n",
    "\n",
    "if API_KEY is None:\n",
    "    raise ValueError(\"API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "ğŸš€ TMDB ë“œë¼ë§ˆ ì „ì²´ ìˆ˜ì§‘ (2016-01-01 ~ 2025-11-29)\n",
      "==========================================================================================\n",
      "\n",
      "ğŸ“… ì „ì²´ ê¸°ê°„: 2005-01-01 ~ 2015-12-31\n",
      "âš¡ Rate Limit: ì´ˆë‹¹ 25 ìš”ì²­ (TMDB ì œí•œ 40 ëŒ€ë¹„ ì—¬ìœ )\n",
      "âš ï¸ discover/tv 500í˜ì´ì§€ ì œí•œ â†’ ì¬ê·€ì  ë‚ ì§œ ë¶„í• ë¡œ 100% ëˆ„ë½ ë°©ì§€\n",
      "==========================================================================================\n",
      "\n",
      "ğŸ“Œ 1ë‹¨ê³„: TV ì‹œë¦¬ì¦ˆ ID ì „ì²´ ìˆ˜ì§‘ ì‹œì‘\n",
      "ğŸ“… 2005-01-01 ~ 2015-12-31: ê²°ê³¼ 43,869ê°œ, í˜ì´ì§€ 2194ê°œ\n",
      "ğŸ”€ í˜ì´ì§€ 500 ì´ˆê³¼ â†’ ë‚ ì§œ ë¶„í• : [2005-01-01 ~ 2010-07-02], [2010-07-03 ~ 2015-12-31]\n",
      "  ğŸ“… 2005-01-01 ~ 2010-07-02: ê²°ê³¼ 17,936ê°œ, í˜ì´ì§€ 897ê°œ\n",
      "  ğŸ”€ í˜ì´ì§€ 500 ì´ˆê³¼ â†’ ë‚ ì§œ ë¶„í• : [2005-01-01 ~ 2007-10-02], [2007-10-03 ~ 2010-07-02]\n",
      "    ğŸ“… 2005-01-01 ~ 2007-10-02: ê²°ê³¼ 8,086ê°œ, í˜ì´ì§€ 405ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ… í™•ì • êµ¬ê°„: 2005-01-01 ~ 2007-10-02 â†’ ID 8,086ê°œ\n",
      "    ğŸ“… 2007-10-03 ~ 2010-07-02: ê²°ê³¼ 9,850ê°œ, í˜ì´ì§€ 493ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ… í™•ì • êµ¬ê°„: 2007-10-03 ~ 2010-07-02 â†’ ID 9,850ê°œ\n",
      "  âœ… ë³‘í•©: 2005-01-01 ~ 2010-07-02 â†’ ID 17,936ê°œ\n",
      "  ğŸ“… 2010-07-03 ~ 2015-12-31: ê²°ê³¼ 25,933ê°œ, í˜ì´ì§€ 1297ê°œ\n",
      "  ğŸ”€ í˜ì´ì§€ 500 ì´ˆê³¼ â†’ ë‚ ì§œ ë¶„í• : [2010-07-03 ~ 2013-04-01], [2013-04-02 ~ 2015-12-31]\n",
      "    ğŸ“… 2010-07-03 ~ 2013-04-01: ê²°ê³¼ 11,856ê°œ, í˜ì´ì§€ 593ê°œ\n",
      "    ğŸ”€ í˜ì´ì§€ 500 ì´ˆê³¼ â†’ ë‚ ì§œ ë¶„í• : [2010-07-03 ~ 2011-11-16], [2011-11-17 ~ 2013-04-01]\n",
      "      ğŸ“… 2010-07-03 ~ 2011-11-16: ê²°ê³¼ 5,767ê°œ, í˜ì´ì§€ 289ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âœ… í™•ì • êµ¬ê°„: 2010-07-03 ~ 2011-11-16 â†’ ID 5,767ê°œ\n",
      "      ğŸ“… 2011-11-17 ~ 2013-04-01: ê²°ê³¼ 6,089ê°œ, í˜ì´ì§€ 305ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âœ… í™•ì • êµ¬ê°„: 2011-11-17 ~ 2013-04-01 â†’ ID 6,089ê°œ\n",
      "    âœ… ë³‘í•©: 2010-07-03 ~ 2013-04-01 â†’ ID 11,856ê°œ\n",
      "    ğŸ“… 2013-04-02 ~ 2015-12-31: ê²°ê³¼ 14,077ê°œ, í˜ì´ì§€ 704ê°œ\n",
      "    ğŸ”€ í˜ì´ì§€ 500 ì´ˆê³¼ â†’ ë‚ ì§œ ë¶„í• : [2013-04-02 ~ 2014-08-16], [2014-08-17 ~ 2015-12-31]\n",
      "      ğŸ“… 2013-04-02 ~ 2014-08-16: ê²°ê³¼ 6,499ê°œ, í˜ì´ì§€ 325ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âœ… í™•ì • êµ¬ê°„: 2013-04-02 ~ 2014-08-16 â†’ ID 6,499ê°œ\n",
      "      ğŸ“… 2014-08-17 ~ 2015-12-31: ê²°ê³¼ 7,578ê°œ, í˜ì´ì§€ 379ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âœ… í™•ì • êµ¬ê°„: 2014-08-17 ~ 2015-12-31 â†’ ID 7,578ê°œ\n",
      "    âœ… ë³‘í•©: 2013-04-02 ~ 2015-12-31 â†’ ID 14,077ê°œ\n",
      "  âœ… ë³‘í•©: 2010-07-03 ~ 2015-12-31 â†’ ID 25,933ê°œ\n",
      "âœ… ë³‘í•©: 2005-01-01 ~ 2015-12-31 â†’ ID 43,869ê°œ\n",
      "\n",
      "==========================================================================================\n",
      "âœ¨ ì´ ê³ ìœ  ID: 43,869ê°œ\n",
      "\n",
      "ğŸ“Œ 2ë‹¨ê³„: ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...\n",
      "âš¡ Workers: 6ê°œ (ì•ˆì „í•œ ë™ì‹œì„±)\n",
      "âš¡ Rate Limit: ì´ˆë‹¹ 25 ìš”ì²­\n",
      "â±  ëŒ€ëµì ì¸ ìµœì†Œ ì´ë¡ ì¹˜: 29.2ë¶„ (IDë‹¹ 1~2íšŒ í˜¸ì¶œ í¬í•¨)\n",
      "ğŸ’¾ ìë™ ë°±ì—…: ë§¤ 1000ê°œ ë˜ëŠ” 5ë¶„ë§ˆë‹¤\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:   2%|â–ˆ                                                             | 765/43869 [01:06<1:02:14, 11.54it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==========================================================\n",
    "# 0. API í‚¤ ë° ê¸°ë³¸ ì„¤ì •\n",
    "# ==========================================================\n",
    "\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "TIMEOUT = 10\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 1. Rate Limiter (ì´ˆë‹¹ ìš”ì²­ ìˆ˜ ì œí•œ: 25 req/së¡œ ì•ˆì •í™”)\n",
    "# ==========================================================\n",
    "class RateLimiter:\n",
    "    \"\"\"ì´ˆë‹¹ ìš”ì²­ ìˆ˜ë¥¼ ì œí•œí•˜ëŠ” Rate Limiter (ì „ì—­ì—ì„œ ê³µìœ )\"\"\"\n",
    "    def __init__(self, max_calls=25, period=1.0):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = []\n",
    "        self.lock = threading.Lock()\n",
    "        self.global_backoff_until = 0\n",
    "    \n",
    "    def wait(self):\n",
    "        \"\"\"í•„ìš”ì‹œ ëŒ€ê¸° (429 ë˜ëŠ” ì´ˆë‹¹ ìš”ì²­ ìˆ˜ ì´ˆê³¼ ë°©ì§€)\"\"\"\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "\n",
    "            # ê¸€ë¡œë²Œ ë°±ì˜¤í”„ ì²´í¬\n",
    "            if self.global_backoff_until > now:\n",
    "                sleep_time = self.global_backoff_until - now\n",
    "                print(f\"â¸ï¸  ê¸€ë¡œë²Œ ë°±ì˜¤í”„ ëŒ€ê¸° ì¤‘... {sleep_time:.1f}ì´ˆ\")\n",
    "                time.sleep(sleep_time)\n",
    "                now = time.time()\n",
    "\n",
    "            # ì˜¤ë˜ëœ í˜¸ì¶œ ì œê±°\n",
    "            self.calls = [c for c in self.calls if now - c < self.period]\n",
    "            \n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                sleep_time = self.period - (now - self.calls[0]) + 0.05\n",
    "                if sleep_time > 0:\n",
    "                    time.sleep(sleep_time)\n",
    "            \n",
    "            self.calls.append(time.time())\n",
    "    \n",
    "    def set_global_backoff(self, seconds):\n",
    "        \"\"\"ê¸€ë¡œë²Œ ë°±ì˜¤í”„ ì„¤ì • (429 ì—ëŸ¬ ë°œìƒ ì‹œ)\"\"\"\n",
    "        with self.lock:\n",
    "            backoff_until = time.time() + seconds\n",
    "            if backoff_until > self.global_backoff_until:\n",
    "                self.global_backoff_until = backoff_until\n",
    "                print(f\"ğŸ”´ ê¸€ë¡œë²Œ ë°±ì˜¤í”„ ì„¤ì •: {seconds}ì´ˆ\")\n",
    "\n",
    "\n",
    "# ì „ì—­ Rate Limiter (ì•ˆì • 25 req/s)\n",
    "rate_limiter = RateLimiter(max_calls=25, period=1.0)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 2. ì„¸ì…˜ ìƒì„±\n",
    "# ==========================================================\n",
    "def create_session():\n",
    "    \"\"\"ì¬ì‹œë„ ë¡œì§ê³¼ ì—°ê²° í’€ì´ ìˆëŠ” ì„¸ì…˜ ìƒì„±\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[500, 502, 503, 504],\n",
    "        respect_retry_after_header=True\n",
    "    )\n",
    "    adapter = HTTPAdapter(\n",
    "        max_retries=retry,\n",
    "        pool_connections=20,\n",
    "        pool_maxsize=20\n",
    "    )\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "session = create_session()\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3. ê³µí†µ API ìš”ì²­ í•¨ìˆ˜ (tmdb_get)\n",
    "# ==========================================================\n",
    "def tmdb_get(path, params=None, retry_count=0):\n",
    "    \"\"\"\n",
    "    TMDB GET ìš”ì²­ ê³µí†µ í•¨ìˆ˜\n",
    "    - api_key ìë™ í¬í•¨\n",
    "    - Rate limit ì²˜ë¦¬\n",
    "    - 429 / íƒ€ì„ì•„ì›ƒ / ê¸°íƒ€ HTTP ì—ëŸ¬ ì¬ì‹œë„\n",
    "    \"\"\"\n",
    "    if retry_count >= MAX_RETRIES:\n",
    "        print(f\"âš ï¸  ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {path}\")\n",
    "        return None\n",
    "    \n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    params[\"api_key\"] = API_KEY\n",
    "    params.setdefault(\"language\", \"en-US\")\n",
    "    \n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "    \n",
    "    rate_limiter.wait()\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, params=params, headers=HEADERS, timeout=TIMEOUT)\n",
    "        \n",
    "        # 429 ì—ëŸ¬ ì²˜ë¦¬\n",
    "        if response.status_code == 429:\n",
    "            retry_after = response.headers.get('Retry-After')\n",
    "            wait_time = int(retry_after) if retry_after else 10 * (2 ** retry_count)\n",
    "            \n",
    "            print(f\"ğŸ”´ 429 Rate Limit! {wait_time}ì´ˆ ëŒ€ê¸°... (ì¬ì‹œë„ {retry_count + 1}/{MAX_RETRIES})\")\n",
    "            rate_limiter.set_global_backoff(wait_time + 2)\n",
    "            time.sleep(wait_time + 2)\n",
    "            return tmdb_get(path, params, retry_count + 1)\n",
    "        \n",
    "        # 404 ì—ëŸ¬ëŠ” ì¡°ìš©íˆ ì²˜ë¦¬\n",
    "        if response.status_code == 404:\n",
    "            return None\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "        \n",
    "    except requests.Timeout:\n",
    "        print(f\"âš ï¸  íƒ€ì„ì•„ì›ƒ: {path}\")\n",
    "        if retry_count < MAX_RETRIES:\n",
    "            time.sleep(2 ** retry_count)\n",
    "            return tmdb_get(path, params, retry_count + 1)\n",
    "        return None\n",
    "    except requests.RequestException as e:\n",
    "        if retry_count < MAX_RETRIES:\n",
    "            wait_time = 2 ** retry_count\n",
    "            print(f\"âš ï¸  ìš”ì²­ ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count + 1}/{MAX_RETRIES}): {e}\")\n",
    "            time.sleep(wait_time)\n",
    "            return tmdb_get(path, params, retry_count + 1)\n",
    "        print(f\"âŒ ìµœì¢… ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 4. í—¬í¼ í•¨ìˆ˜\n",
    "# ==========================================================\n",
    "def simple_list_to_str(lst, key=\"name\"):\n",
    "    \"\"\"ë¦¬ìŠ¤íŠ¸ ì•ˆ dictì—ì„œ íŠ¹ì • keyë§Œ ë½‘ì•„ ë¬¸ìì—´ë¡œ ë³€í™˜\"\"\"\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join([str(item.get(key, \"\")) for item in lst if item.get(key)])\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 5. TV Series ID ìˆ˜ì§‘ (100% ëˆ„ë½ ë°©ì§€: ì¬ê·€ì  ë‚ ì§œ ë¶„í• )\n",
    "# ==========================================================\n",
    "def fetch_single_page(page, start_date, end_date):\n",
    "    \"\"\"discover/tv ë‹¨ì¼ í˜ì´ì§€ ìˆ˜ì§‘\"\"\"\n",
    "    params = {\n",
    "        \"sort_by\": \"popularity.desc\",\n",
    "        \"first_air_date.gte\": start_date,\n",
    "        \"first_air_date.lte\": end_date,\n",
    "        \"page\": page,\n",
    "        \"include_adult\": True,\n",
    "    }\n",
    "    \n",
    "    data = tmdb_get(\"/discover/tv\", params)\n",
    "    if data:\n",
    "        return data.get(\"results\", []), data.get(\"total_pages\", 1), data.get(\"total_results\", 0)\n",
    "    return [], 1, 0\n",
    "\n",
    "\n",
    "def collect_ids_in_range(start_date, end_date, depth=0):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ë‚ ì§œ êµ¬ê°„ì— ëŒ€í•´:\n",
    "    - total_pages <= 500: í•´ë‹¹ êµ¬ê°„ì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ ìˆ˜ì§‘\n",
    "    - total_pages > 500 : ë‚ ì§œë¥¼ ë°˜ìœ¼ë¡œ ìª¼ê°œì„œ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘\n",
    "    â†’ 500í˜ì´ì§€ ì œí•œ ë•Œë¬¸ì— IDê°€ ì˜ë¦¬ëŠ” ë¬¸ì œë¥¼ ë°©ì§€ (100% ëˆ„ë½ ë°©ì§€)\n",
    "    \"\"\"\n",
    "    indent = \"  \" * depth\n",
    "    results, total_pages, total_results = fetch_single_page(1, start_date, end_date)\n",
    "\n",
    "    # ê²°ê³¼ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ ì¡°ìš©íˆ ì¢…ë£Œ\n",
    "    if total_results == 0:\n",
    "        print(f\"{indent}ğŸ“­ {start_date} ~ {end_date}: ê²°ê³¼ 0ê°œ\")\n",
    "        return set()\n",
    "\n",
    "    print(f\"{indent}ğŸ“… {start_date} ~ {end_date}: ê²°ê³¼ {total_results:,}ê°œ, í˜ì´ì§€ {total_pages}ê°œ\")\n",
    "\n",
    "    # TMDB discoverëŠ” ìµœëŒ€ 500í˜ì´ì§€ë§Œ ì ‘ê·¼ ê°€ëŠ¥ â†’ 500 ì´í•˜ì´ë©´ ì•ˆì „\n",
    "    if total_pages <= 500:\n",
    "        all_ids = set([s[\"id\"] for s in results])\n",
    "        max_pages = total_pages\n",
    "\n",
    "        if max_pages > 1:\n",
    "            # ë‚˜ë¨¸ì§€ í˜ì´ì§€ ë©€í‹°ìŠ¤ë ˆë“œ ìˆ˜ì§‘\n",
    "            with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "                futures = {\n",
    "                    executor.submit(fetch_single_page, page, start_date, end_date): page \n",
    "                    for page in range(2, max_pages + 1)\n",
    "                }\n",
    "                \n",
    "                for future in tqdm(as_completed(futures), total=len(futures),\n",
    "                                   desc=f\"{indent}  í˜ì´ì§€ ìˆ˜ì§‘\", leave=False):\n",
    "                    try:\n",
    "                        page_results, _, _ = future.result()\n",
    "                        for s in page_results:\n",
    "                            all_ids.add(s[\"id\"])\n",
    "                    except Exception as e:\n",
    "                        print(f\"{indent}  í˜ì´ì§€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "        print(f\"{indent}âœ… í™•ì • êµ¬ê°„: {start_date} ~ {end_date} â†’ ID {len(all_ids):,}ê°œ\")\n",
    "        return all_ids\n",
    "\n",
    "    # total_pages > 500ì´ë©´ ë‚ ì§œë¥¼ ë°˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¬ê·€ ìˆ˜ì§‘\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    if start_dt >= end_dt:\n",
    "        # ì´ë¡ ìƒ ê±°ì˜ ë‚˜ì˜¤ê¸° ì–´ë ¤ìš´ ì¼€ì´ìŠ¤ì§€ë§Œ, ë°©ì–´ì ìœ¼ë¡œ ë©”ì‹œì§€ ì¶œë ¥\n",
    "        print(f\"{indent}âš ï¸ ë‹¨ì¼ì¼({start_date})ì— 500í˜ì´ì§€ ì´ˆê³¼ ë°œìƒ. ì¼ë¶€ ëˆ„ë½ ê°€ëŠ¥.\")\n",
    "        # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ 500í˜ì´ì§€ë§Œ ìˆ˜ì§‘ (ì™„ì „ ë°©ì§€ëŠ” ì´ë¡ ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥ ì¼€ì´ìŠ¤)\n",
    "        all_ids = set([s[\"id\"] for s in results])\n",
    "        max_pages = 500\n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            futures = {\n",
    "                executor.submit(fetch_single_page, page, start_date, end_date): page \n",
    "                for page in range(2, max_pages + 1)\n",
    "            }\n",
    "            for future in tqdm(as_completed(futures), total=len(futures),\n",
    "                               desc=f\"{indent}  (ë¹„ìƒ) í˜ì´ì§€ ìˆ˜ì§‘\", leave=False):\n",
    "                try:\n",
    "                    page_results, _, _ = future.result()\n",
    "                    for s in page_results:\n",
    "                        all_ids.add(s[\"id\"])\n",
    "                except Exception as e:\n",
    "                    print(f\"{indent}  í˜ì´ì§€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}\")\n",
    "        return all_ids\n",
    "\n",
    "    # ì¤‘ê°„ ë‚ ì§œ ê³„ì‚°í•´ì„œ ë°˜ìœ¼ë¡œ ìª¼ê°œê¸°\n",
    "    mid_dt = start_dt + (end_dt - start_dt) / 2\n",
    "    mid_date = mid_dt.date().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # ë‘ êµ¬ê°„: [start ~ mid], [mid+1 ~ end]\n",
    "    left_end_dt = datetime.strptime(mid_date, \"%Y-%m-%d\")\n",
    "    right_start_dt = left_end_dt + timedelta(days=1)\n",
    "\n",
    "    left_start = start_date\n",
    "    left_end = left_end_dt.strftime(\"%Y-%m-%d\")\n",
    "    right_start = right_start_dt.strftime(\"%Y-%m-%d\")\n",
    "    right_end = end_date\n",
    "\n",
    "    print(f\"{indent}ğŸ”€ í˜ì´ì§€ 500 ì´ˆê³¼ â†’ ë‚ ì§œ ë¶„í• : [{left_start} ~ {left_end}], [{right_start} ~ {right_end}]\")\n",
    "\n",
    "    ids_left = collect_ids_in_range(left_start, left_end, depth+1)\n",
    "    ids_right = collect_ids_in_range(right_start, right_end, depth+1)\n",
    "\n",
    "    merged = ids_left.union(ids_right)\n",
    "    print(f\"{indent}âœ… ë³‘í•©: {start_date} ~ {end_date} â†’ ID {len(merged):,}ê°œ\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 6. OTT ì œê³µ ì •ë³´ ìˆ˜ì§‘ (/tv/{id}/watch/providers ë³„ë„ í˜¸ì¶œ)\n",
    "# ==========================================================\n",
    "def fetch_providers(series_id):\n",
    "    \"\"\"\n",
    "    OTT ì œê³µ ì •ë³´ ìˆ˜ì§‘\n",
    "    - /tv/{id}/watch/providers\n",
    "    - ì£¼ìš” êµ­ê°€: US, GB, KR\n",
    "    \"\"\"\n",
    "    data = tmdb_get(f\"/tv/{series_id}/watch/providers\", params={\"language\": \"en-US\"})\n",
    "    if not data:\n",
    "        return \"\"\n",
    "    \n",
    "    results = data.get(\"results\", {})\n",
    "    all_providers = set()\n",
    "    target_countries = ['US', 'GB', 'KR']\n",
    "    \n",
    "    for country in target_countries:\n",
    "        if country in results:\n",
    "            info = results[country]\n",
    "            flatrate = info.get(\"flatrate\", [])\n",
    "            for p in flatrate:\n",
    "                name = p.get(\"provider_name\")\n",
    "                if name:\n",
    "                    all_providers.add(name)\n",
    "    \n",
    "    return \", \".join(sorted(all_providers)) if all_providers else \"\"\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 7. TV Series ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (append_to_response + ë³„ë„ providers)\n",
    "# ==========================================================\n",
    "def fetch_tv_details(series_id):\n",
    "    \"\"\"\n",
    "    TV ì‹œë¦¬ì¦ˆ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘\n",
    "    - /tv/{id}?append_to_response=reviews,keywords,aggregate_credits\n",
    "    - watch/providersëŠ” ë³„ë„ ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ìˆ˜ì§‘\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"append_to_response\": \"reviews,keywords,aggregate_credits\"\n",
    "    }\n",
    "    \n",
    "    data = tmdb_get(f\"/tv/{series_id}\", params)\n",
    "    if not data:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # last_episode_to_air\n",
    "        last_ep = data.get(\"last_episode_to_air\") or {}\n",
    "        \n",
    "        # genre_ids / genres\n",
    "        genres_list = data.get(\"genres\", []) or []\n",
    "        genre_ids = \", \".join([str(g.get(\"id\", \"\")) for g in genres_list if g.get(\"id\") is not None])\n",
    "\n",
    "        created_by_str = simple_list_to_str(data.get(\"created_by\", []))\n",
    "\n",
    "        # ===== ê¸°ë³¸ í•„ë“œ =====\n",
    "        record = {\n",
    "            \"id\": data.get(\"id\"),\n",
    "            \"title\": data.get(\"name\"),\n",
    "            \"type\": \"tv_series\",\n",
    "            \"adult\": data.get(\"adult\"),\n",
    "            \"backdrop_path\": data.get(\"backdrop_path\"),\n",
    "            \"created_by\": created_by_str,\n",
    "            \"episode_run_time\": \", \".join(map(str, data.get(\"episode_run_time\", []))) if data.get(\"episode_run_time\") else \"\",\n",
    "            \"first_air_date\": data.get(\"first_air_date\"),\n",
    "            \"genres\": simple_list_to_str(genres_list),\n",
    "            \"genre_ids\": genre_ids,\n",
    "            \"homepage\": data.get(\"homepage\"),\n",
    "            \"in_production\": data.get(\"in_production\"),\n",
    "            \"languages\": \", \".join(data.get(\"languages\", [])) if data.get(\"languages\") else \"\",\n",
    "            \"last_air_date\": data.get(\"last_air_date\"),\n",
    "\n",
    "            # last_episode_to_air ì„¸ë¶€\n",
    "            \"last_episode_to_air_id\": last_ep.get(\"id\"),\n",
    "            \"last_episode_to_air_name\": last_ep.get(\"name\"),\n",
    "            \"last_episode_to_air_overview\": last_ep.get(\"overview\"),\n",
    "            \"last_episode_to_air_vote_average\": last_ep.get(\"vote_average\"),\n",
    "            \"last_episode_to_air_vote_count\": last_ep.get(\"vote_count\"),\n",
    "            \"last_episode_to_air_air_date\": last_ep.get(\"air_date\"),\n",
    "            \"last_episode_to_air_episode_number\": last_ep.get(\"episode_number\"),\n",
    "            \"last_episode_to_air_production_code\": last_ep.get(\"production_code\"),\n",
    "            \"last_episode_to_air_runtime\": last_ep.get(\"runtime\"),\n",
    "            \"last_episode_to_air_season_number\": last_ep.get(\"season_number\"),\n",
    "            \"last_episode_to_air_show_id\": last_ep.get(\"show_id\"),\n",
    "            \"last_episode_to_air_still_path\": last_ep.get(\"still_path\"),\n",
    "            \n",
    "            \"name\": data.get(\"name\"),\n",
    "            \"next_episode_to_air\": str(data.get(\"next_episode_to_air\")) if data.get(\"next_episode_to_air\") else \"\",\n",
    "            \"networks\": simple_list_to_str(data.get(\"networks\", [])),\n",
    "            \"number_of_episodes\": data.get(\"number_of_episodes\"),\n",
    "            \"number_of_seasons\": data.get(\"number_of_seasons\"),\n",
    "            \"origin_country\": \", \".join(data.get(\"origin_country\", [])) if data.get(\"origin_country\") else \"\",\n",
    "            \"original_language\": data.get(\"original_language\"),\n",
    "            \"original_name\": data.get(\"original_name\"),\n",
    "            \"overview\": data.get(\"overview\"),\n",
    "            \"popularity\": data.get(\"popularity\"),\n",
    "            \"poster_path\": data.get(\"poster_path\"),\n",
    "            \"production_companies\": simple_list_to_str(data.get(\"production_companies\", [])),\n",
    "            \"production_countries\": simple_list_to_str(data.get(\"production_countries\", [])),\n",
    "            \"seasons\": \"; \".join(\n",
    "                f\"S{s.get('season_number')}: {s.get('name')} ({s.get('episode_count')} eps)\"\n",
    "                for s in (data.get(\"seasons\") or [])\n",
    "                if s.get('season_number') is not None\n",
    "            ),\n",
    "            \"spoken_languages\": simple_list_to_str(data.get(\"spoken_languages\", [])),\n",
    "            \"status\": data.get(\"status\"),\n",
    "            \"tagline\": data.get(\"tagline\"),\n",
    "            \"type_detail\": data.get(\"type\"),\n",
    "            \"vote_average\": data.get(\"vote_average\"),\n",
    "            \"vote_count\": data.get(\"vote_count\"),\n",
    "        }\n",
    "\n",
    "        # ===== ë¦¬ë·° (reviews) =====\n",
    "        reviews_block = data.get(\"reviews\") or {}\n",
    "        reviews = reviews_block.get(\"results\", []) if isinstance(reviews_block, dict) else []\n",
    "        all_reviews = []\n",
    "        for review in reviews[:5]:\n",
    "            author = review.get(\"author\", \"unknown\")\n",
    "            rating = review.get(\"author_details\", {}).get(\"rating\")\n",
    "            rating_str = f\" (Rating: {rating})\" if rating is not None else \"\"\n",
    "            content = review.get(\"content\", \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \")[:200]\n",
    "            all_reviews.append(f\"{author}{rating_str}: {content}\")\n",
    "        record[\"review\"] = \" || \".join(all_reviews) if all_reviews else \"\"\n",
    "\n",
    "        # ===== í‚¤ì›Œë“œ (keywords) =====\n",
    "        keywords_block = data.get(\"keywords\") or {}\n",
    "        keywords = keywords_block.get(\"results\", []) if isinstance(keywords_block, dict) else []\n",
    "        record[\"keyword\"] = \", \".join([k.get(\"name\", \"\") for k in keywords[:20] if k.get(\"name\")])\n",
    "\n",
    "        # ===== aggregate_credits (top_cast / directors / writers) =====\n",
    "        credits = data.get(\"aggregate_credits\") or {}\n",
    "        cast = credits.get(\"cast\", []) if isinstance(credits, dict) else []\n",
    "        crew = credits.get(\"crew\", []) if isinstance(credits, dict) else []\n",
    "\n",
    "        # ì£¼ì—° ë°°ìš° 4ëª…\n",
    "        record[\"top_cast\"] = \", \".join([c.get(\"name\", \"\") for c in cast[:4] if c.get(\"name\")])\n",
    "\n",
    "        directors_set = set()\n",
    "        writers_set = set()\n",
    "        for c in crew:\n",
    "            name = c.get(\"name\", \"\")\n",
    "            if not name:\n",
    "                continue\n",
    "            jobs = c.get(\"jobs\", []) or []\n",
    "            for job in jobs:\n",
    "                job_name = job.get(\"job\", \"\") or \"\"\n",
    "                if \"Director\" in job_name or \"Series Director\" in job_name:\n",
    "                    directors_set.add(name)\n",
    "                if job_name in [\"Writer\", \"Screenplay\", \"Story\", \"Writer's Assistant\"]:\n",
    "                    writers_set.add(name)\n",
    "\n",
    "        record[\"directors\"] = \", \".join(sorted(directors_set)[:10])\n",
    "        record[\"writers\"] = \", \".join(sorted(writers_set)[:10])\n",
    "\n",
    "        # ===== providers (ë³„ë„ ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ) =====\n",
    "        record[\"providers\"] = fetch_providers(series_id)\n",
    "\n",
    "        return record\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ìƒì„¸ ì •ë³´ ì²˜ë¦¬ ì˜¤ë¥˜ (ID: {series_id}): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 8. MAIN ì‹¤í–‰ë¶€\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 90)\n",
    "    print(\"ğŸš€ TMDB ë“œë¼ë§ˆ ì „ì²´ ìˆ˜ì§‘ (2016-01-01 ~ 2025-11-29)\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    start_date = \"2005-01-01\"\n",
    "    end_date = \"2015-12-31\"\n",
    "    total_start = time.time()\n",
    "\n",
    "    print(f\"\\nğŸ“… ì „ì²´ ê¸°ê°„: {start_date} ~ {end_date}\")\n",
    "    print(f\"âš¡ Rate Limit: ì´ˆë‹¹ 25 ìš”ì²­ (TMDB ì œí•œ 40 ëŒ€ë¹„ ì—¬ìœ )\")\n",
    "    print(f\"âš ï¸ discover/tv 500í˜ì´ì§€ ì œí•œ â†’ ì¬ê·€ì  ë‚ ì§œ ë¶„í• ë¡œ 100% ëˆ„ë½ ë°©ì§€\")\n",
    "    print(\"=\"*90)\n",
    "\n",
    "    # 1) ID ì „ì²´ ìˆ˜ì§‘ (ì¬ê·€ ë¶„í• )\n",
    "    print(\"\\nğŸ“Œ 1ë‹¨ê³„: TV ì‹œë¦¬ì¦ˆ ID ì „ì²´ ìˆ˜ì§‘ ì‹œì‘\")\n",
    "    all_tv_ids = collect_ids_in_range(start_date, end_date)\n",
    "\n",
    "    if not all_tv_ids:\n",
    "        print(\"\\nâš ï¸ ìˆ˜ì§‘ëœ IDê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        raise SystemExit\n",
    "\n",
    "    tv_list = sorted(list(all_tv_ids))\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"âœ¨ ì´ ê³ ìœ  ID: {len(tv_list):,}ê°œ\")\n",
    "\n",
    "    # 2) ìƒì„¸ ì •ë³´ ìˆ˜ì§‘\n",
    "    print(f\"\\nğŸ“Œ 2ë‹¨ê³„: ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...\")\n",
    "    print(f\"âš¡ Workers: 6ê°œ (ì•ˆì „í•œ ë™ì‹œì„±)\")\n",
    "    print(f\"âš¡ Rate Limit: ì´ˆë‹¹ 25 ìš”ì²­\")\n",
    "    print(f\"â±  ëŒ€ëµì ì¸ ìµœì†Œ ì´ë¡ ì¹˜: {len(tv_list) / 25 / 60:.1f}ë¶„ (IDë‹¹ 1~2íšŒ í˜¸ì¶œ í¬í•¨)\")\n",
    "    print(f\"ğŸ’¾ ìë™ ë°±ì—…: ë§¤ 1000ê°œ ë˜ëŠ” 5ë¶„ë§ˆë‹¤\")\n",
    "    print(\"=\"*90 + \"\\n\")\n",
    "\n",
    "    results = []\n",
    "    failed_ids = []\n",
    "    last_save_time = time.time()\n",
    "    save_interval = 300  # 5ë¶„\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        futures = {executor.submit(fetch_tv_details, sid): sid for sid in tv_list}\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"TV ìƒì„¸ ìˆ˜ì§‘\"):\n",
    "            series_id = futures[future]\n",
    "            try:\n",
    "                detail = future.result()\n",
    "                if detail:\n",
    "                    results.append(detail)\n",
    "                else:\n",
    "                    failed_ids.append(series_id)\n",
    "                \n",
    "                # ì¤‘ê°„ ì €ì¥\n",
    "                current_time = time.time()\n",
    "                if (len(results) % 1000 == 0 and len(results) > 0) or \\\n",
    "                   (current_time - last_save_time > save_interval and len(results) > 0):\n",
    "                    df_temp = pd.DataFrame(results)\n",
    "                    backup_file = f\"backup_{len(results)}_{datetime.now().strftime('%H%M')}.csv\"\n",
    "                    df_temp.to_csv(backup_file, index=False, encoding=\"utf-8-sig\")\n",
    "                    print(f\"\\nğŸ’¾ ë°±ì—… ì €ì¥: {backup_file} ({len(results):,}ê°œ)\")\n",
    "                    last_save_time = current_time\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nìˆ˜ì§‘ ì˜¤ë¥˜ (ID: {series_id}): {e}\")\n",
    "                failed_ids.append(series_id)\n",
    "\n",
    "    print(f\"\\nğŸ‰ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(results):,}ê°œ\")\n",
    "    if failed_ids:\n",
    "        print(f\"âš ï¸  ì‹¤íŒ¨í•œ ID: {len(failed_ids)}ê°œ\")\n",
    "        with open(\"failed_ids.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(map(str, failed_ids)))\n",
    "\n",
    "    # 3) ìµœì¢… ì €ì¥\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "\n",
    "        # ì •ë ¬\n",
    "        if \"first_air_date\" in df.columns and \"popularity\" in df.columns:\n",
    "            df = df.sort_values([\"first_air_date\", \"popularity\"], ascending=[True, False])\n",
    "\n",
    "        # 53ë²ˆì§¸ ì»¬ëŸ¼: \"created by\" (ê³µë°± ë²„ì „) ì¶”ê°€\n",
    "        if \"created_by\" in df.columns:\n",
    "            df[\"created by\"] = df[\"created_by\"]\n",
    "\n",
    "        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬ (ìš”ì²­ ìŠ¤í™ 53ê°œ)\n",
    "        column_order = [\n",
    "            \"id\",\n",
    "            \"title\",\n",
    "            \"type\",\n",
    "            \"adult\",\n",
    "            \"backdrop_path\",\n",
    "            \"created_by\",\n",
    "            \"episode_run_time\",\n",
    "            \"first_air_date\",\n",
    "            \"genres\",\n",
    "            \"genre_ids\",\n",
    "            \"homepage\",\n",
    "            \"in_production\",\n",
    "            \"languages\",\n",
    "            \"last_air_date\",\n",
    "            \"last_episode_to_air_id\",\n",
    "            \"last_episode_to_air_name\",\n",
    "            \"last_episode_to_air_overview\",\n",
    "            \"last_episode_to_air_vote_average\",\n",
    "            \"last_episode_to_air_vote_count\",\n",
    "            \"last_episode_to_air_air_date\",\n",
    "            \"last_episode_to_air_episode_number\",\n",
    "            \"last_episode_to_air_production_code\",\n",
    "            \"last_episode_to_air_runtime\",\n",
    "            \"last_episode_to_air_season_number\",\n",
    "            \"last_episode_to_air_show_id\",\n",
    "            \"last_episode_to_air_still_path\",\n",
    "            \"name\",\n",
    "            \"next_episode_to_air\",\n",
    "            \"networks\",\n",
    "            \"number_of_episodes\",\n",
    "            \"number_of_seasons\",\n",
    "            \"origin_country\",\n",
    "            \"original_language\",\n",
    "            \"original_name\",\n",
    "            \"overview\",\n",
    "            \"popularity\",\n",
    "            \"poster_path\",\n",
    "            \"production_companies\",\n",
    "            \"production_countries\",\n",
    "            \"seasons\",\n",
    "            \"spoken_languages\",\n",
    "            \"status\",\n",
    "            \"tagline\",\n",
    "            \"type_detail\",\n",
    "            \"vote_average\",\n",
    "            \"vote_count\",\n",
    "            \"review\",\n",
    "            \"keyword\",\n",
    "            \"top_cast\",\n",
    "            \"directors\",\n",
    "            \"writers\",\n",
    "            \"created by\",     # alias\n",
    "            \"providers\",\n",
    "        ]\n",
    "        \n",
    "        existing_columns = [col for col in column_order if col in df.columns]\n",
    "        df = df[existing_columns]\n",
    "        \n",
    "        output_file = \"tv_series_2016_2025_FULL.csv\"\n",
    "        df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        elapsed = time.time() - total_start\n",
    "\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"========================== DONE ==========================\")\n",
    "        print(f\"ğŸ“Œ ì´ ë°ì´í„°: {len(df):,}ê°œ\")\n",
    "        print(f\"ğŸ“Š ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ (ìš”ì²­ 53ê°œ ë°˜ì˜)\")\n",
    "        print(f\"â±  ì†Œìš”ì‹œê°„: {elapsed/60:.2f}ë¶„ ({elapsed/3600:.2f}ì‹œê°„)\")\n",
    "        print(f\"ğŸ’¾ ì €ì¥ íŒŒì¼: {output_file}\")\n",
    "        print(f\"ğŸ“ˆ í‰ê·  ì²˜ë¦¬ ì†ë„: {len(df)/(elapsed/60):.1f}ê°œ/ë¶„\")\n",
    "        if failed_ids:\n",
    "            print(f\"âš ï¸  ì‹¤íŒ¨í•œ ID: {len(failed_ids)}ê°œ (failed_ids.txt ì°¸ì¡°)\")\n",
    "        \n",
    "        # ê°„ë‹¨ ìš”ì•½\n",
    "        if \"first_air_date\" in df.columns:\n",
    "            print(\"\\n[ë°ì´í„° ìš”ì•½]\")\n",
    "            print(f\"- ì—°ë„ ë²”ìœ„: {df['first_air_date'].min()} ~ {df['first_air_date'].max()}\")\n",
    "        if \"popularity\" in df.columns:\n",
    "            print(f\"- í‰ê·  ì¸ê¸°ë„: {df['popularity'].mean():.2f}\")\n",
    "        if \"vote_average\" in df.columns:\n",
    "            print(f\"- í‰ê·  í‰ì : {df['vote_average'].mean():.2f}\")\n",
    "        if \"number_of_episodes\" in df.columns:\n",
    "            print(f\"- ì´ ì—í”¼ì†Œë“œ: {df['number_of_episodes'].sum():,.0f}ê°œ\")\n",
    "\n",
    "        print(\"=\"*90)\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ ìˆ˜ì§‘ëœ ìƒì„¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "ğŸš€ TMDB ë“œë¼ë§ˆ ì „ì²´ ìˆ˜ì§‘ (2005-01-01 ~ 2015-12-31)\n",
      "==========================================================================================\n",
      "\n",
      "ğŸ“… ì „ì²´ ê¸°ê°„: 2005-01-01 ~ 2015-12-31\n",
      "âš¡ Rate Limit: ì´ˆë‹¹ 25 ìš”ì²­ (TMDB ì œí•œ 40 ëŒ€ë¹„ ì—¬ìœ )\n",
      "âš ï¸ discover/tv 500í˜ì´ì§€ ì œí•œ â†’ ì¬ê·€ì  ë‚ ì§œ ë¶„í• ë¡œ 100% ëˆ„ë½ ë°©ì§€\n",
      "==========================================================================================\n",
      "\n",
      "ğŸ“Œ 1ë‹¨ê³„: TV ì‹œë¦¬ì¦ˆ ID ì „ì²´ ìˆ˜ì§‘ ì‹œì‘\n",
      "ğŸ“… 2005-01-01 ~ 2015-12-31: ê²°ê³¼ 43,869ê°œ, í˜ì´ì§€ 2194ê°œ\n",
      "ğŸ”€ í˜ì´ì§€ 500 ì´ˆê³¼ â†’ ë‚ ì§œ ë¶„í• : [2005-01-01 ~ 2010-07-02], [2010-07-03 ~ 2015-12-31]\n",
      "  ğŸ“… 2005-01-01 ~ 2010-07-02: ê²°ê³¼ 17,936ê°œ, í˜ì´ì§€ 897ê°œ\n",
      "  ğŸ”€ í˜ì´ì§€ 500 ì´ˆê³¼ â†’ ë‚ ì§œ ë¶„í• : [2005-01-01 ~ 2007-10-02], [2007-10-03 ~ 2010-07-02]\n",
      "    ğŸ“… 2005-01-01 ~ 2007-10-02: ê²°ê³¼ 8,086ê°œ, í˜ì´ì§€ 405ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ… í™•ì • êµ¬ê°„: 2005-01-01 ~ 2007-10-02 â†’ ID 8,086ê°œ\n",
      "    ğŸ“… 2007-10-03 ~ 2010-07-02: ê²°ê³¼ 9,850ê°œ, í˜ì´ì§€ 493ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ… í™•ì • êµ¬ê°„: 2007-10-03 ~ 2010-07-02 â†’ ID 9,850ê°œ\n",
      "  âœ… ë³‘í•©: 2005-01-01 ~ 2010-07-02 â†’ ID 17,936ê°œ\n",
      "  ğŸ“… 2010-07-03 ~ 2015-12-31: ê²°ê³¼ 25,933ê°œ, í˜ì´ì§€ 1297ê°œ\n",
      "  ğŸ”€ í˜ì´ì§€ 500 ì´ˆê³¼ â†’ ë‚ ì§œ ë¶„í• : [2010-07-03 ~ 2013-04-01], [2013-04-02 ~ 2015-12-31]\n",
      "    ğŸ“… 2010-07-03 ~ 2013-04-01: ê²°ê³¼ 11,856ê°œ, í˜ì´ì§€ 593ê°œ\n",
      "    ğŸ”€ í˜ì´ì§€ 500 ì´ˆê³¼ â†’ ë‚ ì§œ ë¶„í• : [2010-07-03 ~ 2011-11-16], [2011-11-17 ~ 2013-04-01]\n",
      "      ğŸ“… 2010-07-03 ~ 2011-11-16: ê²°ê³¼ 5,767ê°œ, í˜ì´ì§€ 289ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âœ… í™•ì • êµ¬ê°„: 2010-07-03 ~ 2011-11-16 â†’ ID 5,767ê°œ\n",
      "      ğŸ“… 2011-11-17 ~ 2013-04-01: ê²°ê³¼ 6,089ê°œ, í˜ì´ì§€ 305ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âœ… í™•ì • êµ¬ê°„: 2011-11-17 ~ 2013-04-01 â†’ ID 6,089ê°œ\n",
      "    âœ… ë³‘í•©: 2010-07-03 ~ 2013-04-01 â†’ ID 11,856ê°œ\n",
      "    ğŸ“… 2013-04-02 ~ 2015-12-31: ê²°ê³¼ 14,077ê°œ, í˜ì´ì§€ 704ê°œ\n",
      "    ğŸ”€ í˜ì´ì§€ 500 ì´ˆê³¼ â†’ ë‚ ì§œ ë¶„í• : [2013-04-02 ~ 2014-08-16], [2014-08-17 ~ 2015-12-31]\n",
      "      ğŸ“… 2013-04-02 ~ 2014-08-16: ê²°ê³¼ 6,499ê°œ, í˜ì´ì§€ 325ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âœ… í™•ì • êµ¬ê°„: 2013-04-02 ~ 2014-08-16 â†’ ID 6,499ê°œ\n",
      "      ğŸ“… 2014-08-17 ~ 2015-12-31: ê²°ê³¼ 7,578ê°œ, í˜ì´ì§€ 379ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âœ… í™•ì • êµ¬ê°„: 2014-08-17 ~ 2015-12-31 â†’ ID 7,578ê°œ\n",
      "    âœ… ë³‘í•©: 2013-04-02 ~ 2015-12-31 â†’ ID 14,077ê°œ\n",
      "  âœ… ë³‘í•©: 2010-07-03 ~ 2015-12-31 â†’ ID 25,933ê°œ\n",
      "âœ… ë³‘í•©: 2005-01-01 ~ 2015-12-31 â†’ ID 43,869ê°œ\n",
      "\n",
      "==========================================================================================\n",
      "âœ¨ ì´ ê³ ìœ  ID: 43,869ê°œ\n",
      "\n",
      "ğŸ“Œ 2ë‹¨ê³„: ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...\n",
      "âš¡ Workers: 6ê°œ (ì•ˆì „í•œ ë™ì‹œì„±)\n",
      "âš¡ Rate Limit: ì´ˆë‹¹ 25 ìš”ì²­\n",
      "â±  ëŒ€ëµì ì¸ ìµœì†Œ ì´ë¡ ì¹˜: 29.2ë¶„ (IDë‹¹ 1~2íšŒ í˜¸ì¶œ í¬í•¨)\n",
      "ğŸ’¾ ìë™ ë°±ì—…: ë§¤ 1000ê°œ ë˜ëŠ” 5ë¶„ë§ˆë‹¤\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:   2%|â–ˆâ–                                                             | 1003/43869 [02:07<55:15, 12.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_1000_1501.csv (1,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:   5%|â–ˆâ–ˆâ–‰                                                            | 2005/43869 [03:53<54:58, 12.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_2000_1503.csv (2,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:   7%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                        | 3000/43869 [06:12<9:53:46,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_3000_1505.csv (3,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                         | 4009/43869 [08:02<47:01, 14.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_4000_1507.csv (4,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                      | 5000/43869 [10:05<3:49:51,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_5000_1509.csv (5,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                      | 6045/43869 [11:35<44:10, 14.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_6000_1511.csv (6,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                     | 7029/43869 [12:59<41:37, 14.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_7000_1512.csv (7,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                 | 7488/43869 [21:49<141:32:10, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_7469_1521.csv (7,469ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                   | 8007/43869 [22:33<42:12, 14.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_8000_1522.csv (8,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                  | 9010/43869 [23:58<44:14, 13.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_9000_1523.csv (9,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 10010/43869 [25:22<39:55, 14.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_10000_1525.csv (10,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             | 11001/43869 [26:46<1:48:36,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_11000_1526.csv (11,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                             | 12015/43869 [28:11<38:22, 13.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_12000_1527.csv (12,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                          | 13000/43869 [29:36<1:58:51,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_13000_1529.csv (13,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                          | 14016/43869 [31:00<34:06, 14.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_14000_1530.csv (14,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 15015/43869 [32:24<34:54, 13.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_15000_1532.csv (15,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                       | 16024/43869 [33:49<29:47, 15.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_16000_1533.csv (16,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 17000/43869 [35:13<2:13:39,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_17000_1534.csv (17,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 18019/43869 [36:37<30:51, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_18000_1536.csv (18,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 19001/43869 [38:01<2:02:51,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_19000_1537.csv (19,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 20017/43869 [39:25<28:18, 14.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_20000_1539.csv (20,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 21023/43869 [40:50<23:38, 16.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_21000_1540.csv (21,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 22001/43869 [42:14<2:04:24,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_22000_1541.csv (22,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 23023/43869 [43:40<22:51, 15.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_23000_1543.csv (23,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 24001/43869 [45:05<1:51:54,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_24000_1544.csv (24,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 25024/43869 [46:31<21:44, 14.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_25000_1546.csv (25,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 26024/43869 [47:57<19:56, 14.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_26000_1547.csv (26,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 27023/43869 [49:24<21:03, 13.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_27000_1549.csv (27,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 28028/43869 [50:50<18:30, 14.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_28000_1550.csv (28,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 29027/43869 [52:16<16:37, 14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_29000_1551.csv (29,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 30028/43869 [53:42<15:34, 14.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ë°±ì—… ì €ì¥: backup_30000_1553.csv (30,000ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TV ìƒì„¸ ìˆ˜ì§‘:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 30434/43869 [54:17<23:58,  9.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ==========================================================\n",
    "# 0. API í‚¤ ë° ê¸°ë³¸ ì„¤ì •\n",
    "# ==========================================================\n",
    "\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "TIMEOUT = 10\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 1. Rate Limiter (ì´ˆë‹¹ ìš”ì²­ ìˆ˜ ì œí•œ: 25 req/së¡œ ì•ˆì •í™”)\n",
    "# ==========================================================\n",
    "class RateLimiter:\n",
    "    \"\"\"ì´ˆë‹¹ ìš”ì²­ ìˆ˜ë¥¼ ì œí•œí•˜ëŠ” Rate Limiter (ì „ì—­ì—ì„œ ê³µìœ )\"\"\"\n",
    "    def __init__(self, max_calls=25, period=1.0):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = []\n",
    "        self.lock = threading.Lock()\n",
    "        self.global_backoff_until = 0\n",
    "    \n",
    "    def wait(self):\n",
    "        \"\"\"í•„ìš”ì‹œ ëŒ€ê¸° (429 ë˜ëŠ” ì´ˆë‹¹ ìš”ì²­ ìˆ˜ ì´ˆê³¼ ë°©ì§€)\"\"\"\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "\n",
    "            # ê¸€ë¡œë²Œ ë°±ì˜¤í”„ ì²´í¬\n",
    "            if self.global_backoff_until > now:\n",
    "                sleep_time = self.global_backoff_until - now\n",
    "                print(f\"â¸ï¸  ê¸€ë¡œë²Œ ë°±ì˜¤í”„ ëŒ€ê¸° ì¤‘... {sleep_time:.1f}ì´ˆ\")\n",
    "                time.sleep(sleep_time)\n",
    "                now = time.time()\n",
    "\n",
    "            # ì˜¤ë˜ëœ í˜¸ì¶œ ì œê±°\n",
    "            self.calls = [c for c in self.calls if now - c < self.period]\n",
    "            \n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                sleep_time = self.period - (now - self.calls[0]) + 0.05\n",
    "                if sleep_time > 0:\n",
    "                    time.sleep(sleep_time)\n",
    "            \n",
    "            self.calls.append(time.time())\n",
    "    \n",
    "    def set_global_backoff(self, seconds):\n",
    "        \"\"\"ê¸€ë¡œë²Œ ë°±ì˜¤í”„ ì„¤ì • (429 ì—ëŸ¬ ë°œìƒ ì‹œ)\"\"\"\n",
    "        with self.lock:\n",
    "            backoff_until = time.time() + seconds\n",
    "            if backoff_until > self.global_backoff_until:\n",
    "                self.global_backoff_until = backoff_until\n",
    "                print(f\"ğŸ”´ ê¸€ë¡œë²Œ ë°±ì˜¤í”„ ì„¤ì •: {seconds}ì´ˆ\")\n",
    "\n",
    "\n",
    "# ì „ì—­ Rate Limiter (ì•ˆì • 25 req/s)\n",
    "rate_limiter = RateLimiter(max_calls=25, period=1.0)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 2. ì„¸ì…˜ ìƒì„±\n",
    "# ==========================================================\n",
    "def create_session():\n",
    "    \"\"\"ì¬ì‹œë„ ë¡œì§ê³¼ ì—°ê²° í’€ì´ ìˆëŠ” ì„¸ì…˜ ìƒì„±\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[500, 502, 503, 504],\n",
    "        respect_retry_after_header=True\n",
    "    )\n",
    "    adapter = HTTPAdapter(\n",
    "        max_retries=retry,\n",
    "        pool_connections=20,\n",
    "        pool_maxsize=20\n",
    "    )\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "session = create_session()\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3. ê³µí†µ API ìš”ì²­ í•¨ìˆ˜ (tmdb_get)\n",
    "# ==========================================================\n",
    "def tmdb_get(path, params=None, retry_count=0):\n",
    "    \"\"\"\n",
    "    TMDB GET ìš”ì²­ ê³µí†µ í•¨ìˆ˜\n",
    "    - api_key ìë™ í¬í•¨\n",
    "    - Rate limit ì²˜ë¦¬\n",
    "    - 429 / íƒ€ì„ì•„ì›ƒ / ê¸°íƒ€ HTTP ì—ëŸ¬ ì¬ì‹œë„\n",
    "    \"\"\"\n",
    "    if retry_count >= MAX_RETRIES:\n",
    "        print(f\"âš ï¸  ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {path}\")\n",
    "        return None\n",
    "    \n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    params[\"api_key\"] = API_KEY\n",
    "    params.setdefault(\"language\", \"en-US\")\n",
    "    \n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "    \n",
    "    rate_limiter.wait()\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, params=params, headers=HEADERS, timeout=TIMEOUT)\n",
    "        \n",
    "        # 429 ì—ëŸ¬ ì²˜ë¦¬\n",
    "        if response.status_code == 429:\n",
    "            retry_after = response.headers.get('Retry-After')\n",
    "            wait_time = int(retry_after) if retry_after else 10 * (2 ** retry_count)\n",
    "            \n",
    "            print(f\"ğŸ”´ 429 Rate Limit! {wait_time}ì´ˆ ëŒ€ê¸°... (ì¬ì‹œë„ {retry_count + 1}/{MAX_RETRIES})\")\n",
    "            rate_limiter.set_global_backoff(wait_time + 2)\n",
    "            time.sleep(wait_time + 2)\n",
    "            return tmdb_get(path, params, retry_count + 1)\n",
    "        \n",
    "        # 404 ì—ëŸ¬ëŠ” ì¡°ìš©íˆ ì²˜ë¦¬\n",
    "        if response.status_code == 404:\n",
    "            return None\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "        \n",
    "    except requests.Timeout:\n",
    "        print(f\"âš ï¸  íƒ€ì„ì•„ì›ƒ: {path}\")\n",
    "        if retry_count < MAX_RETRIES:\n",
    "            time.sleep(2 ** retry_count)\n",
    "            return tmdb_get(path, params, retry_count + 1)\n",
    "        return None\n",
    "    except requests.RequestException as e:\n",
    "        if retry_count < MAX_RETRIES:\n",
    "            wait_time = 2 ** retry_count\n",
    "            print(f\"âš ï¸  ìš”ì²­ ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count + 1}/{MAX_RETRIES}): {e}\")\n",
    "            time.sleep(wait_time)\n",
    "            return tmdb_get(path, params, retry_count + 1)\n",
    "        print(f\"âŒ ìµœì¢… ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 4. í—¬í¼ í•¨ìˆ˜\n",
    "# ==========================================================\n",
    "def simple_list_to_str(lst, key=\"name\"):\n",
    "    \"\"\"ë¦¬ìŠ¤íŠ¸ ì•ˆ dictì—ì„œ íŠ¹ì • keyë§Œ ë½‘ì•„ ë¬¸ìì—´ë¡œ ë³€í™˜\"\"\"\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \", \".join([str(item.get(key, \"\")) for item in lst if item.get(key)])\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 5. TV Series ID ìˆ˜ì§‘ (100% ëˆ„ë½ ë°©ì§€: ì¬ê·€ì  ë‚ ì§œ ë¶„í• )\n",
    "# ==========================================================\n",
    "def fetch_single_page(page, start_date, end_date):\n",
    "    \"\"\"discover/tv ë‹¨ì¼ í˜ì´ì§€ ìˆ˜ì§‘\"\"\"\n",
    "    params = {\n",
    "        \"sort_by\": \"popularity.desc\",\n",
    "        \"first_air_date.gte\": start_date,\n",
    "        \"first_air_date.lte\": end_date,\n",
    "        \"page\": page,\n",
    "        \"include_adult\": True,\n",
    "    }\n",
    "    \n",
    "    data = tmdb_get(\"/discover/tv\", params)\n",
    "    if data:\n",
    "        return data.get(\"results\", []), data.get(\"total_pages\", 1), data.get(\"total_results\", 0)\n",
    "    return [], 1, 0\n",
    "\n",
    "\n",
    "def collect_ids_in_range(start_date, end_date, depth=0):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ë‚ ì§œ êµ¬ê°„ì— ëŒ€í•´:\n",
    "    - total_pages <= 500: í•´ë‹¹ êµ¬ê°„ì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ ìˆ˜ì§‘\n",
    "    - total_pages > 500 : ë‚ ì§œë¥¼ ë°˜ìœ¼ë¡œ ìª¼ê°œì„œ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘\n",
    "    â†’ 500í˜ì´ì§€ ì œí•œ ë•Œë¬¸ì— IDê°€ ì˜ë¦¬ëŠ” ë¬¸ì œë¥¼ ë°©ì§€ (100% ëˆ„ë½ ë°©ì§€)\n",
    "    \"\"\"\n",
    "    indent = \"  \" * depth\n",
    "    results, total_pages, total_results = fetch_single_page(1, start_date, end_date)\n",
    "\n",
    "    # ê²°ê³¼ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ ì¡°ìš©íˆ ì¢…ë£Œ\n",
    "    if total_results == 0:\n",
    "        print(f\"{indent}ğŸ“­ {start_date} ~ {end_date}: ê²°ê³¼ 0ê°œ\")\n",
    "        return set()\n",
    "\n",
    "    print(f\"{indent}ğŸ“… {start_date} ~ {end_date}: ê²°ê³¼ {total_results:,}ê°œ, í˜ì´ì§€ {total_pages}ê°œ\")\n",
    "\n",
    "    # TMDB discoverëŠ” ìµœëŒ€ 500í˜ì´ì§€ë§Œ ì ‘ê·¼ ê°€ëŠ¥ â†’ 500 ì´í•˜ì´ë©´ ì•ˆì „\n",
    "    if total_pages <= 500:\n",
    "        all_ids = set([s[\"id\"] for s in results])\n",
    "        max_pages = total_pages\n",
    "\n",
    "        if max_pages > 1:\n",
    "            # ë‚˜ë¨¸ì§€ í˜ì´ì§€ ë©€í‹°ìŠ¤ë ˆë“œ ìˆ˜ì§‘\n",
    "            with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "                futures = {\n",
    "                    executor.submit(fetch_single_page, page, start_date, end_date): page \n",
    "                    for page in range(2, max_pages + 1)\n",
    "                }\n",
    "                \n",
    "                for future in tqdm(as_completed(futures), total=len(futures),\n",
    "                                   desc=f\"{indent}  í˜ì´ì§€ ìˆ˜ì§‘\", leave=False):\n",
    "                    try:\n",
    "                        page_results, _, _ = future.result()\n",
    "                        for s in page_results:\n",
    "                            all_ids.add(s[\"id\"])\n",
    "                    except Exception as e:\n",
    "                        print(f\"{indent}  í˜ì´ì§€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "        print(f\"{indent}âœ… í™•ì • êµ¬ê°„: {start_date} ~ {end_date} â†’ ID {len(all_ids):,}ê°œ\")\n",
    "        return all_ids\n",
    "\n",
    "    # total_pages > 500ì´ë©´ ë‚ ì§œë¥¼ ë°˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¬ê·€ ìˆ˜ì§‘\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    if start_dt >= end_dt:\n",
    "        # ì´ë¡ ìƒ ê±°ì˜ ë‚˜ì˜¤ê¸° ì–´ë ¤ìš´ ì¼€ì´ìŠ¤ì§€ë§Œ, ë°©ì–´ì ìœ¼ë¡œ ë©”ì‹œì§€ ì¶œë ¥\n",
    "        print(f\"{indent}âš ï¸ ë‹¨ì¼ì¼({start_date})ì— 500í˜ì´ì§€ ì´ˆê³¼ ë°œìƒ. ì¼ë¶€ ëˆ„ë½ ê°€ëŠ¥.\")\n",
    "        # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ 500í˜ì´ì§€ë§Œ ìˆ˜ì§‘ (ì™„ì „ ë°©ì§€ëŠ” ì´ë¡ ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥ ì¼€ì´ìŠ¤)\n",
    "        all_ids = set([s[\"id\"] for s in results])\n",
    "        max_pages = 500\n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            futures = {\n",
    "                executor.submit(fetch_single_page, page, start_date, end_date): page \n",
    "                for page in range(2, max_pages + 1)\n",
    "            }\n",
    "            for future in tqdm(as_completed(futures), total=len(futures),\n",
    "                               desc=f\"{indent}  (ë¹„ìƒ) í˜ì´ì§€ ìˆ˜ì§‘\", leave=False):\n",
    "                try:\n",
    "                    page_results, _, _ = future.result()\n",
    "                    for s in page_results:\n",
    "                        all_ids.add(s[\"id\"])\n",
    "                except Exception as e:\n",
    "                    print(f\"{indent}  í˜ì´ì§€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}\")\n",
    "        return all_ids\n",
    "\n",
    "    # ì¤‘ê°„ ë‚ ì§œ ê³„ì‚°í•´ì„œ ë°˜ìœ¼ë¡œ ìª¼ê°œê¸°\n",
    "    mid_dt = start_dt + (end_dt - start_dt) / 2\n",
    "    mid_date = mid_dt.date().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # ë‘ êµ¬ê°„: [start ~ mid], [mid+1 ~ end]\n",
    "    left_end_dt = datetime.strptime(mid_date, \"%Y-%m-%d\")\n",
    "    right_start_dt = left_end_dt + timedelta(days=1)\n",
    "\n",
    "    left_start = start_date\n",
    "    left_end = left_end_dt.strftime(\"%Y-%m-%d\")\n",
    "    right_start = right_start_dt.strftime(\"%Y-%m-%d\")\n",
    "    right_end = end_date\n",
    "\n",
    "    print(f\"{indent}ğŸ”€ í˜ì´ì§€ 500 ì´ˆê³¼ â†’ ë‚ ì§œ ë¶„í• : [{left_start} ~ {left_end}], [{right_start} ~ {right_end}]\")\n",
    "\n",
    "    ids_left = collect_ids_in_range(left_start, left_end, depth+1)\n",
    "    ids_right = collect_ids_in_range(right_start, right_end, depth+1)\n",
    "\n",
    "    merged = ids_left.union(ids_right)\n",
    "    print(f\"{indent}âœ… ë³‘í•©: {start_date} ~ {end_date} â†’ ID {len(merged):,}ê°œ\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 6. OTT ì œê³µ ì •ë³´ ìˆ˜ì§‘ (/tv/{id}/watch/providers ë³„ë„ í˜¸ì¶œ)\n",
    "# ==========================================================\n",
    "def fetch_providers(series_id):\n",
    "    \"\"\"\n",
    "    OTT ì œê³µ ì •ë³´ ìˆ˜ì§‘\n",
    "    - /tv/{id}/watch/providers\n",
    "    - ì£¼ìš” êµ­ê°€: US, GB, KR\n",
    "    \"\"\"\n",
    "    data = tmdb_get(f\"/tv/{series_id}/watch/providers\", params={\"language\": \"en-US\"})\n",
    "    if not data:\n",
    "        return \"\"\n",
    "    \n",
    "    results = data.get(\"results\", {})\n",
    "    all_providers = set()\n",
    "    target_countries = ['US', 'GB', 'KR']\n",
    "    \n",
    "    for country in target_countries:\n",
    "        if country in results:\n",
    "            info = results[country]\n",
    "            flatrate = info.get(\"flatrate\", [])\n",
    "            for p in flatrate:\n",
    "                name = p.get(\"provider_name\")\n",
    "                if name:\n",
    "                    all_providers.add(name)\n",
    "    \n",
    "    return \", \".join(sorted(all_providers)) if all_providers else \"\"\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 7. TV Series ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (append_to_response + ë³„ë„ providers)\n",
    "# ==========================================================\n",
    "def fetch_tv_details(series_id):\n",
    "    \"\"\"\n",
    "    TV ì‹œë¦¬ì¦ˆ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘\n",
    "    - /tv/{id}?append_to_response=reviews,keywords,aggregate_credits\n",
    "    - watch/providersëŠ” ë³„ë„ ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ìˆ˜ì§‘\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"append_to_response\": \"reviews,keywords,aggregate_credits\"\n",
    "    }\n",
    "    \n",
    "    data = tmdb_get(f\"/tv/{series_id}\", params)\n",
    "    if not data:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # last_episode_to_air\n",
    "        last_ep = data.get(\"last_episode_to_air\") or {}\n",
    "        \n",
    "        # genre_ids / genres\n",
    "        genres_list = data.get(\"genres\", []) or []\n",
    "        genre_ids = \", \".join([str(g.get(\"id\", \"\")) for g in genres_list if g.get(\"id\") is not None])\n",
    "\n",
    "        created_by_str = simple_list_to_str(data.get(\"created_by\", []))\n",
    "\n",
    "        # ===== ê¸°ë³¸ í•„ë“œ (name, created by, poster_path ì œì™¸) =====\n",
    "        record = {\n",
    "            \"id\": data.get(\"id\"),\n",
    "            \"title\": data.get(\"name\"),\n",
    "            \"type\": \"tv_series\",\n",
    "            \"adult\": data.get(\"adult\"),\n",
    "            \"backdrop_path\": data.get(\"backdrop_path\"),\n",
    "            \"created_by\": created_by_str,\n",
    "            \"episode_run_time\": \", \".join(map(str, data.get(\"episode_run_time\", []))) if data.get(\"episode_run_time\") else \"\",\n",
    "            \"first_air_date\": data.get(\"first_air_date\"),\n",
    "            \"genres\": simple_list_to_str(genres_list),\n",
    "            \"genre_ids\": genre_ids,\n",
    "            \"homepage\": data.get(\"homepage\"),\n",
    "            \"in_production\": data.get(\"in_production\"),\n",
    "            \"languages\": \", \".join(data.get(\"languages\", [])) if data.get(\"languages\") else \"\",\n",
    "            \"last_air_date\": data.get(\"last_air_date\"),\n",
    "\n",
    "            # last_episode_to_air ì„¸ë¶€\n",
    "            \"last_episode_to_air_id\": last_ep.get(\"id\"),\n",
    "            \"last_episode_to_air_name\": last_ep.get(\"name\"),\n",
    "            \"last_episode_to_air_overview\": last_ep.get(\"overview\"),\n",
    "            \"last_episode_to_air_vote_average\": last_ep.get(\"vote_average\"),\n",
    "            \"last_episode_to_air_vote_count\": last_ep.get(\"vote_count\"),\n",
    "            \"last_episode_to_air_air_date\": last_ep.get(\"air_date\"),\n",
    "            \"last_episode_to_air_episode_number\": last_ep.get(\"episode_number\"),\n",
    "            \"last_episode_to_air_production_code\": last_ep.get(\"production_code\"),\n",
    "            \"last_episode_to_air_runtime\": last_ep.get(\"runtime\"),\n",
    "            \"last_episode_to_air_season_number\": last_ep.get(\"season_number\"),\n",
    "            \"last_episode_to_air_show_id\": last_ep.get(\"show_id\"),\n",
    "            \"last_episode_to_air_still_path\": last_ep.get(\"still_path\"),\n",
    "            \n",
    "            \"next_episode_to_air\": str(data.get(\"next_episode_to_air\")) if data.get(\"next_episode_to_air\") else \"\",\n",
    "            \"networks\": simple_list_to_str(data.get(\"networks\", [])),\n",
    "            \"number_of_episodes\": data.get(\"number_of_episodes\"),\n",
    "            \"number_of_seasons\": data.get(\"number_of_seasons\"),\n",
    "            \"origin_country\": \", \".join(data.get(\"origin_country\", [])) if data.get(\"origin_country\") else \"\",\n",
    "            \"original_language\": data.get(\"original_language\"),\n",
    "            \"original_name\": data.get(\"original_name\"),\n",
    "            \"overview\": data.get(\"overview\"),\n",
    "            \"popularity\": data.get(\"popularity\"),\n",
    "            \"production_companies\": simple_list_to_str(data.get(\"production_companies\", [])),\n",
    "            \"production_countries\": simple_list_to_str(data.get(\"production_countries\", [])),\n",
    "            \"seasons\": \"; \".join(\n",
    "                f\"S{s.get('season_number')}: {s.get('name')} ({s.get('episode_count')} eps)\"\n",
    "                for s in (data.get(\"seasons\") or [])\n",
    "                if s.get('season_number') is not None\n",
    "            ),\n",
    "            \"spoken_languages\": simple_list_to_str(data.get(\"spoken_languages\", [])),\n",
    "            \"status\": data.get(\"status\"),\n",
    "            \"tagline\": data.get(\"tagline\"),\n",
    "            \"type_detail\": data.get(\"type\"),\n",
    "            \"vote_average\": data.get(\"vote_average\"),\n",
    "            \"vote_count\": data.get(\"vote_count\"),\n",
    "        }\n",
    "\n",
    "        # ===== ë¦¬ë·° (reviews) =====\n",
    "        reviews_block = data.get(\"reviews\") or {}\n",
    "        reviews = reviews_block.get(\"results\", []) if isinstance(reviews_block, dict) else []\n",
    "        all_reviews = []\n",
    "        for review in reviews[:5]:\n",
    "            author = review.get(\"author\", \"unknown\")\n",
    "            rating = review.get(\"author_details\", {}).get(\"rating\")\n",
    "            rating_str = f\" (Rating: {rating})\" if rating is not None else \"\"\n",
    "            content = review.get(\"content\", \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \")[:200]\n",
    "            all_reviews.append(f\"{author}{rating_str}: {content}\")\n",
    "        record[\"review\"] = \" || \".join(all_reviews) if all_reviews else \"\"\n",
    "\n",
    "        # ===== í‚¤ì›Œë“œ (keywords) =====\n",
    "        keywords_block = data.get(\"keywords\") or {}\n",
    "        keywords = keywords_block.get(\"results\", []) if isinstance(keywords_block, dict) else []\n",
    "        record[\"keyword\"] = \", \".join([k.get(\"name\", \"\") for k in keywords[:20] if k.get(\"name\")])\n",
    "\n",
    "        # ===== aggregate_credits (top_cast / directors / writers) =====\n",
    "        credits = data.get(\"aggregate_credits\") or {}\n",
    "        cast = credits.get(\"cast\", []) if isinstance(credits, dict) else []\n",
    "        crew = credits.get(\"crew\", []) if isinstance(credits, dict) else []\n",
    "\n",
    "        # ì£¼ì—° ë°°ìš° 4ëª…\n",
    "        record[\"top_cast\"] = \", \".join([c.get(\"name\", \"\") for c in cast[:4] if c.get(\"name\")])\n",
    "\n",
    "        directors_set = set()\n",
    "        writers_set = set()\n",
    "        for c in crew:\n",
    "            name = c.get(\"name\", \"\")\n",
    "            if not name:\n",
    "                continue\n",
    "            jobs = c.get(\"jobs\", []) or []\n",
    "            for job in jobs:\n",
    "                job_name = job.get(\"job\", \"\") or \"\"\n",
    "                if \"Director\" in job_name or \"Series Director\" in job_name:\n",
    "                    directors_set.add(name)\n",
    "                if job_name in [\"Writer\", \"Screenplay\", \"Story\", \"Writer's Assistant\"]:\n",
    "                    writers_set.add(name)\n",
    "\n",
    "        record[\"directors\"] = \", \".join(sorted(directors_set)[:10])\n",
    "        record[\"writers\"] = \", \".join(sorted(writers_set)[:10])\n",
    "\n",
    "        # ===== providers (ë³„ë„ ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ) =====\n",
    "        record[\"providers\"] = fetch_providers(series_id)\n",
    "\n",
    "        return record\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ìƒì„¸ ì •ë³´ ì²˜ë¦¬ ì˜¤ë¥˜ (ID: {series_id}): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 8. MAIN ì‹¤í–‰ë¶€\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 90)\n",
    "    print(\"ğŸš€ TMDB ë“œë¼ë§ˆ ì „ì²´ ìˆ˜ì§‘ (2005-01-01 ~ 2015-12-31)\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    start_date = \"2005-01-01\"\n",
    "    end_date = \"2015-12-31\"\n",
    "    total_start = time.time()\n",
    "\n",
    "    print(f\"\\nğŸ“… ì „ì²´ ê¸°ê°„: {start_date} ~ {end_date}\")\n",
    "    print(f\"âš¡ Rate Limit: ì´ˆë‹¹ 25 ìš”ì²­ (TMDB ì œí•œ 40 ëŒ€ë¹„ ì—¬ìœ )\")\n",
    "    print(f\"âš ï¸ discover/tv 500í˜ì´ì§€ ì œí•œ â†’ ì¬ê·€ì  ë‚ ì§œ ë¶„í• ë¡œ 100% ëˆ„ë½ ë°©ì§€\")\n",
    "    print(\"=\"*90)\n",
    "\n",
    "    # 1) ID ì „ì²´ ìˆ˜ì§‘ (ì¬ê·€ ë¶„í• )\n",
    "    print(\"\\nğŸ“Œ 1ë‹¨ê³„: TV ì‹œë¦¬ì¦ˆ ID ì „ì²´ ìˆ˜ì§‘ ì‹œì‘\")\n",
    "    all_tv_ids = collect_ids_in_range(start_date, end_date)\n",
    "\n",
    "    if not all_tv_ids:\n",
    "        print(\"\\nâš ï¸ ìˆ˜ì§‘ëœ IDê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        raise SystemExit\n",
    "\n",
    "    tv_list = sorted(list(all_tv_ids))\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"âœ¨ ì´ ê³ ìœ  ID: {len(tv_list):,}ê°œ\")\n",
    "\n",
    "    # 2) ìƒì„¸ ì •ë³´ ìˆ˜ì§‘\n",
    "    print(f\"\\nğŸ“Œ 2ë‹¨ê³„: ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...\")\n",
    "    print(f\"âš¡ Workers: 6ê°œ (ì•ˆì „í•œ ë™ì‹œì„±)\")\n",
    "    print(f\"âš¡ Rate Limit: ì´ˆë‹¹ 25 ìš”ì²­\")\n",
    "    print(f\"â±  ëŒ€ëµì ì¸ ìµœì†Œ ì´ë¡ ì¹˜: {len(tv_list) / 25 / 60:.1f}ë¶„ (IDë‹¹ 1~2íšŒ í˜¸ì¶œ í¬í•¨)\")\n",
    "    print(f\"ğŸ’¾ ìë™ ë°±ì—…: ë§¤ 1000ê°œ ë˜ëŠ” 5ë¶„ë§ˆë‹¤\")\n",
    "    print(\"=\"*90 + \"\\n\")\n",
    "\n",
    "    results = []\n",
    "    failed_ids = []\n",
    "    last_save_time = time.time()\n",
    "    save_interval = 300  # 5ë¶„\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        futures = {executor.submit(fetch_tv_details, sid): sid for sid in tv_list}\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"TV ìƒì„¸ ìˆ˜ì§‘\"):\n",
    "            series_id = futures[future]\n",
    "            try:\n",
    "                detail = future.result()\n",
    "                if detail:\n",
    "                    results.append(detail)\n",
    "                else:\n",
    "                    failed_ids.append(series_id)\n",
    "                \n",
    "                # ì¤‘ê°„ ì €ì¥\n",
    "                current_time = time.time()\n",
    "                if (len(results) % 1000 == 0 and len(results) > 0) or \\\n",
    "                   (current_time - last_save_time > save_interval and len(results) > 0):\n",
    "                    df_temp = pd.DataFrame(results)\n",
    "                    backup_file = f\"backup_{len(results)}_{datetime.now().strftime('%H%M')}.csv\"\n",
    "                    df_temp.to_csv(backup_file, index=False, encoding=\"utf-8-sig\")\n",
    "                    print(f\"\\nğŸ’¾ ë°±ì—… ì €ì¥: {backup_file} ({len(results):,}ê°œ)\")\n",
    "                    last_save_time = current_time\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nìˆ˜ì§‘ ì˜¤ë¥˜ (ID: {series_id}): {e}\")\n",
    "                failed_ids.append(series_id)\n",
    "\n",
    "    print(f\"\\nğŸ‰ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(results):,}ê°œ\")\n",
    "    if failed_ids:\n",
    "        print(f\"âš ï¸  ì‹¤íŒ¨í•œ ID: {len(failed_ids)}ê°œ\")\n",
    "        with open(\"failed_ids.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(map(str, failed_ids)))\n",
    "\n",
    "    # 3) ìµœì¢… ì €ì¥\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "\n",
    "        # ì •ë ¬\n",
    "        if \"first_air_date\" in df.columns and \"popularity\" in df.columns:\n",
    "            df = df.sort_values([\"first_air_date\", \"popularity\"], ascending=[True, False])\n",
    "\n",
    "        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬ (name, created by, poster_path ì œì™¸)\n",
    "        column_order = [\n",
    "            \"id\",\n",
    "            \"title\",\n",
    "            \"type\",\n",
    "            \"adult\",\n",
    "            \"backdrop_path\",\n",
    "            \"created_by\",\n",
    "            \"episode_run_time\",\n",
    "            \"first_air_date\",\n",
    "            \"genres\",\n",
    "            \"genre_ids\",\n",
    "            \"homepage\",\n",
    "            \"in_production\",\n",
    "            \"languages\",\n",
    "            \"last_air_date\",\n",
    "            \"last_episode_to_air_id\",\n",
    "            \"last_episode_to_air_name\",\n",
    "            \"last_episode_to_air_overview\",\n",
    "            \"last_episode_to_air_vote_average\",\n",
    "            \"last_episode_to_air_vote_count\",\n",
    "            \"last_episode_to_air_air_date\",\n",
    "            \"last_episode_to_air_episode_number\",\n",
    "            \"last_episode_to_air_production_code\",\n",
    "            \"last_episode_to_air_runtime\",\n",
    "            \"last_episode_to_air_season_number\",\n",
    "            \"last_episode_to_air_show_id\",\n",
    "            \"last_episode_to_air_still_path\",\n",
    "            \"next_episode_to_air\",\n",
    "            \"networks\",\n",
    "            \"number_of_episodes\",\n",
    "            \"number_of_seasons\",\n",
    "            \"origin_country\",\n",
    "            \"original_language\",\n",
    "            \"original_name\",\n",
    "            \"overview\",\n",
    "            \"popularity\",\n",
    "            \"production_companies\",\n",
    "            \"production_countries\",\n",
    "            \"seasons\",\n",
    "            \"spoken_languages\",\n",
    "            \"status\",\n",
    "            \"tagline\",\n",
    "            \"type_detail\",\n",
    "            \"vote_average\",\n",
    "            \"vote_count\",\n",
    "            \"review\",\n",
    "            \"keyword\",\n",
    "            \"top_cast\",\n",
    "            \"directors\",\n",
    "            \"writers\",\n",
    "            \"providers\",\n",
    "        ]\n",
    "        \n",
    "        existing_columns = [col for col in column_order if col in df.columns]\n",
    "        df = df[existing_columns]\n",
    "        \n",
    "        output_file = \"tv_series_2005_2015_FULL.csv\"\n",
    "        df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        elapsed = time.time() - total_start\n",
    "\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"========================== DONE ==========================\")\n",
    "        print(f\"ğŸ“Œ ì´ ë°ì´í„°: {len(df):,}ê°œ\")\n",
    "        print(f\"ğŸ“Š ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ\")\n",
    "        print(f\"â±  ì†Œìš”ì‹œê°„: {elapsed/60:.2f}ë¶„ ({elapsed/3600:.2f}ì‹œê°„)\")\n",
    "        print(f\"ğŸ’¾ ì €ì¥ íŒŒì¼: {output_file}\")\n",
    "        print(f\"ğŸ“ˆ í‰ê·  ì²˜ë¦¬ ì†ë„: {len(df)/(elapsed/60):.1f}ê°œ/ë¶„\")\n",
    "        if failed_ids:\n",
    "            print(f\"âš ï¸  ì‹¤íŒ¨í•œ ID: {len(failed_ids)}ê°œ (failed_ids.txt ì°¸ì¡°)\")\n",
    "        \n",
    "        # ê°„ë‹¨ ìš”ì•½\n",
    "        if \"first_air_date\" in df.columns:\n",
    "            print(\"\\n[ë°ì´í„° ìš”ì•½]\")\n",
    "            print(f\"- ì—°ë„ ë²”ìœ„: {df['first_air_date'].min()} ~ {df['first_air_date'].max()}\")\n",
    "        if \"popularity\" in df.columns:\n",
    "            print(f\"- í‰ê·  ì¸ê¸°ë„: {df['popularity'].mean():.2f}\")\n",
    "        if \"vote_average\" in df.columns:\n",
    "            print(f\"- í‰ê·  í‰ì : {df['vote_average'].mean():.2f}\")\n",
    "        if \"number_of_episodes\" in df.columns:\n",
    "            print(f\"- ì´ ì—í”¼ì†Œë“œ: {df['number_of_episodes'].sum():,.0f}ê°œ\")\n",
    "\n",
    "        print(\"=\"*90)\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ ìˆ˜ì§‘ëœ ìƒì„¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìˆ˜ì§‘ ê²°ê³¼ ì ê²€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Œ TMDB TV DATASET VALIDATION REPORT\n",
      "================================================================================\n",
      "\n",
      "[1] ì»¬ëŸ¼ ê²€ì¦\n",
      "- ì´ ì»¬ëŸ¼ ìˆ˜: 53\n",
      "- ëˆ„ë½ëœ ì»¬ëŸ¼: []\n",
      "- ì¶”ê°€ëœ(ì˜ˆìƒ ì™¸) ì»¬ëŸ¼: ['created by']\n",
      "\n",
      "[2] ê²°ì¸¡ì¹˜ ë¹„ìœ¨ TOP 20\n",
      "last_episode_to_air_production_code    0.997795\n",
      "next_episode_to_air                    0.987767\n",
      "review                                 0.981474\n",
      "tagline                                0.949235\n",
      "writers                                0.803776\n",
      "providers                              0.787324\n",
      "created by                             0.781041\n",
      "created_by                             0.781041\n",
      "keyword                                0.695671\n",
      "last_episode_to_air_still_path         0.694201\n",
      "last_episode_to_air_overview           0.679752\n",
      "directors                              0.675362\n",
      "episode_run_time                       0.622211\n",
      "homepage                               0.593949\n",
      "production_companies                   0.563794\n",
      "last_episode_to_air_runtime            0.495258\n",
      "overview                               0.442207\n",
      "production_countries                   0.436881\n",
      "networks                               0.392308\n",
      "top_cast                               0.384223\n",
      "dtype: float64\n",
      "\n",
      "[3] ID ì¤‘ë³µ ì²´í¬\n",
      "- ì¤‘ë³µ ID ìˆ˜: 0\n",
      "\n",
      "[4] ë‚ ì§œí˜• ì»¬ëŸ¼ ê²€ì¦\n",
      "- first_air_date: íŒŒì‹± ì‹¤íŒ¨ 4ê°œ\n",
      "- last_air_date: íŒŒì‹± ì‹¤íŒ¨ 369ê°œ\n",
      "- last_episode_to_air_air_date: íŒŒì‹± ì‹¤íŒ¨ 12ê°œ\n",
      "\n",
      "[5] ìˆ«ìí˜• ì»¬ëŸ¼ ë²”ìœ„ ê²€ì¦\n",
      "\n",
      "[popularity]\n",
      "count    99320.000000\n",
      "mean         0.834768\n",
      "std          4.499726\n",
      "min          0.000000\n",
      "25%          0.036200\n",
      "50%          0.296600\n",
      "75%          0.878025\n",
      "max       1204.502000\n",
      "Name: popularity, dtype: float64\n",
      "\n",
      "[vote_average]\n",
      "count    99320.000000\n",
      "mean         2.667808\n",
      "std          3.576350\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          6.500000\n",
      "max         10.000000\n",
      "Name: vote_average, dtype: float64\n",
      "\n",
      "[vote_count]\n",
      "count    99320.000000\n",
      "mean        14.996818\n",
      "std        206.564310\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max      19244.000000\n",
      "Name: vote_count, dtype: float64\n",
      "\n",
      "[number_of_episodes]\n",
      "count    99111.000000\n",
      "mean        18.110805\n",
      "std         47.716653\n",
      "min          0.000000\n",
      "25%          2.000000\n",
      "50%          7.000000\n",
      "75%         17.000000\n",
      "max       2695.000000\n",
      "Name: number_of_episodes, dtype: float64\n",
      "\n",
      "[number_of_seasons]\n",
      "count    99320.000000\n",
      "mean         1.398540\n",
      "std          1.398505\n",
      "min          0.000000\n",
      "25%          1.000000\n",
      "50%          1.000000\n",
      "75%          1.000000\n",
      "max         80.000000\n",
      "Name: number_of_seasons, dtype: float64\n",
      "\n",
      "[6] ì „ì²´ í†µê³„ ìš”ì•½\n",
      "                          count unique                               top  \\\n",
      "id                      99320.0    NaN                               NaN   \n",
      "title                     99320  96553                        The Family   \n",
      "type                      99320      1                         tv_series   \n",
      "adult                     99320      1                             False   \n",
      "backdrop_path             62949  62870  /rGf6hakFofQy1eDODzjjmOS70pB.jpg   \n",
      "created_by                21747  18072                     Na Yeong-seok   \n",
      "episode_run_time          37522   1043                                45   \n",
      "first_air_date            99316   3621                        2018-01-01   \n",
      "genres                    66349   1753                       Documentary   \n",
      "genre_ids                 66349   1753                                99   \n",
      "homepage                  40329  39987                   http://ullu.app   \n",
      "in_production             99320      2                              True   \n",
      "languages                 69840   1139                                en   \n",
      "last_air_date             98951   3622                        2024-11-19   \n",
      "last_episode_to_air_id  99308.0    NaN                               NaN   \n",
      "\n",
      "                         freq            mean             std      min  \\\n",
      "id                        NaN   195544.018455    76290.691175    767.0   \n",
      "title                      10             NaN             NaN      NaN   \n",
      "type                    99320             NaN             NaN      NaN   \n",
      "adult                   99320             NaN             NaN      NaN   \n",
      "backdrop_path               5             NaN             NaN      NaN   \n",
      "created_by                 37             NaN             NaN      NaN   \n",
      "episode_run_time         4891             NaN             NaN      NaN   \n",
      "first_air_date            288             NaN             NaN      NaN   \n",
      "genres                  13445             NaN             NaN      NaN   \n",
      "genre_ids               13445             NaN             NaN      NaN   \n",
      "homepage                   31             NaN             NaN      NaN   \n",
      "in_production           60301             NaN             NaN      NaN   \n",
      "languages               15614             NaN             NaN      NaN   \n",
      "last_air_date             236             NaN             NaN      NaN   \n",
      "last_episode_to_air_id    NaN  4439168.517531  1544860.734211  52979.0   \n",
      "\n",
      "                               25%        50%        75%        max  \n",
      "id                       119723.75   216789.5  259356.25   306917.0  \n",
      "title                          NaN        NaN        NaN        NaN  \n",
      "type                           NaN        NaN        NaN        NaN  \n",
      "adult                          NaN        NaN        NaN        NaN  \n",
      "backdrop_path                  NaN        NaN        NaN        NaN  \n",
      "created_by                     NaN        NaN        NaN        NaN  \n",
      "episode_run_time               NaN        NaN        NaN        NaN  \n",
      "first_air_date                 NaN        NaN        NaN        NaN  \n",
      "genres                         NaN        NaN        NaN        NaN  \n",
      "genre_ids                      NaN        NaN        NaN        NaN  \n",
      "homepage                       NaN        NaN        NaN        NaN  \n",
      "in_production                  NaN        NaN        NaN        NaN  \n",
      "languages                      NaN        NaN        NaN        NaN  \n",
      "last_air_date                  NaN        NaN        NaN        NaN  \n",
      "last_episode_to_air_id  3253729.25  4650636.0  5749643.0  6725393.0  \n",
      "\n",
      "[7] ë¬¸ìì—´ í•„ë“œ ê°„ë‹¨ ê²€ì¦\n",
      "- genres: ê³ ìœ ê°’ 1753ê°œ, ë¹ˆ ë¬¸ìì—´ ìˆ˜ 0\n",
      "- keyword: ê³ ìœ ê°’ 23516ê°œ, ë¹ˆ ë¬¸ìì—´ ìˆ˜ 0\n",
      "- providers: ê³ ìœ ê°’ 2420ê°œ, ë¹ˆ ë¬¸ìì—´ ìˆ˜ 0\n",
      "- top_cast: ê³ ìœ ê°’ 57907ê°œ, ë¹ˆ ë¬¸ìì—´ ìˆ˜ 0\n",
      "\n",
      "ğŸ‰ ë°ì´í„° ê²€ì¦ ì™„ë£Œ!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('tv_series_2016_2025_FULL.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Œ TMDB TV DATASET VALIDATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) ì»¬ëŸ¼ 53ê°œ ì¡´ì¬ ì—¬ë¶€ ì²´í¬\n",
    "# -------------------------------------------------------------------\n",
    "expected_cols = [\n",
    "    \"id\", \"title\", \"type\", \"adult\", \"backdrop_path\", \"created_by\",\n",
    "    \"episode_run_time\", \"first_air_date\", \"genres\", \"genre_ids\",\n",
    "    \"homepage\", \"in_production\", \"languages\", \"last_air_date\",\n",
    "    \"last_episode_to_air_id\", \"last_episode_to_air_name\",\n",
    "    \"last_episode_to_air_overview\", \"last_episode_to_air_vote_average\",\n",
    "    \"last_episode_to_air_vote_count\", \"last_episode_to_air_air_date\",\n",
    "    \"last_episode_to_air_episode_number\", \"last_episode_to_air_production_code\",\n",
    "    \"last_episode_to_air_runtime\", \"last_episode_to_air_season_number\",\n",
    "    \"last_episode_to_air_show_id\", \"last_episode_to_air_still_path\",\n",
    "    \"name\", \"next_episode_to_air\", \"networks\", \"number_of_episodes\",\n",
    "    \"number_of_seasons\", \"origin_country\", \"original_language\",\n",
    "    \"original_name\", \"overview\", \"popularity\", \"poster_path\",\n",
    "    \"production_companies\", \"production_countries\", \"seasons\",\n",
    "    \"spoken_languages\", \"status\", \"tagline\", \"type_detail\",\n",
    "    \"vote_average\", \"vote_count\", \"review\", \"keyword\",\n",
    "    \"top_cast\", \"directors\", \"writers\", \"providers\"\n",
    "]\n",
    "\n",
    "missing = [c for c in expected_cols if c not in df.columns]\n",
    "extra = [c for c in df.columns if c not in expected_cols]\n",
    "\n",
    "print(\"\\n[1] ì»¬ëŸ¼ ê²€ì¦\")\n",
    "print(f\"- ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}\")\n",
    "print(f\"- ëˆ„ë½ëœ ì»¬ëŸ¼: {missing}\")\n",
    "print(f\"- ì¶”ê°€ëœ(ì˜ˆìƒ ì™¸) ì»¬ëŸ¼: {extra}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ì²´í¬\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n[2] ê²°ì¸¡ì¹˜ ë¹„ìœ¨ TOP 20\")\n",
    "na_rate = df.isna().mean().sort_values(ascending=False)\n",
    "print(na_rate.head(20))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) ID ì¤‘ë³µ ì²´í¬\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n[3] ID ì¤‘ë³µ ì²´í¬\")\n",
    "dup = df[\"id\"].duplicated().sum()\n",
    "print(f\"- ì¤‘ë³µ ID ìˆ˜: {dup}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) ë‚ ì§œ ì»¬ëŸ¼ ê²€ì¦\n",
    "# -------------------------------------------------------------------\n",
    "date_cols = [\"first_air_date\", \"last_air_date\", \"last_episode_to_air_air_date\"]\n",
    "\n",
    "print(\"\\n[4] ë‚ ì§œí˜• ì»¬ëŸ¼ ê²€ì¦\")\n",
    "for col in date_cols:\n",
    "    try:\n",
    "        parsed = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        nulls = parsed.isna().sum()\n",
    "        print(f\"- {col}: íŒŒì‹± ì‹¤íŒ¨ {nulls}ê°œ\")\n",
    "    except Exception as e:\n",
    "        print(f\"- {col}: ì˜¤ë¥˜ ë°œìƒ {e}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) ìˆ«ìí˜• ì»¬ëŸ¼ ë²”ìœ„ ê²€ì¦\n",
    "# -------------------------------------------------------------------\n",
    "num_cols = [\"popularity\", \"vote_average\", \"vote_count\",\n",
    "            \"number_of_episodes\", \"number_of_seasons\"]\n",
    "\n",
    "print(\"\\n[5] ìˆ«ìí˜• ì»¬ëŸ¼ ë²”ìœ„ ê²€ì¦\")\n",
    "for col in num_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n[{col}]\")\n",
    "        print(df[col].describe())\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6) ê¸°ì´ˆ í†µê³„ ê²€ì¦\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n[6] ì „ì²´ í†µê³„ ìš”ì•½\")\n",
    "print(df.describe(include=\"all\").transpose().head(15))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7) ë¬¸ìì—´ ì£¼ìš” í•„ë“œ ê²€ì¦\n",
    "# -------------------------------------------------------------------\n",
    "string_cols = [\"genres\", \"keyword\", \"providers\", \"top_cast\"]\n",
    "\n",
    "print(\"\\n[7] ë¬¸ìì—´ í•„ë“œ ê°„ë‹¨ ê²€ì¦\")\n",
    "for col in string_cols:\n",
    "    uniq = df[col].nunique(dropna=True)\n",
    "    empty = (df[col] == \"\").sum() if col in df else None\n",
    "    print(f\"- {col}: ê³ ìœ ê°’ {uniq}ê°œ, ë¹ˆ ë¬¸ìì—´ ìˆ˜ {empty}\")\n",
    "\n",
    "print(\"\\nğŸ‰ ë°ì´í„° ê²€ì¦ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
