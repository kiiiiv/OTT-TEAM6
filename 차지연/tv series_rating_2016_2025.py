# ==========================================================
# IMDB FULL DATA COLLECTOR
# - GraphQL Reviews (ì „ì²´ í˜ì´ì§€)
# - IMDB Rating (JSON-LD)
# - Metascore (JSON-LD ë˜ëŠ” <span class="score-meta">)
# - ëŒ€ìƒ: imdb_id ë³´ìœ  & vote_count >= 30 TV Series
#   ì…ë ¥ íŒŒì¼: tv_series_2016_2025_FULL.csv
# ==========================================================

import asyncio
import aiohttp
import pandas as pd
import json
from datetime import datetime
from pathlib import Path
import time
from urllib.parse import quote
import html
import re
import argparse

# ==========================================================
# ì„¤ì •
# ==========================================================

# GraphQL API ì„¤ì •
GRAPHQL_URL = "https://caching.graphql.imdb.com/"
OPERATION_NAME = "TitleReviewsRefine"
PERSISTED_QUERY_HASH = "d389bc70c27f09c00b663705f0112254e8a7c75cde1cfd30e63a2d98c1080c87"

# Rate Limiting (GraphQL + HTML í†µí•©)
MAX_CALLS_PER_SECOND = 2
TIMEOUT = aiohttp.ClientTimeout(total=30, connect=10)
MAX_RETRIES = 3

# í•œ ë²ˆì— ê°€ì ¸ì˜¬ ë¦¬ë·° ìˆ˜
REVIEWS_PER_REQUEST = 25

# User-Agent
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Safari/537.36"
)

# ì¶œë ¥ íŒŒì¼
OUTPUT_REVIEWS_CSV = "imdb_reviews_graphql.csv"
OUTPUT_TITLES_CSV = "imdb_title_stats.csv"
OUTPUT_REVIEWS_PARQUET = "imdb_reviews_graphql.parquet"
CHECKPOINT_FILE = "imdb_graphql_checkpoint.json"

# í†µê³„
stats = {
    "series_total": 0,
    "series_success": 0,
    "series_failed": 0,
    "reviews_total": 0,
    "requests": 0,
    "start_time": None
}

# ==========================================================
# Rate Limiter
# ==========================================================
class RateLimiter:
    def __init__(self, rate):
        self.rate = rate
        self.tokens = rate
        self.updated_at = time.monotonic()
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        async with self.lock:
            now = time.monotonic()
            elapsed = now - self.updated_at
            self.tokens = min(self.rate, self.tokens + elapsed * self.rate)
            self.updated_at = now
            
            if self.tokens < 1:
                sleep_time = (1 - self.tokens) / self.rate
                await asyncio.sleep(sleep_time)
                self.tokens = 1
            
            self.tokens -= 1

rate_limiter = RateLimiter(MAX_CALLS_PER_SECOND)

# ==========================================================
# ê³µí†µ HTTP í˜¸ì¶œ í•¨ìˆ˜
# ==========================================================
async def get_json(session, url, retry=0):
    """JSON ì‘ë‹µìš© (GraphQL ë“±)"""
    if retry >= MAX_RETRIES:
        return None
    
    await rate_limiter.acquire()
    stats["requests"] += 1

    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "application/json",
        "Content-Type": "application/json",
    }

    try:
        async with session.get(url, headers=headers, timeout=TIMEOUT) as resp:
            if resp.status == 429 and retry < MAX_RETRIES - 1:
                wait_time = 5 * (retry + 1)
                print(f"âš ï¸  Rate limited, waiting {wait_time}s...")
                await asyncio.sleep(wait_time)
                return await get_json(session, url, retry + 1)
            
            if resp.status != 200:
                if retry < MAX_RETRIES - 1:
                    await asyncio.sleep(2 ** retry)
                    return await get_json(session, url, retry + 1)
                return None
            
            return await resp.json()
    except asyncio.TimeoutError:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await get_json(session, url, retry + 1)
        return None
    except Exception:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await get_json(session, url, retry + 1)
        return None


async def get_html(session, url, retry=0):
    """IMDB title HTMLìš©"""
    if retry >= MAX_RETRIES:
        return None
    
    await rate_limiter.acquire()
    stats["requests"] += 1

    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
    }

    try:
        async with session.get(url, headers=headers, timeout=TIMEOUT) as resp:
            if resp.status == 429 and retry < MAX_RETRIES - 1:
                wait_time = 5 * (retry + 1)
                print(f"âš ï¸  HTML Rate limited, waiting {wait_time}s...")
                await asyncio.sleep(wait_time)
                return await get_html(session, url, retry + 1)
            
            if resp.status != 200:
                if retry < MAX_RETRIES - 1:
                    await asyncio.sleep(2 ** retry)
                    return await get_html(session, url, retry + 1)
                return None
            
            return await resp.text()
    except asyncio.TimeoutError:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await get_html(session, url, retry + 1)
        return None
    except Exception:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await get_html(session, url, retry + 1)
        return None

# ==========================================================
# GraphQL ì¿¼ë¦¬ ìƒì„± (ë¦¬ë·°)
# ==========================================================
def build_graphql_url(imdb_id, after_cursor=None, first=25, sort_by="HELPFULNESS_SCORE"):
    variables = {
        "const": imdb_id,
        "first": first,
        "locale": "en-US",
        "sort": {
            "by": sort_by,
            "order": "DESC"
        },
        "filter": {}
    }
    
    if after_cursor:
        variables["after"] = after_cursor
    
    extensions = {
        "persistedQuery": {
            "sha256Hash": PERSISTED_QUERY_HASH,
            "version": 1
        }
    }
    
    variables_json = json.dumps(variables, separators=(',', ':'))
    extensions_json = json.dumps(extensions, separators=(',', ':'))
    
    url = (
        f"{GRAPHQL_URL}?"
        f"operationName={OPERATION_NAME}"
        f"&variables={quote(variables_json)}"
        f"&extensions={quote(extensions_json)}"
    )
    
    return url

# ==========================================================
# ë¦¬ë·° íŒŒì‹±
# ==========================================================
def clean_html_text(text):
    if not text:
        return None
    text = html.unescape(text)
    text = text.replace('<br/>', '\n').replace('<br>', '\n')
    text = re.sub('<[^<]+?>', '', text)
    return text.strip()

def parse_review_node(node, imdb_id):
    try:
        # í…ìŠ¤íŠ¸
        text_data = node.get('text', {}).get('originalText', {})
        review_text_html = text_data.get('plaidHtml')
        review_text = clean_html_text(review_text_html)

        # helpful
        helpfulness = node.get('helpfulness', {})
        up_votes = helpfulness.get('upVotes', 0)
        down_votes = helpfulness.get('downVotes', 0)
        total_votes = up_votes + down_votes

        # ì‘ì„±ì
        author = node.get('author', {})
        username = author.get('username', {}).get('text')
        user_id = author.get('userId')

        # ë©”íƒ€ë°ì´í„°
        summary = node.get('summary', {})
        review_title = summary.get('originalText')

        return {
            'imdb_id': imdb_id,
            'review_id': node.get('id'),
            'username': username,
            'user_id': user_id,
            'author_rating': node.get('authorRating'),
            'helpful_up_votes': up_votes,
            'helpful_down_votes': down_votes,
            'helpful_total': total_votes,
            'helpful_ratio': round(up_votes / total_votes, 3) if total_votes > 0 else None,
            'submission_date': node.get('submissionDate'),
            'review_title': review_title,
            'review_text': review_text,
            'review_text_length': len(review_text) if review_text else 0,
            'is_spoiler': node.get('spoiler', False),
        }
    except Exception as e:
        print(f"âš ï¸  Error parsing review node: {e}")
        return None

# ==========================================================
# 1) í•œ ì‹œë¦¬ì¦ˆì˜ ë¦¬ë·° ì „ì²´ ìˆ˜ì§‘
# ==========================================================
async def fetch_all_reviews_for_series(session, imdb_id, series_title="", max_reviews=None):
    all_reviews = []
    after_cursor = None
    page = 0
    
    try:
        while True:
            page += 1
            url = build_graphql_url(imdb_id, after_cursor, REVIEWS_PER_REQUEST)
            response = await get_json(session, url)
            
            if not response:
                break
            
            data = response.get('data', {})
            title_data = data.get('title', {})
            reviews_data = title_data.get('reviews', {})
            total_reviews = reviews_data.get('total', 0)
            
            edges = reviews_data.get('edges', [])
            if not edges:
                break
            
            for edge in edges:
                node = edge.get('node', {})
                review = parse_review_node(node, imdb_id)
                if review:
                    all_reviews.append(review)
            
            if max_reviews and len(all_reviews) >= max_reviews:
                all_reviews = all_reviews[:max_reviews]
                break
            
            page_info = reviews_data.get('pageInfo', {})
            has_next_page = page_info.get('hasNextPage', False)
            after_cursor = page_info.get('endCursor')
            
            if not has_next_page or not after_cursor:
                break
            
            await asyncio.sleep(0.1)
        
        stats["reviews_total"] += len(all_reviews)
        
        if all_reviews:
            print(f"âœ… [Reviews] {series_title} ({imdb_id}): {len(all_reviews):,}/{total_reviews}ê°œ ìˆ˜ì§‘")
        
        return all_reviews
    
    except Exception as e:
        print(f"âŒ [Reviews] {series_title} ({imdb_id}): {str(e)[:100]}")
        return all_reviews

# ==========================================================
# 2) IMDB HTMLì—ì„œ Rating + Metascore ì¶”ì¶œ
# ==========================================================
def parse_title_stats_from_html(imdb_id, html_text):
    """
    IMDB title HTMLì—ì„œ
    - ratingValue, ratingCount (JSON-LD)
    - metascore (metacritic.score ë˜ëŠ” <span class="score-meta">)
    ë¥¼ ì¶”ì¶œ
    """
    imdb_rating = None
    imdb_rating_count = None
    metascore = None

    # 1) JSON-LD ë¸”ë¡ ì¶”ì¶œ
    ld_match = re.search(
        r'<script type="application/ld\+json">(.*?)</script>',
        html_text,
        re.S
    )
    if ld_match:
        try:
            data = json.loads(ld_match.group(1))
            agg = data.get("aggregateRating", {})
            imdb_rating = agg.get("ratingValue")
            imdb_rating_count = agg.get("ratingCount")

            # ì¼ë¶€ í˜ì´ì§€ëŠ” JSON ì•ˆì— metacritic ì •ë³´ í¬í•¨
            mc = data.get("metacritic") or {}
            if isinstance(mc, dict):
                metascore = mc.get("score", metascore)
        except Exception as e:
            print(f"âš ï¸  JSON-LD parse error ({imdb_id}): {e}")

    # 2) HTML span.score-meta Fallback
    if metascore is None:
        ms_match = re.search(r'<span class="score-meta">(\d+)</span>', html_text)
        if ms_match:
            metascore = ms_match.group(1)

    return {
        "imdb_id": imdb_id,
        "imdb_rating": imdb_rating,
        "imdb_rating_count": imdb_rating_count,
        "metascore": metascore,
    }

async def fetch_imdb_title_stats(session, imdb_id):
    url = f"https://www.imdb.com/title/{imdb_id}/"
    html_text = await get_html(session, url)
    if not html_text:
        print(f"âš ï¸  [Title] {imdb_id}: HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
        return {
            "imdb_id": imdb_id,
            "imdb_rating": None,
            "imdb_rating_count": None,
            "metascore": None,
        }
    return parse_title_stats_from_html(imdb_id, html_text)

# ==========================================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ==========================================================
def save_checkpoint(processed_ids):
    stats_copy = stats.copy()
    if stats_copy.get('start_time'):
        stats_copy['start_time'] = stats_copy['start_time'].isoformat()
    
    checkpoint = {
        'processed_ids': list(processed_ids),
        'stats': stats_copy,
        'timestamp': datetime.now().isoformat()
    }
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(checkpoint, f)

def load_checkpoint():
    processed_ids = set()
    
    # 1) ì²´í¬í¬ì¸íŠ¸ íŒŒì¼
    if Path(CHECKPOINT_FILE).exists():
        try:
            with open(CHECKPOINT_FILE, 'r') as f:
                checkpoint = json.load(f)
                processed_ids.update(checkpoint.get('processed_ids', []))
                print(f"ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ {len(checkpoint.get('processed_ids', [])):,}ê°œ ID ë¡œë“œ")
        except (json.JSONDecodeError, Exception) as e:
            print(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì†ìƒë¨, ì‚­ì œí•˜ê³  ì§„í–‰: {e}")
            try:
                Path(CHECKPOINT_FILE).unlink()
            except:
                pass
    
    # 2) ê¸°ì¡´ ë¦¬ë·° CSV ê¸°ì¤€ (ì´ë¯¸ ìˆ˜ì§‘ëœ imdb_id)
    if Path(OUTPUT_REVIEWS_CSV).exists():
        try:
            df_existing = pd.read_csv(OUTPUT_REVIEWS_CSV)
            if 'imdb_id' in df_existing.columns:
                existing_ids = df_existing['imdb_id'].unique()
                processed_ids.update(existing_ids)
                print(f"ğŸ“Œ ê¸°ì¡´ ë¦¬ë·° CSVì—ì„œ {len(existing_ids):,}ê°œ ì‹œë¦¬ì¦ˆ ë°œê²¬")
        except Exception as e:
            print(f"âš ï¸  ê¸°ì¡´ ë¦¬ë·° CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return processed_ids

# ==========================================================
# ë©”ì¸ ì‹¤í–‰
# ==========================================================
async def main(input_csv_path, vote_threshold=30, max_reviews_per_series=None):
    print("=" * 90)
    print("ğŸš€ IMDB FULL DATA COLLECTOR (Reviews + Rating + Metascore)")
    print("=" * 90)
    
    stats["start_time"] = datetime.now()
    t0 = datetime.now()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv(input_csv_path)
    df_filtered = df[(df['vote_count'] >= vote_threshold) & (df['imdb_id'].notna())]
    df_filtered = df_filtered.drop_duplicates(subset=['imdb_id'])
    
    print(f"âœ… ì „ì²´ ì‹œë¦¬ì¦ˆ: {len(df):,}ê°œ")
    print(f"âœ… í•„í„°ë§ (vote_count>={vote_threshold} & imdb_id ì¡´ì¬): {len(df_filtered):,}ê°œ")
    
    if len(df_filtered) == 0:
        print("âš ï¸  ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    processed_ids = load_checkpoint()
    series_list = df_filtered[['id', 'title', 'imdb_id']].to_dict('records')
    
    if processed_ids:
        print(f"ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ ê¸°ì¤€ ì´ë¯¸ ì²˜ë¦¬ëœ ì‹œë¦¬ì¦ˆ: {len(processed_ids):,}ê°œ")
        series_list = [s for s in series_list if s['imdb_id'] not in processed_ids]
        print(f"ğŸ“Œ ë‚¨ì€ ì‘ì—…: {len(series_list):,}ê°œ")
    
    if len(series_list) == 0:
        print("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    stats["series_total"] = len(series_list)
    
    # 3. í¬ë¡¤ë§ ì„¤ì •
    print(f"\nğŸš€ í¬ë¡¤ë§ ì‹œì‘")
    print(f"âš™ï¸  Rate Limit: {MAX_CALLS_PER_SECOND}íšŒ/ì´ˆ")
    print(f"âš™ï¸  ë¦¬ë·°/ìš”ì²­: {REVIEWS_PER_REQUEST}ê°œ")
    
    if max_reviews_per_series:
        print(f"âš™ï¸  ì‹œë¦¬ì¦ˆë‹¹ ìµœëŒ€ ë¦¬ë·°: {max_reviews_per_series}ê°œ")
        estimated_time = len(series_list) * (max_reviews_per_series / REVIEWS_PER_REQUEST / MAX_CALLS_PER_SECOND) / 60
    else:
        print(f"âš™ï¸  ì‹œë¦¬ì¦ˆë‹¹ ìµœëŒ€ ë¦¬ë·°: ì „ì²´")
        estimated_time = len(series_list) * 5  # ëŒ€ëµ ì¶”ì •
    
    print(f"â±ï¸  ëŸ¬í”„ ì˜ˆìƒ ì‹œê°„: {estimated_time:.0f}ë¶„")
    
    connector = aiohttp.TCPConnector(
        limit=20,
        force_close=False,
        enable_cleanup_closed=True
    )
    
    all_reviews_results = []
    all_title_stats_results = []
    batch_size = 10
    
    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as session:
        for i in range(0, len(series_list), batch_size):
            batch = series_list[i:i+batch_size]
            
            # ê° ì‹œë¦¬ì¦ˆë³„ (ë¦¬ë·° + íƒ€ì´í‹€ í†µê³„) ë™ì‹œ ì²˜ë¦¬
            async def process_series(s):
                imdb_id = s['imdb_id']
                title = s['title']
                reviews, title_stats = await asyncio.gather(
                    fetch_all_reviews_for_series(
                        session,
                        imdb_id,
                        title,
                        max_reviews_per_series
                    ),
                    fetch_imdb_title_stats(session, imdb_id)
                )
                return s, reviews, title_stats
            
            tasks = [process_series(s) for s in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in batch_results:
                if isinstance(result, Exception):
                    stats["series_failed"] += 1
                    continue
                
                series, reviews, title_stats = result
                imdb_id = series['imdb_id']
                
                # ë¦¬ë·° ìˆ˜ì§‘ ê²°ê³¼
                if isinstance(reviews, list) and reviews:
                    all_reviews_results.extend(reviews)
                    processed_ids.add(imdb_id)
                    stats["series_success"] += 1
                elif isinstance(reviews, list):
                    # ë¦¬ë·° 0ê°œì§€ë§Œ ì‹œë¦¬ì¦ˆ ìì²´ëŠ” ì²˜ë¦¬ ì™„ë£Œë¡œ ê°„ì£¼
                    processed_ids.add(imdb_id)
                    stats["series_success"] += 1
                else:
                    stats["series_failed"] += 1
                
                # íƒ€ì´í‹€ í†µê³„ ê²°ê³¼
                if isinstance(title_stats, dict):
                    all_title_stats_results.append(title_stats)
            
            # ì£¼ê¸°ì  ì €ì¥
            if (i + batch_size) % 50 == 0:
                # ì²´í¬í¬ì¸íŠ¸
                save_checkpoint(processed_ids)
                
                # ë¦¬ë·° ì¤‘ê°„ ì €ì¥
                if all_reviews_results:
                    df_batch = pd.DataFrame(all_reviews_results)
                    df_batch = df_batch.drop_duplicates(subset=['review_id'])
                    file_exists = Path(OUTPUT_REVIEWS_CSV).exists()
                    df_batch.to_csv(
                        OUTPUT_REVIEWS_CSV,
                        mode='a',
                        header=not file_exists,
                        index=False,
                        encoding='utf-8-sig'
                    )
                    all_reviews_results.clear()
                    print(f"ğŸ’¾ ë¦¬ë·° ì¤‘ê°„ ì €ì¥ ì™„ë£Œ ({len(df_batch):,}ê°œ)")
                
                # íƒ€ì´í‹€ í†µê³„ ì¤‘ê°„ ì €ì¥
                if all_title_stats_results:
                    df_titles_batch = pd.DataFrame(all_title_stats_results)
                    df_titles_batch = df_titles_batch.drop_duplicates(subset=['imdb_id'])
                    file_exists_titles = Path(OUTPUT_TITLES_CSV).exists()
                    df_titles_batch.to_csv(
                        OUTPUT_TITLES_CSV,
                        mode='a',
                        header=not file_exists_titles,
                        index=False,
                        encoding='utf-8-sig'
                    )
                    all_title_stats_results.clear()
                    print(f"ğŸ’¾ íƒ€ì´í‹€ í†µê³„ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ ({len(df_titles_batch):,}ê°œ)")
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            elapsed = (datetime.now() - t0).total_seconds() / 60
            progress = stats["series_success"] + stats["series_failed"]
            rate = progress / elapsed if elapsed > 0 else 0
            eta = (stats["series_total"] - progress) / rate if rate > 0 else 0
            
            print(
                f"\nğŸ“Š ì§„í–‰: {progress}/{stats['series_total']} "
                f"({progress/stats['series_total']*100:.1f}%) | "
                f"ì„±ê³µ: {stats['series_success']} | ì‹¤íŒ¨: {stats['series_failed']} | "
                f"ì´ ë¦¬ë·°: {stats['reviews_total']:,}ê°œ | "
                f"ìš”ì²­: {stats['requests']:,}íšŒ | "
                f"ì†ë„: {rate:.1f}ì‹œë¦¬ì¦ˆ/ë¶„ | ETA: {eta:.0f}ë¶„\n"
            )
    
    # 4. ìµœì¢… ì €ì¥
    print("\nğŸ’¾ ìµœì¢… ì €ì¥ ì¤‘...")

    # ë‚¨ì€ ë¦¬ë·° ì €ì¥
    if all_reviews_results:
        df_batch = pd.DataFrame(all_reviews_results)
        df_batch = df_batch.drop_duplicates(subset=['review_id'])
        file_exists = Path(OUTPUT_REVIEWS_CSV).exists()
        df_batch.to_csv(
            OUTPUT_REVIEWS_CSV,
            mode='a',
            header=not file_exists,
            index=False,
            encoding='utf-8-sig'
        )

    # ë‚¨ì€ íƒ€ì´í‹€ í†µê³„ ì €ì¥
    if all_title_stats_results:
        df_titles_batch = pd.DataFrame(all_title_stats_results)
        df_titles_batch = df_titles_batch.drop_duplicates(subset=['imdb_id'])
        file_exists_titles = Path(OUTPUT_TITLES_CSV).exists()
        df_titles_batch.to_csv(
            OUTPUT_TITLES_CSV,
            mode='a',
            header=not file_exists_titles,
            index=False,
            encoding='utf-8-sig'
        )

    # ë¦¬ë·° ì „ì²´ ì¤‘ë³µ ì œê±°
    if Path(OUTPUT_REVIEWS_CSV).exists():
        df_results = pd.read_csv(OUTPUT_REVIEWS_CSV)
        df_results = df_results.drop_duplicates(subset=['review_id'])
        df_results.to_csv(OUTPUT_REVIEWS_CSV, index=False, encoding='utf-8-sig')
    else:
        df_results = pd.DataFrame()

    # íƒ€ì´í‹€ í†µê³„ ì¤‘ë³µ ì œê±°
    if Path(OUTPUT_TITLES_CSV).exists():
        df_titles = pd.read_csv(OUTPUT_TITLES_CSV)
        df_titles = df_titles.drop_duplicates(subset=['imdb_id'])
        df_titles.to_csv(OUTPUT_TITLES_CSV, index=False, encoding='utf-8-sig')
    else:
        df_titles = pd.DataFrame()
    
    # parquet ì €ì¥ (ë¦¬ë·°ë§Œ)
    try:
        if not df_results.empty:
            df_results.to_parquet(OUTPUT_REVIEWS_PARQUET, index=False)
    except Exception as e:
        print(f"âš ï¸  Parquet ì €ì¥ ì‹¤íŒ¨: {e}")

    # ì²´í¬í¬ì¸íŠ¸ ì œê±°
    if Path(CHECKPOINT_FILE).exists():
        Path(CHECKPOINT_FILE).unlink()
    
    # 5. ìµœì¢… í†µê³„ ì¶œë ¥
    elapsed = (datetime.now() - t0).total_seconds() / 60
    
    print("\n" + "=" * 90)
    print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 90)
    print(f"ğŸ“Œ ì‹œë¦¬ì¦ˆ: {stats['series_success']:,}/{stats['series_total']:,}ê°œ ì„±ê³µ")
    
    if not df_results.empty:
        print(f"ğŸ“Œ ì´ ë¦¬ë·°: {len(df_results):,}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
        if stats['series_success'] > 0:
            print(f"ğŸ“Œ í‰ê·  ë¦¬ë·°: {len(df_results)/stats['series_success']:.1f}ê°œ/ì‹œë¦¬ì¦ˆ")
    else:
        print("ğŸ“Œ ì´ ë¦¬ë·°: 0ê°œ")
    
    if not df_titles.empty:
        print(f"ğŸ“Œ íƒ€ì´í‹€ í†µê³„: {len(df_titles):,}ê°œ ì‹œë¦¬ì¦ˆ")
    
    print(f"ğŸ“Œ ì´ ìš”ì²­: {stats['requests']:,}íšŒ")
    print(f"â±ï¸  ì´ ì‹œê°„: {elapsed:.1f}ë¶„ ({elapsed/60:.2f}ì‹œê°„)")
    
    if stats['series_success'] > 0 and elapsed > 0:
        print(f"ğŸ“Š ì†ë„: {stats['series_success']/elapsed:.1f}ì‹œë¦¬ì¦ˆ/ë¶„")
        if not df_results.empty:
            print(f"ğŸ“Š ë¦¬ë·° ìˆ˜ì§‘ ì†ë„: {len(df_results)/elapsed:.0f}ê°œ/ë¶„")
    
    print("=" * 90)
    
    # ìƒ˜í”Œ ì¶œë ¥
    if not df_results.empty:
        print("\nğŸ“Š ë¦¬ë·° ìƒ˜í”Œ:")
        print(df_results.head(3).to_string())
        print(f"\nâœ… ë¦¬ë·° ê²°ê³¼ íŒŒì¼: {OUTPUT_REVIEWS_CSV}")
    
    if not df_titles.empty:
        print("\nğŸ“Š íƒ€ì´í‹€ í†µê³„ ìƒ˜í”Œ:")
        print(df_titles.head(3).to_string())
        print(f"\nâœ… íƒ€ì´í‹€ ê²°ê³¼ íŒŒì¼: {OUTPUT_TITLES_CSV}")

# ==========================================================
# ì‹¤í–‰
# ==========================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='IMDB FULL DATA COLLECTOR')
    parser.add_argument('--input', '-i', default='tv_series_2016_2025_FULL.csv',
                        help='ì…ë ¥ CSV íŒŒì¼ (ê¸°ë³¸: tv_series_2016_2025_FULL.csv)')
    parser.add_argument('--vote', '-v', type=int, default=30,
                        help='ìµœì†Œ vote_count (ê¸°ë³¸: 30)')
    parser.add_argument('--max-reviews', '-m', type=int, default=None,
                        help='ì‹œë¦¬ì¦ˆë‹¹ ìµœëŒ€ ë¦¬ë·° ìˆ˜ (ê¸°ë³¸: ì „ì²´)')
    
    args = parser.parse_args()
    
    if not Path(args.input).exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
    else:
        asyncio.run(main(args.input, args.vote, args.max_reviews))
